<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    
    <link rel="shortcut icon" href="../../../img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>IBM Watson - Administration Guide</title>
    <link href="../../../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../../../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../../../css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="../../../css/highlight.css">
    <link href="../../../css/extra.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../../../js/jquery-3.2.1.min.js"></script>
    <script src="../../../js/bootstrap-3.3.7.min.js"></script>
    <script src="../../../js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "Transcripts (Automated by IBM Watson)", url: "#_top", children: [
              {title: "Overview", url: "#overview" },
              {title: "Configuration", url: "#configuration" },
              {title: "Workflow Operations", url: "#workflow-operations" },
          ]},
        ];

    </script>
    <script src="../../../js/base.js"></script>
      <script src="../../../javascript/extra.js"></script>
      <script src="../../../javascript/popper.js"></script>
      <script src="../../../javascript/tippy.js"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
  <a name="_top"></a>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../microsoftazuretranscripts/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../microsoftazuretranscripts/" class="btn btn-xs btn-link">
        Microsoft Azure
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../googlespeechtranscripts/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../googlespeechtranscripts/" class="btn btn-xs btn-link">
        Google Speech
      </a>
    </div>
    
  </div>

    

    <h1 id="transcripts-automated-by-ibm-watson">Transcripts (Automated by IBM Watson)</h1>
<blockquote>
<p>Using this requires you to turn on an Opencast plugin.
Take a look <a href="../../plugin-management/">at the plugin management documentation</a> to find out how you can do that.</p>
</blockquote>
<h2 id="overview">Overview</h2>
<p>The IBMWatsonTranscriptionService invokes the IBM Watson Speech-to-Text service via REST API to translate audio to
 text.</p>
<p>During the execution of an Opencast workflow, an audio file is extracted from one of the presenter videos and sent to
the IBM Watson Speech-to-Text service. When the results are received, they are converted to the desired caption format
and attached to the media package.</p>
<p>Workflow 1 runs:</p>
<ul>
<li>Audio file created</li>
<li>Watson Speech-to-Text job started</li>
<li>Workflow finishes</li>
</ul>
<p>Translation finishes, callback with results is received, and workflow 2 is started.</p>
<p>Workflow 2 runs:</p>
<ul>
<li>File with results is converted and attached to media package</li>
<li>Media package is republished with captions/transcripts</li>
</ul>
<p>IBM Watson Speech-to-Text service documentation, including which languages are currently supported, can be found
 <a href="https://cloud.ibm.com/docs/services/speech-to-text/index.html#about">here</a>.</p>
<h2 id="configuration">Configuration</h2>
<h3 id="step-1-get-ibm-watson-credentials">Step 1: Get IBM Watson credentials</h3>
<ul>
<li><a href="https://console.bluemix.net">Create a 30-day trial account in IBM Cloud</a></li>
<li><a href="https://console.bluemix.net/docs/services/watson/getting-started-iam.html#iam">Get service credentials</a></li>
</ul>
<p>As of 10/30/2018, the service has migrated to token-based Identity and Access Management (IAM) authentication so user
and password are not generated anymore. Previously created instances can still use user name and password.
Details can be found <a href="https://cloud.ibm.com/docs/services/speech-to-text/release-notes.html#October2018b">here</a>.</p>
<h3 id="step-2-configure-ibmwatsontranscriptionservice">Step 2: Configure IBMWatsonTranscriptionService</h3>
<p>Edit  <em>etc/org.opencastproject.transcription.ibmwatson.IBMWatsonTranscriptionService.cfg</em>:</p>
<ul>
<li>Set <em>enabled</em>=true</li>
<li>Use service credentials obtained above to set <em>ibm_watson_api_key</em> (<em>ibm.watson.user</em> and <em>ibm.watson.psw</em>
are
still supported to be used with instances created previously)</li>
<li>Enter the IBM Watson Speech-to-Text url in <em>ibm.watson.service.url</em>, if not using the default
(https://stream.watsonplatform.net/speech-to-text/api)</li>
<li>Enter the appropriate language model in <em>ibm.watson.model</em>, if not using the default (<em>en-US_BroadbandModel</em>)</li>
<li>In <em>workflow</em>, enter the workflow definition id of the workflow to be used to attach the generated
transcripts/captions</li>
<li>Enter a <em>notification.email</em> to get job failure notifications. If not entered, the email in
etc/custom.properties (org.opencastproject.admin.email) will be used. Configure the SmtpService.
If no email address specified in either <em>notification.email</em> or <em>org.opencastproject.admin.email</em>,
email notifications will be disabled.</li>
<li>If re-submitting requests is desired in case of failures, configure <em>max-attempts</em> and <em>retry.workflow</em>.
If <em>max.attempts</em> &gt; 1 and the service receives an error callback, it will start the workflow specified in
<em>retry_workflow</em> to create a new job. When <em>max.attempts</em> is reached for a track, the service will stop retrying.</li>
</ul>
<pre><code># Change enabled to true to enable this service.
enabled=false

# IBM Watson Speech-to-Text service url
# Default: https://stream.watsonplatform.net/speech-to-text/api
# ibm.watson.service.url=https://stream.watsonplatform.net/speech-to-text/api

# APi key obtained when registering with the IBM Watson Speech-to_text service.
# If empty, user and password below will be used.
ibm.watson.api.key=&lt;API_KEY&gt;

# User obtained when registering with the IBM Watson Speech-to_text service.
# Mandatory if ibm.watson.api.key not entered.
#ibm.watson.user=&lt;SERVICE_USER&gt;

# Password obtained when registering with the IBM Watson Speech-to_text service
# Mandatory if ibm.watson.api.key not entered.
#ibm.watson.password=&lt;SERVICE_PSW&gt;

# Language model to be used. See the IBM Watson Speech-to-Text service documentation
# for available models.
# Default: en-US_BroadbandModel
#ibm.watson.model=en-US_BroadbandModel

# Workflow to be executed when results are ready to be attached to media package.
# Default: attach-watson-transcripts
#workflow=attach-watson-transcripts

# Interval the workflow dispatcher runs to start workflows to attach transcripts to the media package
# after the transcription job is completed. In seconds.
# Default: 60
#workflow.dispatch.interval=60

# How long it should wait to check jobs after their start date + track duration has passed.
# This is only used if we didn't get a callback from the ibm watson speech-to-text service.
# In seconds.
# Default: 600
#completion.check.buffer=600

# How long to wait after a transcription is supposed to finish before marking the job as
# canceled in the database. In seconds. Default is 2 hours.
# Default: 7200
#max.processing.time=7200

# How long to keep result files in the working file repository in days.
# Default: 7
#cleanup.results.days=7

# Email to send notifications of errors. If not entered, the value from
# org.opencastproject.admin.email in custom.properties will be used.
#notification.email=

# Start transcription job load
# Default: 0.1
#job.load.start.transcription=0.1

# Number of max attempts. If max attempts &gt; 1 and the service returned an error after the recognitions job was
# accepted or the job did not return any results, the transcription is re-submitted. Default is to not retry.
# Default: 1
#max.attempts=

# If max.attempts &gt; 1, name of workflow to use for retries.
#retry.workflow=

</code></pre>
<h3 id="step-3-add-encoding-profile-for-extracting-audio">Step 3: Add encoding profile for extracting audio</h3>
<p>The IBM Watson Speech-to-Text service has limitations on audio file size. Try using the encoding profile suggested in
etc/encoding/watson-audio.properties.</p>
<h3 id="step-4-add-workflow-operations-and-create-new-workflow">Step 4: Add workflow operations and create new workflow</h3>
<p>Add the following operations to your workflow. We suggest adding them after the media package is
published so that users can watch videos without having to wait for the transcription to finish, but it
depends on your use case. The only requirement is to take a snapshot of the media package so that
the second workflow can retrieve it from the Asset Manager to attach the caption/transcripts.</p>
<pre><code class="language-xml">&lt;!-- Extract audio from one of the presenter videos --&gt;

&lt;operation
  id=&quot;encode&quot;
  fail-on-error=&quot;true&quot;
  exception-handler-workflow=&quot;partial-error&quot;
  description=&quot;Extract audio for transcript generation&quot;&gt;
  &lt;configurations&gt;
    &lt;configuration key=&quot;source-tags&quot;&gt;engage-download&lt;/configuration&gt;
    &lt;configuration key=&quot;target-flavor&quot;&gt;audio/ogg&lt;/configuration&gt;
    &lt;!-- The target tag 'transcript' will be used in the next 'start-watson-transcription' operation --&gt;
    &lt;configuration key=&quot;target-tags&quot;&gt;transcript&lt;/configuration&gt;
    &lt;configuration key=&quot;encoding-profile&quot;&gt;audio-opus&lt;/configuration&gt;
    &lt;!-- If there is more than one file that match the source-tags, use only the first one --&gt;
    &lt;configuration key=&quot;process-first-match-only&quot;&gt;true&lt;/configuration&gt;
  &lt;/configurations&gt;
&lt;/operation&gt;

&lt;!-- Start IBM Watson recognitions job --&gt;

&lt;operation
  id=&quot;start-watson-transcription&quot;
  fail-on-error=&quot;true&quot;
  exception-handler-workflow=&quot;partial-error&quot;
  description=&quot;Start IBM Watson transcription job&quot;&gt;
  &lt;configurations&gt;
    &lt;!--  Skip this operation if flavor already exists. Used for cases when mp already has captions. --&gt;
    &lt;configuration key=&quot;skip-if-flavor-exists&quot;&gt;captions/vtt+en&lt;/configuration&gt;
    &lt;!-- Audio to be translated, produced in the previous compose operation --&gt;
    &lt;configuration key=&quot;source-tag&quot;&gt;transcript&lt;/configuration&gt;
  &lt;/configurations&gt;
&lt;/operation&gt;

</code></pre>
<p>Create a workflow that will add the generated caption/transcript to the media package and republish it.
A sample one can be found in etc/workflows/attach-watson-transcripts.xml</p>
<p>If re-submitting requests is desired in case of failures, create a workflow that will start a transcription job.
A sample one can be found in etc/workflows/retry-watson-transcripts.xml</p>
<h2 id="workflow-operations">Workflow Operations</h2>
<ul>
<li><a href="../../../workflowoperationhandlers/start-watson-transcription-woh/">start-watson-transcription</a></li>
<li><a href="../../../workflowoperationhandlers/attach-watson-transcription-woh/">attach-watson-transcription</a></li>
</ul>

  <br>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../microsoftazuretranscripts/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../microsoftazuretranscripts/" class="btn btn-xs btn-link">
        Microsoft Azure
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../googlespeechtranscripts/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../googlespeechtranscripts/" class="btn btn-xs btn-link">
        Google Speech
      </a>
    </div>
    
  </div>

    <br>
</div>

<footer class="container-fluid wm-page-content">
      <p>
        <a href="https://github.com/opencast/opencast/edit/develop/docs/guides/admin/docs/configuration/transcription.configuration/watsontranscripts.md"><i class="fa fa-github"></i>
Edit on GitHub</a>
      </p>
  <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>