<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    
    <link rel="shortcut icon" href="../../../img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>Google Speech - Administration Guide</title>
    <link href="../../../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../../../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../../../css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="../../../css/highlight.css">
    <link href="../../../css/extra.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../../../js/jquery-3.2.1.min.js"></script>
    <script src="../../../js/bootstrap-3.3.7.min.js"></script>
    <script src="../../../js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "Transcripts (Automated by Google Speech)", url: "#_top", children: [
              {title: "Overview", url: "#overview" },
              {title: "Configuration", url: "#configuration" },
              {title: "Transcription delay before cancellation", url: "#transcription-delay-before-cancellation" },
              {title: "Workflow Operations", url: "#workflow-operations" },
          ]},
        ];

    </script>
    <script src="../../../js/base.js"></script>
      <script src="../../../javascript/extra.js"></script>
      <script src="../../../javascript/popper.js"></script>
      <script src="../../../javascript/tippy.js"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
  <a name="_top"></a>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../watsontranscripts/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../watsontranscripts/" class="btn btn-xs btn-link">
        IBM Watson
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../amberscripttranscripts/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../amberscripttranscripts/" class="btn btn-xs btn-link">
        AmberScript
      </a>
    </div>
    
  </div>

    

    <h1 id="transcripts-automated-by-google-speech">Transcripts (Automated by Google Speech)</h1>
<blockquote>
<p>Using this requires you to turn on an Opencast plugin.
Take a look <a href="../../plugin-management/">at the plugin management documentation</a> to find out how you can do that.</p>
</blockquote>
<h2 id="overview">Overview</h2>
<p>The GoogleSpeechTranscriptionService invokes the Google Speech-to-Text service via REST API to transcribe audio to text.</p>
<p>During the execution of an Opencast workflow, an audio file is extracted from one of the presenter videos and sent 
to the Google Speech-to-Text service. When the results are received, they are converted to the desired caption format 
and attached to the media package.</p>
<p><strong>Note that because Google's Speech-to-Text service can take a while to process a recording, 
we do not wait for it to finish before proceeding with the rest of Opencast's normal processing, the transcription process is asynchronous.</strong></p>
<ul>
<li>Workflow 1 runs:<ul>
<li>Audio file is created</li>
<li>Google Speech-to-Text job is started</li>
<li>Workflow finishes</li>
</ul>
</li>
</ul>
<p>Translation finishes, workflow 2 is started.</p>
<ul>
<li>Workflow 2 runs:<ul>
<li>File with results is converted and attached to media package</li>
<li>Media package is republished with captions/transcripts</li>
</ul>
</li>
</ul>
<p>Google Speech-to-Text service documentation, including which languages are currently supported, can be found
 <a href="https://cloud.google.com/speech-to-text/docs/basics">here</a>.</p>
<h2 id="configuration">Configuration</h2>
<p><strong>Notes</strong>: Instructions and screenshots provided in this section are based on Google Speech-to-Text documentation at the time of writing this document. 
For up to date instructions please search for <em>'google speech to text configuration'</em> or visit <a href="https://console.cloud.google.com/getting-started">Google Cloud service page</a>.</p>
<h3 id="step-1-activate-google-speech-and-google-cloud-storage-apis">Step 1: Activate Google Speech and Google Cloud Storage APIs</h3>
<ul>
<li>Log in to your Google account and <a href="https://cloud.google.com/free/">Activate a 12 months free trial Google Cloud Platform services</a></li>
<li><a href="https://console.cloud.google.com/getting-started">Create a Project to store your credentials and billing information</a><ul>
<li>Click  <em>Select a project</em> to create new project or use existing project</li>
</ul>
</li>
<li><a href="https://console.cloud.google.com/getting-started">Enable Google Speech API</a><ul>
<li>Expand the menu on the left</li>
<li>Go to <em>APIs &amp; Service</em> <strong>&gt;</strong> <em>Libraries</em></li>
<li>Find the <strong>Cloud Speech API</strong> and click <em>Enable</em> to enable the Google Cloud Speech API</li>
</ul>
</li>
<li><a href="https://console.cloud.google.com/getting-started">Enable Google Cloud Storage and Google Cloud Storage JSON API</a><ul>
<li>Go to <em>APIs &amp; Service</em> <strong>&gt;</strong> <em>Libraries</em></li>
<li>Find <strong>Google Cloud Storage</strong> and <strong>Google Cloud Storage JSON API</strong> and enable them if there are not.</li>
</ul>
</li>
<li>Create a cloud storage bucket. This is where you will temporary host the files you want to transcribe<ul>
<li>Go to your <a href="https://console.cloud.google.com/home/dashboard">Google Cloud Dashboard</a></li>
<li>Expand the menu on the left</li>
<li>Go to <em>Storage</em> <strong>&gt;</strong> Browser</li>
<li>Click <em>CREATE BUCKET</em> &nbsp;&nbsp;to create a bucket for the selected project</li>
</ul>
</li>
</ul>
<h3 id="step-2-get-google-cloud-credentials">Step 2: Get Google Cloud credentials</h3>
<ul>
<li>Go to your <a href="https://console.cloud.google.com/home/dashboard">Google Cloud Dashboard</a></li>
<li>Expand the menu on the left</li>
<li>Go to <em>APIs &amp; Service</em> <strong>&gt;</strong> <em>Credentials</em></li>
<li>Click on the tab <em>OAuth Consent Screen</em></li>
<li>Fill in a <em>Project name</em> and Save it. Don't worry about the other fields.</li>
</ul>
<p><img alt="Screenshot" src="../google-speech-authorisation.png" /></p>
<ul>
<li>Go back to Credentials</li>
<li>Click the button that says <em>Create Credentials</em> </li>
<li>select <em>OAuth Client ID</em></li>
</ul>
<p><img alt="Screenshot" src="../google-speech-create_oauth-client-id.png" /></p>
<ul>
<li>Choose <em>Web Application</em> and give it a name.</li>
<li>Add <strong>https://developers.google.com/oauthplayground</strong> in <em>Authorized redirect URIs</em>. You will need to use this in the next step to get your refresh token</li>
<li>Click <em>Create</em> and take note of your <strong>Client ID</strong> and <strong>Client Secret</strong></li>
</ul>
<p><img alt="Screenshot" src="../google-speech_create_token.png" /></p>
<h4 id="getting-your-refresh-token-and-authorization-endpoint">Getting your Refresh Token and Authorization endpoint</h4>
<ul>
<li>Go to <a href="https://developers.google.com/oauthplayground">https://developers.google.com/oauthplayground</a> (Make sure you added this URL to your Authorized redirect URIs in the previous step.)</li>
<li>In the top right corner, click the settings icon</li>
<li>Take note of your <em>Token endpoint</em>. It is the token endpoint URL needed for the configuration. </li>
<li>Make sure the Access token location is set to <strong>Authorization header w/ Bearer prefix</strong></li>
<li>Make sure Access type is set to <em>Offline</em></li>
<li>Make sure Force prompt is set to 'Consent Screen'</li>
<li>Check <em>Use your own OAuth credentials</em> </li>
<li>Paste your <strong>Client ID</strong> and <strong>Client Secret</strong> created previously.</li>
<li>Close the settings. </li>
</ul>
<p><img alt="Screenshot" src="../google-speech_client-secret.png" /></p>
<ul>
<li>Select the scope of your APIs<ul>
<li>Click <code>Step 1 Select &amp; authorize APIs</code> tab on the left</li>
<li>Find <strong>Cloud Speech API v1</strong> and click on <em>https://www.googleapis.com/auth/cloud-platform</em> to select it.</li>
<li>Find <strong>Cloud Storage API v1</strong> from the list, expand it and click on <em>https://www.googleapis.com/auth/devstorage.full_control</em> to select it</li>
<li>Find <strong>Cloud Storage JSON API v1</strong> expand it and select <em>https://www.googleapis.com/auth/devstorage.full_control</em></li>
<li>Click <em>Authorize APIs</em>, allow access to your account when prompted. There will be a few warning prompts, just proceed.
(On some browser you may need to click the advanced option before you can proceed to next page)</li>
</ul>
</li>
<li>When you get to <code>step 2 Exchange authorization code for tokens</code> tab, click <em>Exchange authorization code for tokens</em>.</li>
</ul>
<p><img alt="Screenshot" src="../google-speech_genarated-token.png" /></p>
<ul>
<li>You will need the OAuth Client ID, OAuth Client secret ,the Refresh token and Token endpoint for the configuration file</li>
</ul>
<h3 id="step-3-configure-googlespeechtranscriptionservice">Step 3: Configure GoogleSpeechTranscriptionService</h3>
<p>Edit  <em>etc/org.opencastproject.transcription.googlespeech.GoogleSpeechTranscriptionService.cfg</em>:</p>
<ul>
<li>Set <em>enabled</em>=true</li>
<li>Use <strong><em>OAuth Client ID</em></strong>, <strong><em>OAuth Client secret</em></strong>, <strong><em>Refresh token</em></strong>, <strong><em>Token endpoint</em></strong> and <strong><em>storage bucket</em></strong> created above to respectively set <em>google.cloud.client.id</em> , <em>google.cloud.client.secret</em> , <em>google.cloud.refresh.token</em> , <em>google.cloud.token.endpoint.url</em> and <em>google.cloud.storage.bucket</em></li>
<li>Enter the appropriate language in <em>google.speech.language</em>, default is (<em>en-US</em>). List of supported language: <a href="https://cloud.google.com/speech-to-text/docs/languages">https://cloud.google.com/speech-to-text/docs/languages</a></li>
<li>Remove profanity (bad language) from transcription by using <em>google.speech.profanity.filter</em>, default is (<em>false</em>), not removed by default</li>
<li>Set the transcription model using <em>google.speech.transcription.model</em>, default is (<em>default</em>). List of models: https://cloud.google.com/speech-to-text/docs/transcription-model</li>
<li>Enable punctuation for transcription result by setting <em>google.speech.transcription.punctuation</em>, to (<em>true</em>) default is (<em>false</em>)</li>
<li>In <em>workflow</em>, enter the workflow definition id of the workflow to be used to attach the generated transcripts/captions</li>
<li>Enter a <em>notification.email</em> to get job failure notifications. If not entered, the email in <em>etc/custom.properties</em> (org.opencastproject.admin.email) will be used.
If no email address specified in either <em>notification.email</em> or <em>org.opencastproject.admin.email</em>,
email notifications will be disabled. </li>
</ul>
<p>Example of configuration file: </p>
<pre><code># Change enabled to true to enable this service. 
enabled=false

# Google Cloud Service details 
google.cloud.client.id=&lt;OAUTH_CLIENT_ID&gt;
google.cloud.client.secret=&lt;OAUTH_CLIENT_SECRET&gt;
google.cloud.refresh.token=1&lt;REFRESH_TOKEN&gt;
google.cloud.token.endpoint.url=&lt;TOKEN_ENDPOINT&gt;

# google cloud storage bucket
google.cloud.storage.bucket=&lt;BUCKET_NAME&gt;

# Language of the supplied audio. See the Google Speech-to-Text service documentation
# for available languages. If empty, the default will be used (&quot;en-US&quot;).
google.speech.language=

# Filter out profanities from result. Default is false
google.speech.profanity.filter=false

# Enable punctuations for transcription. Default is false
google.speech.transcription.punctuation=true

# Transcription model to use
# If empty, the &quot;default&quot; model will be used
google.speech.transcription.model=default

# Workflow to be executed when results are ready to be attached to media package.
workflow=google-speech-attach-transcripts

# Interval the workflow dispatcher runs to start workflows to attach transcripts to the media package
# after the transcription job is completed.
# (in seconds) Default is 1 minute.
workflow.dispatch.interval=60

# How long it should wait to check jobs after their start date + track duration has passed.
# The default is 5 minutes.
# (in seconds)
completion.check.buffer=300

# How long to wait after a transcription is supposed to finish before marking the job as 
# cancelled in the database. Default is 5 hours.
# (in seconds)
max.processing.time=18000

# How long to keep result files in the working file repository in days.
# The default is 7 days.
cleanup.results.days=7

# Email to send notifications of errors. If not entered, the value from
# org.opencastproject.admin.email in custom.properties will be used.
notification.email=localadmin@domain
</code></pre>
<h3 id="step-4-add-encoding-profile-for-extracting-audio">Step 4: Add encoding profile for extracting audio</h3>
<p>The Google Speech-to-Text service has limitations on audio types. <a href="https://cloud.google.com/speech-to-text/docs/reference/rest/v1p1beta1/RecognitionConfig#AudioEncoding">Supported audio type are here</a>.
By default Opencast will use the encoding settings in etc/encoding/googlespeech-audio.properties.</p>
<h3 id="step-5-add-workflow-operations-and-create-new-workflow">Step 5: Add workflow operations and create new workflow</h3>
<p>Add the following operations to your workflow. We suggest adding them after the media package is
published so that users can watch videos without having to wait for the transcription to finish, but it
depends on your use case. The only requirement is to take a snapshot of the media package so that
the second workflow can retrieve it from the archive to attach the caption/transcripts.</p>
<pre><code class="language-xml">    &lt;!--  Encode audio to flac --&gt;
    &lt;operation
      id=&quot;encode&quot;
      fail-on-error=&quot;true&quot;
      exception-handler-workflow=&quot;partial-error&quot;
      description=&quot;Extract audio for transcript generation&quot;&gt;
      &lt;configurations&gt;
        &lt;configuration key=&quot;source-flavor&quot;&gt;*/source&lt;/configuration&gt;
        &lt;configuration key=&quot;target-flavor&quot;&gt;audio/flac&lt;/configuration&gt;
        &lt;configuration key=&quot;target-tags&quot;&gt;transcript&lt;/configuration&gt;
        &lt;configuration key=&quot;encoding-profile&quot;&gt;audio-flac&lt;/configuration&gt;
        &lt;configuration key=&quot;process-first-match-only&quot;&gt;true&lt;/configuration&gt;
      &lt;/configurations&gt;
    &lt;/operation&gt; 

    &lt;!-- Start Google Speech transcription job --&gt;
    &lt;operation
      id=&quot;google-speech-start-transcription&quot;
      fail-on-error=&quot;true&quot;
      exception-handler-workflow=&quot;partial-error&quot;
      description=&quot;Start Google Speech transcription job&quot;&gt;
      &lt;configurations&gt;
        &lt;!--  Skip this operation if flavor already exists. Used for cases when mediapackage already has captions. --&gt;
        &lt;configuration key=&quot;skip-if-flavor-exists&quot;&gt;captions/timedtext&lt;/configuration&gt;
        &lt;configuration key=&quot;language-code&quot;&gt;en-US&lt;/configuration&gt;
        &lt;!-- Audio to be translated, produced in the previous compose operation --&gt;
        &lt;configuration key=&quot;source-tag&quot;&gt;transcript&lt;/configuration&gt;
      &lt;/configurations&gt;
    &lt;/operation&gt;


</code></pre>
<h3 id="step-6-create-a-workflow-that-will-add-the-generated-captiontranscript-to-the-media-package-and-republish-it">Step 6: Create a workflow that will add the generated caption/transcript to the media package and republish it</h3>
<p>A sample one can be found in etc/workflows/google-speech-attach-transcripts.xml</p>
<pre><code class="language-xml"> &lt;!-- Attach caption/transcript --&gt;

    &lt;operation id=&quot;google-speech-attach-transcription&quot;
      fail-on-error=&quot;true&quot;
      exception-handler-workflow=&quot;partial-error&quot; 
      description=&quot;Attach captions/transcription&quot;&gt;
      &lt;configurations&gt;
        &lt;!-- This is filled out by the transcription service when starting this workflow --&gt;
        &lt;configuration key=&quot;transcription-job-id&quot;&gt;${transcriptionJobId}&lt;/configuration&gt;
        &lt;configuration key=&quot;line-size&quot;&gt;80&lt;/configuration&gt;
        &lt;configuration key=&quot;target-flavor&quot;&gt;captions/timedtext&lt;/configuration&gt;
        &lt;configuration key=&quot;target-tag&quot;&gt;archive&lt;/configuration&gt;
        &lt;configuration key=&quot;target-caption-format&quot;&gt;vtt&lt;/configuration&gt;
      &lt;/configurations&gt;
    &lt;/operation&gt;

    &lt;!-- Publish to engage player --&gt;

    &lt;operation id=&quot;publish-engage&quot;
      fail-on-error=&quot;true&quot;
      exception-handler-workflow=&quot;partial-error&quot;
      description=&quot;Distribute and publish to engage server&quot;&gt;
      &lt;configurations&gt;
        &lt;configuration key=&quot;download-source-flavors&quot;&gt;dublincore/*,security/*,captions/*&lt;/configuration&gt;
        &lt;configuration key=&quot;strategy&quot;&gt;merge&lt;/configuration&gt;
        &lt;configuration key=&quot;check-availability&quot;&gt;false&lt;/configuration&gt;
      &lt;/configurations&gt;
    &lt;/operation&gt;

    &lt;!-- Publish to oaipmh --&gt;

    &lt;operation
      id=&quot;republish-oaipmh&quot;
      exception-handler-workflow=&quot;partial-error&quot;
      description=&quot;Update recording metadata in default OAI-PMH repository&quot;&gt;
      &lt;configurations&gt;
        &lt;configuration key=&quot;source-flavors&quot;&gt;dublincore/*,security/*,captions/*&lt;/configuration&gt;
        &lt;configuration key=&quot;repository&quot;&gt;default&lt;/configuration&gt;
      &lt;/configurations&gt;
    &lt;/operation&gt;

</code></pre>
<h2 id="transcription-delay-before-cancellation">Transcription delay before cancellation</h2>
<p>If an event is deleted before the end of Google transcription process, or the Google Speech to Text API has some issues, or
something unexpected happens, the transcription process for the event will not be immediately cancelled.
Instead, transcription will be attempted several times based on the video duration and configuration properties: <code>completion.check.buffer</code> and <code>max.processing.time</code>.</p>
<p><code>Video duration</code> + <code>completion.check.buffer</code> + <code>max.processing.time</code> set the duration before a 
Google transcription job is cancelled.</p>
<p><code>completion.check.buffer</code> 5 minutes by default</p>
<p><code>completion.check.buffer</code> 5 hours by default.</p>
<p>All these values can be changed in Google Transcription properties file: <em>etc/org.opencastproject.transcription.googlespeech.GoogleSpeechTranscriptionService.cfg</em></p>
<p>For example, if you have a 30 min video, using the default values, it will take 5 hours and 35 min before the transcription
is cancelled (when something goes wrong).</p>
<h2 id="workflow-operations">Workflow Operations</h2>
<ul>
<li><a href="../../../workflowoperationhandlers/google-speech-attach-transcription-woh/">google-speech-attach-transcription</a></li>
<li><a href="../../../workflowoperationhandlers/google-speech-start-transcription-woh/">google-speech-start-transcription</a></li>
</ul>

  <br>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../watsontranscripts/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../watsontranscripts/" class="btn btn-xs btn-link">
        IBM Watson
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../amberscripttranscripts/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../amberscripttranscripts/" class="btn btn-xs btn-link">
        AmberScript
      </a>
    </div>
    
  </div>

    <br>
</div>

<footer class="container-fluid wm-page-content">
      <p>
        <a href="https://github.com/opencast/opencast/edit/develop/docs/guides/admin/docs/configuration/transcription.configuration/googlespeechtranscripts.md"><i class="fa fa-github"></i>
Edit on GitHub</a>
      </p>
  <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>