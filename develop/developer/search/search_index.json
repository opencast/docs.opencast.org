{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Opencast Development Guides These guides will help you if you want to participate in Opencast development. Development Process Committers Decision Making Proposal Log QA Coordinator Release Manager Committers Reviewing, Merging and Declining Pull Requests Security Issues Licenses and Legal Matters Localization Governance Development Environment Docker Packaging Guidelines Modules Administrative User Interface Development Style Guide Capture Agent Stream Security \u2026 Configure a Local Cluster for Testing External API Project Infrastructure Maven Repository","title":"Home"},{"location":"#opencast-development-guides","text":"These guides will help you if you want to participate in Opencast development. Development Process Committers Decision Making Proposal Log QA Coordinator Release Manager Committers Reviewing, Merging and Declining Pull Requests Security Issues Licenses and Legal Matters Localization Governance Development Environment Docker Packaging Guidelines Modules Administrative User Interface Development Style Guide Capture Agent Stream Security \u2026 Configure a Local Cluster for Testing External API Project Infrastructure Maven Repository","title":"Opencast Development Guides"},{"location":"asset-manager/","text":"Asset Manager Architecture Modules The AssetManager consists of the following modules: asset-manager-api An API module defining the core AssetManager functions, properties and the query language. asset-manager-impl The default implementation of the AssetManager as an OSGi service, containing the storage API for pluggable asset stores. asset-manager-storage-fs The default implementation of the AssetStore. Depends on asset-manager-impl. asset-manager-util Additional functionality for the AssetManager providing utilities such as starting workflows on archived snapshots, etc. asset-manager-workflowoperation A workflow operation handler to take media package snapshots of a media package from inside a running workflow. High Level View TODO Describes components and how they relate. Classes TODO Most important classes and how they relate Default Implementation AssetStore Assets are stored in the following directory structure. $BASE_PATH |\u2014 <organization_id> |\u2014 <media_package_id> |\u2014 <version> |\u2014 manifest.xml |\u2014 <media_package_element_id>.<ext> Database The asset manager uses four tables oc_assets_snapshot Manages snapshots. Each snapshot may be linked to zero or more assets. oc_assets_asset Manages the assets of a snapshot. oc_assets_properties Manages the properties. This table is indirectly linked to the snapshot table via column mediapackage_id . oc_assets_version_claim Manages the next free version number per episode. Security TODO Usage Taking Snapshots TODO Working with Properties Properties are associated with an episode, not a single snapshot. They act as annotations helping services to work with saved media packages without having to implement their own storage layer. Properties are typed and can be used to create queries. Getting Started Let's start with an fictious example of an ApprovalService. The approval service keeps track of approvals given by an editor to publish a media package. Only approved media packages may be published and the editor should also be able to leave a comment defining a publication as prohibited. Here, three properties are needed, an approval flag, a text field for comments and a time stamp for the date of approval. The following code snippet sets a property on an episode, with am referring to the AssetManager and mp the media package id of type String of the episode. AssetManager am = \u2026; String mp = \u2026; // a media package id am.setProperty(Property.mk(PropertyId.mk( mp, \"org.opencastproject.approval\", \"approval\"), Value.mk(true))); It is recommended to use namespace names after the service's package name, in the example: org.opencastproject.approval . This code looks overly verbose. Also you need to deal with namespace names and property names directly. That's cumbersome and error prone even though you might intoduce constants for them. To help remedy this situation a little helper class class PropertySchema exists. It is strongly recommended to make use of it. Here's how it goes. static class ApprovalPops extends PropertySchema { public ApprovalProps(AQueryBuilder q) { super(q, \"org.opencastproject.approval\"); } public PropertyField<Boolean> approved() { return booleanProp(\"approved\"); } public PropertyField<String> comment() { return stringProp(\"comment\"); } public PropertyField<Date> date() { return dateProp(\"date\"); } } Now you can set properties like this. am.setProperty(p.approved().mk(mp, false)); am.setProperty(p.comment().mk(mp, \"Audio quality is too poor!\")); am.setProperty(p.date().mk(mp, new Date()); Now, if you want to find all episodes that have been rejected you need to create and run the following query. AQueryBuilder q = am.createQuery(); AResult r = q.select(q.snapshot()).where(p.approved().eq(true)).run(); This query yields all snapshots of all episodes that have been approved. But that's not exactly what we want as we are only interested in the latest snapshot generated when we re-run the approval process, and resetting all previous approvals. q.select(q.snapshot()) .where(p.approved().eq(true).and(q.version().isLatest()) .run(); This will only return the latest version of each episode. However, along with the information of the approved episodes,we want to display when they were approved. Looking at the AResult and ARecord interfaces it seems that properties need to be selected in order to fetch them. q.select(q.snapshot(), q.properties()) .where(p.approved().eq(true).and(q.version().isLatest()) .run(); Here we go. Now we can access all properties stored with the returned snapshots. Now, let's assume other services make heavy use of properties too. This may cause serious database IO if we always select all properties like we did using the q.properties() target. Let's do better. q.select(q.snapshot(), q.propertiesOf(\"org.opencastproject.approval\")) .where(p.approved().eq(true).and(q.version().isLatest()) .run(); This will return only the properties of our service's namespace. But do we have to deal with namespace strings again? No. q.select(q.snapshot(), q.propertiesOf(p.allProperties())) .where(p.approved().eq(true).and(q.version().isLatest()) .run(); Our implementation of PropertySchema provides as with a ready to use target for the properties of our namespace only. In our use case we could reduce IO even further since we're only interested in the date property. q.select(q.snapshot(), q.propertiesOf(p.date().target())) .where(p.approved().eq(true).and(q.version().isLatest()) .run(); This is the query returns only the latest snapshots of all episodes being approved together with the date of approval. Now that you've seen how to create properties let's move on to delete them again. Deleting Properties Properties are deleted pretty much like they are queried, using a delete query. q.delete(q.propertiesOf(p.allProperties())).run(); The above query deletes all properties that belong to schema p from all episodes. If you want to restrict deletion to a single episode, add an id predicate to the where clause. q.delete(q.propertiesOf(p.allProperties())) .where(q.mediaPackageId(mpId)) .run(); Deleting just a single property from all episodes is also possible. q.delete(p.approved()).run(); Or multiple properties at once. q.delete(p.approved(), p.comment()).run(); Please see the query API documentation for further information. Value Types The following type are available for properties: Long String Date Boolean Version Version is the AssetManager type that abstracts a snapshot version. Decomposing properties Since properties are type safe they cannot be accessed directly. If you know the type of the property you can access its value using a type evidence constant. String string = p.getValue().get(Value.STRING); Boolean bool = p.getValue().get(Value.BOOLEAN); Type evidence constants are defined in class Value . If the type is unknown since you are iterating a mixed collection of values, for example if you need to decompose the value. Decomposition is the act of pattern matching against the value's type. Each case is handled by a different function, all returning the same type. Let's say you are iterating over a collection of values and want to print them, formatted, to the console. All handle* parameters are functions of type Fn taking the raw value as input and returning a String. List<Value> vs = \u2026; for (Value v : vs) { String f = v.decompose( handleStringFn, handleDateFn, handleLongFn, handleBooleanFn, handleVersionFn); System.out.println(f); } The class org.opencastproject.assetmanager.api.fn.Properties contains various utility functions to help extracting values from properties. Using PropertySchema You've already seen that a property is constructed from a media package id, a namespace, a property name and a value. Since this is a bit cumbersome, the API features an abstract base class to construct property schemas. The resulting schema implementations encapsulate all the string constants so that you don't have to deal with them manually. Please see the example in the Getting Started section. It is strongly recommended to work with schemas as much as possible. Creating and Running Queries Creating and running a query is a two step process. First, you create a new AQueryBuilder . AQueryBuilder q = am.createQuery(); Next, you build a query like this. ASelectQuery s = q.select(q.snapshot()) .where(q.mediaPackageId(mpId).and(q.version().isLatest()); Now it's time to actually run the query against the database. AResult r = s.run(); All this can, of course, be done in a single statement, but it has been broken up in several steps to show you the intermediate types. am.createQuery() .select(q.snapshot()) .where(q.mediaPackageId(mpId).and(q.version().isLatest()) .run(); The result set r contains the retrieved data encapsulated in stream of ARecord objects. If nothing matched the given predicates then a call to r.getRecords() yields an empty stream. Please note that even though a Stream is returned, it does not mean that the result set is actually streamed\u2014or lazily loaded\u2014from the database. The Stream interface is just far more powerful than the collection types from JCL. A note on immutability Please note that all classes of the query API are immutable and therefore safe to be used in a concurrent environment. Whenever you call a factory method on an instance of one of the query classes a new instance is yielded. They never mutate state. Accessing Query Results Running a query yields an object of type AResult which in turn yields the found result records. Besides it also provides some general result metadata like the set limit, offset etc. An ARecord holds the found snapshots and properties, depending on the select targets and the predicates. If no snapshots have been selected then, none will be returned here. The same holds true for properties. However, an ARecord instance holding the media package id is created regardless of the requested targets. The typical pattern to access query results is to iterate over the stream of records. This can be accomplished using a simple for loop or one of the functional methods that the Stream type provides, e.g. map over the elements of a stream to create a new one. For easy access to fetched resources you may wrap the result in an enrichment. AResult r = \u2026; RichAResult rr = Enrichments.enrich(r); RichAResult features methods to directly access all fetched snapshots and properties. Deleting Snapshots This works exactly like deleting properties, except that you need to specify snapshots instead of properties. Please note that it's also possible to specify snapshots and properties simultanously. q.delete(\"owner\", q.snapshot()).where(q.version().isLatest().not()).run(); The above query deletes all snapshots but the latest. This is a good query to free up some disc space. Snapshots can only be deleted per owner. Query Language Reference The query API features select clause and targets where clause with boolean and relational operations, nesting of boolean operations selecting by properties order-by clause querying and deleting Please see the API doc for further information about the various elements and how to create them.","title":"Asset Manager"},{"location":"asset-manager/#asset-manager","text":"","title":"Asset Manager"},{"location":"asset-manager/#architecture","text":"","title":"Architecture"},{"location":"asset-manager/#modules","text":"The AssetManager consists of the following modules: asset-manager-api An API module defining the core AssetManager functions, properties and the query language. asset-manager-impl The default implementation of the AssetManager as an OSGi service, containing the storage API for pluggable asset stores. asset-manager-storage-fs The default implementation of the AssetStore. Depends on asset-manager-impl. asset-manager-util Additional functionality for the AssetManager providing utilities such as starting workflows on archived snapshots, etc. asset-manager-workflowoperation A workflow operation handler to take media package snapshots of a media package from inside a running workflow.","title":"Modules"},{"location":"asset-manager/#high-level-view","text":"TODO Describes components and how they relate.","title":"High Level View"},{"location":"asset-manager/#classes","text":"TODO Most important classes and how they relate","title":"Classes"},{"location":"asset-manager/#default-implementation","text":"","title":"Default Implementation"},{"location":"asset-manager/#assetstore","text":"Assets are stored in the following directory structure. $BASE_PATH |\u2014 <organization_id> |\u2014 <media_package_id> |\u2014 <version> |\u2014 manifest.xml |\u2014 <media_package_element_id>.<ext>","title":"AssetStore"},{"location":"asset-manager/#database","text":"The asset manager uses four tables oc_assets_snapshot Manages snapshots. Each snapshot may be linked to zero or more assets. oc_assets_asset Manages the assets of a snapshot. oc_assets_properties Manages the properties. This table is indirectly linked to the snapshot table via column mediapackage_id . oc_assets_version_claim Manages the next free version number per episode.","title":"Database"},{"location":"asset-manager/#security","text":"TODO","title":"Security"},{"location":"asset-manager/#usage","text":"","title":"Usage"},{"location":"asset-manager/#taking-snapshots","text":"TODO","title":"Taking Snapshots"},{"location":"asset-manager/#working-with-properties","text":"Properties are associated with an episode, not a single snapshot. They act as annotations helping services to work with saved media packages without having to implement their own storage layer. Properties are typed and can be used to create queries.","title":"Working with Properties"},{"location":"asset-manager/#getting-started","text":"Let's start with an fictious example of an ApprovalService. The approval service keeps track of approvals given by an editor to publish a media package. Only approved media packages may be published and the editor should also be able to leave a comment defining a publication as prohibited. Here, three properties are needed, an approval flag, a text field for comments and a time stamp for the date of approval. The following code snippet sets a property on an episode, with am referring to the AssetManager and mp the media package id of type String of the episode. AssetManager am = \u2026; String mp = \u2026; // a media package id am.setProperty(Property.mk(PropertyId.mk( mp, \"org.opencastproject.approval\", \"approval\"), Value.mk(true))); It is recommended to use namespace names after the service's package name, in the example: org.opencastproject.approval . This code looks overly verbose. Also you need to deal with namespace names and property names directly. That's cumbersome and error prone even though you might intoduce constants for them. To help remedy this situation a little helper class class PropertySchema exists. It is strongly recommended to make use of it. Here's how it goes. static class ApprovalPops extends PropertySchema { public ApprovalProps(AQueryBuilder q) { super(q, \"org.opencastproject.approval\"); } public PropertyField<Boolean> approved() { return booleanProp(\"approved\"); } public PropertyField<String> comment() { return stringProp(\"comment\"); } public PropertyField<Date> date() { return dateProp(\"date\"); } } Now you can set properties like this. am.setProperty(p.approved().mk(mp, false)); am.setProperty(p.comment().mk(mp, \"Audio quality is too poor!\")); am.setProperty(p.date().mk(mp, new Date()); Now, if you want to find all episodes that have been rejected you need to create and run the following query. AQueryBuilder q = am.createQuery(); AResult r = q.select(q.snapshot()).where(p.approved().eq(true)).run(); This query yields all snapshots of all episodes that have been approved. But that's not exactly what we want as we are only interested in the latest snapshot generated when we re-run the approval process, and resetting all previous approvals. q.select(q.snapshot()) .where(p.approved().eq(true).and(q.version().isLatest()) .run(); This will only return the latest version of each episode. However, along with the information of the approved episodes,we want to display when they were approved. Looking at the AResult and ARecord interfaces it seems that properties need to be selected in order to fetch them. q.select(q.snapshot(), q.properties()) .where(p.approved().eq(true).and(q.version().isLatest()) .run(); Here we go. Now we can access all properties stored with the returned snapshots. Now, let's assume other services make heavy use of properties too. This may cause serious database IO if we always select all properties like we did using the q.properties() target. Let's do better. q.select(q.snapshot(), q.propertiesOf(\"org.opencastproject.approval\")) .where(p.approved().eq(true).and(q.version().isLatest()) .run(); This will return only the properties of our service's namespace. But do we have to deal with namespace strings again? No. q.select(q.snapshot(), q.propertiesOf(p.allProperties())) .where(p.approved().eq(true).and(q.version().isLatest()) .run(); Our implementation of PropertySchema provides as with a ready to use target for the properties of our namespace only. In our use case we could reduce IO even further since we're only interested in the date property. q.select(q.snapshot(), q.propertiesOf(p.date().target())) .where(p.approved().eq(true).and(q.version().isLatest()) .run(); This is the query returns only the latest snapshots of all episodes being approved together with the date of approval. Now that you've seen how to create properties let's move on to delete them again.","title":"Getting Started"},{"location":"asset-manager/#deleting-properties","text":"Properties are deleted pretty much like they are queried, using a delete query. q.delete(q.propertiesOf(p.allProperties())).run(); The above query deletes all properties that belong to schema p from all episodes. If you want to restrict deletion to a single episode, add an id predicate to the where clause. q.delete(q.propertiesOf(p.allProperties())) .where(q.mediaPackageId(mpId)) .run(); Deleting just a single property from all episodes is also possible. q.delete(p.approved()).run(); Or multiple properties at once. q.delete(p.approved(), p.comment()).run(); Please see the query API documentation for further information.","title":"Deleting Properties"},{"location":"asset-manager/#value-types","text":"The following type are available for properties: Long String Date Boolean Version Version is the AssetManager type that abstracts a snapshot version.","title":"Value Types"},{"location":"asset-manager/#decomposing-properties","text":"Since properties are type safe they cannot be accessed directly. If you know the type of the property you can access its value using a type evidence constant. String string = p.getValue().get(Value.STRING); Boolean bool = p.getValue().get(Value.BOOLEAN); Type evidence constants are defined in class Value . If the type is unknown since you are iterating a mixed collection of values, for example if you need to decompose the value. Decomposition is the act of pattern matching against the value's type. Each case is handled by a different function, all returning the same type. Let's say you are iterating over a collection of values and want to print them, formatted, to the console. All handle* parameters are functions of type Fn taking the raw value as input and returning a String. List<Value> vs = \u2026; for (Value v : vs) { String f = v.decompose( handleStringFn, handleDateFn, handleLongFn, handleBooleanFn, handleVersionFn); System.out.println(f); } The class org.opencastproject.assetmanager.api.fn.Properties contains various utility functions to help extracting values from properties.","title":"Decomposing properties"},{"location":"asset-manager/#using-propertyschema","text":"You've already seen that a property is constructed from a media package id, a namespace, a property name and a value. Since this is a bit cumbersome, the API features an abstract base class to construct property schemas. The resulting schema implementations encapsulate all the string constants so that you don't have to deal with them manually. Please see the example in the Getting Started section. It is strongly recommended to work with schemas as much as possible.","title":"Using PropertySchema"},{"location":"asset-manager/#creating-and-running-queries","text":"Creating and running a query is a two step process. First, you create a new AQueryBuilder . AQueryBuilder q = am.createQuery(); Next, you build a query like this. ASelectQuery s = q.select(q.snapshot()) .where(q.mediaPackageId(mpId).and(q.version().isLatest()); Now it's time to actually run the query against the database. AResult r = s.run(); All this can, of course, be done in a single statement, but it has been broken up in several steps to show you the intermediate types. am.createQuery() .select(q.snapshot()) .where(q.mediaPackageId(mpId).and(q.version().isLatest()) .run(); The result set r contains the retrieved data encapsulated in stream of ARecord objects. If nothing matched the given predicates then a call to r.getRecords() yields an empty stream. Please note that even though a Stream is returned, it does not mean that the result set is actually streamed\u2014or lazily loaded\u2014from the database. The Stream interface is just far more powerful than the collection types from JCL.","title":"Creating and Running Queries"},{"location":"asset-manager/#a-note-on-immutability","text":"Please note that all classes of the query API are immutable and therefore safe to be used in a concurrent environment. Whenever you call a factory method on an instance of one of the query classes a new instance is yielded. They never mutate state.","title":"A note on immutability"},{"location":"asset-manager/#accessing-query-results","text":"Running a query yields an object of type AResult which in turn yields the found result records. Besides it also provides some general result metadata like the set limit, offset etc. An ARecord holds the found snapshots and properties, depending on the select targets and the predicates. If no snapshots have been selected then, none will be returned here. The same holds true for properties. However, an ARecord instance holding the media package id is created regardless of the requested targets. The typical pattern to access query results is to iterate over the stream of records. This can be accomplished using a simple for loop or one of the functional methods that the Stream type provides, e.g. map over the elements of a stream to create a new one. For easy access to fetched resources you may wrap the result in an enrichment. AResult r = \u2026; RichAResult rr = Enrichments.enrich(r); RichAResult features methods to directly access all fetched snapshots and properties.","title":"Accessing Query Results"},{"location":"asset-manager/#deleting-snapshots","text":"This works exactly like deleting properties, except that you need to specify snapshots instead of properties. Please note that it's also possible to specify snapshots and properties simultanously. q.delete(\"owner\", q.snapshot()).where(q.version().isLatest().not()).run(); The above query deletes all snapshots but the latest. This is a good query to free up some disc space. Snapshots can only be deleted per owner.","title":"Deleting Snapshots"},{"location":"asset-manager/#query-language-reference","text":"The query API features select clause and targets where clause with boolean and relational operations, nesting of boolean operations selecting by properties order-by clause querying and deleting Please see the API doc for further information about the various elements and how to create them.","title":"Query Language Reference"},{"location":"code-style/","text":"Code Style Maven will automatically check for coding style violations for Java and interrupt the build process if any are found, displaying a message about it. Apart from that, here are some general rules: General Rules UTF-8 encoded No trailing spaces End files with a newline character Java, HTML and JavaScript Indentation is done with exactly two spaces No line of code should be wider than 120 columns Avoid unnecessary code style changes Markdown Avoid lines wider than 120 columns Avoid unnecessary style changes Everything Else Try applying the Java style rules If in doubt, ask on list Logging Rules The following is a list of logging levels and their use. Level Description TRACE Information that would only be useful when debugging a specific subsystem. DEBUG Information relevant to development. Used to provide detailed information for developers. INFO Information relevant to server administrators. Creating, updating, and deleting files should be logged here. WARN Handled exceptions. A warning should be logged any time there is a possible problem administrators may need to investigate.. ERROR Unhandled exceptions. Problems that were not automatically handled.","title":"Code Style"},{"location":"code-style/#code-style","text":"Maven will automatically check for coding style violations for Java and interrupt the build process if any are found, displaying a message about it. Apart from that, here are some general rules:","title":"Code Style"},{"location":"code-style/#general-rules","text":"UTF-8 encoded No trailing spaces End files with a newline character","title":"General Rules"},{"location":"code-style/#java-html-and-javascript","text":"Indentation is done with exactly two spaces No line of code should be wider than 120 columns Avoid unnecessary code style changes","title":"Java, HTML and JavaScript"},{"location":"code-style/#markdown","text":"Avoid lines wider than 120 columns Avoid unnecessary style changes","title":"Markdown"},{"location":"code-style/#everything-else","text":"Try applying the Java style rules If in doubt, ask on list","title":"Everything Else"},{"location":"code-style/#logging-rules","text":"The following is a list of logging levels and their use. Level Description TRACE Information that would only be useful when debugging a specific subsystem. DEBUG Information relevant to development. Used to provide detailed information for developers. INFO Information relevant to server administrators. Creating, updating, and deleting files should be logged here. WARN Handled exceptions. A warning should be logged any time there is a possible problem administrators may need to investigate.. ERROR Unhandled exceptions. Problems that were not automatically handled.","title":"Logging Rules"},{"location":"committer/","text":"Committers and Contributors What is a committer, and how does that differ from a contributor? The Opencast project welcomes all those who are interested in contributing in concrete ways. Contributions can take many forms, and there are various mechanisms by which one can contribute (signing up for the mailing list or browsing our issue tracker are both great places to start). Anyone can become a contributor, and there is no expectation of future commitment to the project, no specific skill requirements, and no selection process. Some of the contributions individuals have given in the past include: Supporting new users (users are often the best people to support new users) Reporting bugs Identifying requirements Providing design and pedagogical direction Adding features and fixing bugs Assisting with project infrastructure Writing documentation Assisting in translation of both documentation and the system itself Performing Quality Assurance tasks as needed as part of the QA process prior to release As contributors gain experience and familiarity with the project they generally find that making such contributions becomes easier. With a sufficient level of participation and commitment to the project, contributors may be nominated by existing committers to join the committer body. While similar in day-to-day activities as contributors, committers have shown a dedicated interest in supporting the project over the longer term. Committership allows individuals to more easily carry on with their project related activities by giving them direct access to the projects resources. In Opencast, the project's resources include not only code, but also designs, documents, and other non-code resources. As such, we welcome and value committers both in the traditional sense (e.g. programmers) as well as in the broader sense (e.g. people who are committed to the project). The committer body is the group of individuals who drive, manage, and govern the product. They make releases, set policy, and determine the both the quality and feature-set of Opencast. Responsibilities of a committer Committers responsibilities are centered around two key principles, that there is a commitment to excellence and success, and that there is a commitment to being active and involved. These commitments are demonstrated through activities such as: Being involved in quality assurance of the source code, documentation, designs, and other project pieces. Being active and engaging in the broader community, including supporting adopters and providing prompt feedback. Knowing when a change is beyond your ability and asking for help. Keeping informed of project governance and direction. Fostering a collegial environment between the group of committers. Ensuring that incoming code adheres to our license requirements. Minor patches can be accepted without concern, but the main reviewer must ensure that a CLA (and CCLA) is in place prior to merging in the contribution. 20% of the time a committer invests to the project is to be allocated for general purposes, i.e. work not directly associated with the committer's individual or institutional concerns. All committers must signed a ICLA. The privileges and rights of being a committer do not come into effect until an ICLA has been received and processed. The ICLA, as well as the list of ICLA signatories, can be found here . Your employer will also need to sign a CCLA as well. How do I become a committer? Committers must be recommended by existing committers and pass a vote of the committer body. Committers proposing a contributor for commit rights should present a solid background of why this person would be a good fit, as well as their work history within the project. This history, baring special circumstances, should be significant and of a high quality. What happens if I stop contributing? Committer status can be lost through any of the methods below: Committer Emeritus : By being absent or inactive for a period of six months or more, or by request, a committer enters emeritus status. No vote is required to enter this state, and committer status can be restored just by making an inquiry to the project infrastructure group. While in emeritus, committers lose access to project resources (e.g. git write access), as well as lose the ability to vote and engage in discussion on the committers mailing list. Revocation of Commit Privileges : A committer may make a proposal to remove an individual from the committer body. Discussion and voting happen on the confidential committers mailing list, and the committer in question is not entitled to a vote (though they do get to participate in the discussion).","title":"Committers"},{"location":"committer/#committers-and-contributors","text":"","title":"Committers and Contributors"},{"location":"committer/#what-is-a-committer-and-how-does-that-differ-from-a-contributor","text":"The Opencast project welcomes all those who are interested in contributing in concrete ways. Contributions can take many forms, and there are various mechanisms by which one can contribute (signing up for the mailing list or browsing our issue tracker are both great places to start). Anyone can become a contributor, and there is no expectation of future commitment to the project, no specific skill requirements, and no selection process. Some of the contributions individuals have given in the past include: Supporting new users (users are often the best people to support new users) Reporting bugs Identifying requirements Providing design and pedagogical direction Adding features and fixing bugs Assisting with project infrastructure Writing documentation Assisting in translation of both documentation and the system itself Performing Quality Assurance tasks as needed as part of the QA process prior to release As contributors gain experience and familiarity with the project they generally find that making such contributions becomes easier. With a sufficient level of participation and commitment to the project, contributors may be nominated by existing committers to join the committer body. While similar in day-to-day activities as contributors, committers have shown a dedicated interest in supporting the project over the longer term. Committership allows individuals to more easily carry on with their project related activities by giving them direct access to the projects resources. In Opencast, the project's resources include not only code, but also designs, documents, and other non-code resources. As such, we welcome and value committers both in the traditional sense (e.g. programmers) as well as in the broader sense (e.g. people who are committed to the project). The committer body is the group of individuals who drive, manage, and govern the product. They make releases, set policy, and determine the both the quality and feature-set of Opencast.","title":"What is a committer, and how does that differ from a contributor?"},{"location":"committer/#responsibilities-of-a-committer","text":"Committers responsibilities are centered around two key principles, that there is a commitment to excellence and success, and that there is a commitment to being active and involved. These commitments are demonstrated through activities such as: Being involved in quality assurance of the source code, documentation, designs, and other project pieces. Being active and engaging in the broader community, including supporting adopters and providing prompt feedback. Knowing when a change is beyond your ability and asking for help. Keeping informed of project governance and direction. Fostering a collegial environment between the group of committers. Ensuring that incoming code adheres to our license requirements. Minor patches can be accepted without concern, but the main reviewer must ensure that a CLA (and CCLA) is in place prior to merging in the contribution. 20% of the time a committer invests to the project is to be allocated for general purposes, i.e. work not directly associated with the committer's individual or institutional concerns. All committers must signed a ICLA. The privileges and rights of being a committer do not come into effect until an ICLA has been received and processed. The ICLA, as well as the list of ICLA signatories, can be found here . Your employer will also need to sign a CCLA as well.","title":"Responsibilities of a committer"},{"location":"committer/#how-do-i-become-a-committer","text":"Committers must be recommended by existing committers and pass a vote of the committer body. Committers proposing a contributor for commit rights should present a solid background of why this person would be a good fit, as well as their work history within the project. This history, baring special circumstances, should be significant and of a high quality.","title":"How do I become a committer?"},{"location":"committer/#what-happens-if-i-stop-contributing","text":"Committer status can be lost through any of the methods below: Committer Emeritus : By being absent or inactive for a period of six months or more, or by request, a committer enters emeritus status. No vote is required to enter this state, and committer status can be restored just by making an inquiry to the project infrastructure group. While in emeritus, committers lose access to project resources (e.g. git write access), as well as lose the ability to vote and engage in discussion on the committers mailing list. Revocation of Commit Privileges : A committer may make a proposal to remove an individual from the committer body. Discussion and voting happen on the confidential committers mailing list, and the committer in question is not entitled to a vote (though they do get to participate in the discussion).","title":"What happens if I stop contributing?"},{"location":"development-environment-docker/","text":"Development Environment with Docker Setting up and maintaining a proper Opencast build environment can be challenging. The quay.io/opencast/build Docker image, developed by the University of M\u00fcnster, provides such a build environment already configured and ready to use. In fact, because of Docker's isolation functionality, multiple environments can be operated side by side on a single machine. Setting up a Docker build environment Two docker-compose files are provided to start up different development environments. You also need the ActiveMQ configuration (see \"Testing Locally with Docker\" guide in the administration documentation). First, download the support assets: $ mkdir assets $ curl -o assets/activemq.xml https://raw.githubusercontent.com/opencast/opencast-docker/<version>/docker-compose/assets/activemq.xml Now create a folder where the Opencast repository should be located, and expose its path as an environment variable. You must also create the local Maven repository if it does not already exist. $ mkdir -p opencast ~/.m2 $ export OPENCAST_SRC=$PWD/opencast The OPENCAST_SRC variable is used in the compose file to set up a Docker volume so that the host and Docker container can share the Opencast codebase. Similarly, the local Maven repository is shared in order to persist Maven artifacts beyond the lifetime of the Docker container. If you do not want to use the default path ~/.m2 you can set the M2_REPO variable to any other directory on the host system. Next, you should specify your UID and GID. A matching user will then be created within the container so that all new files can also be accessed from the host. If these variables remain unset, both default to 1000. $ export OPENCAST_BUILD_USER_UID=$(id -u) $ export OPENCAST_BUILD_USER_GID=$(id -g) Single node Opencast Development Now download the Docker compose file: $ curl -o docker-compose.yml https://raw.githubusercontent.com/opencast/opencast-docker/<version>/docker-compose/docker-compose.build.yml With this you are ready to start up the build environment: $ docker-compose up -d You can enter the Opencast build environment with the exec command. Omitting the --user opencast-builder argument would give you a root shell, but that is not necessary because the user opencast-builder can use sudo within the container. $ docker-compose exec --user opencast-builder opencast bash There are multiple helper scripts available within the container: # Clone the Opencast source code to the shared volume. $ oc_clone # Build Opencast. $ oc_build # Install Opencast in the same way as it would be installed in the other Opencast Docker images. $ oc_install <distribution> # Run the installed Opencast $ oc_run # Uninstall Opencast. $ oc_uninstall # Remove all Opencast files (database, media packages, etc.). $ oc_clean_data These scripts are provided to automate common tasks, but you can also run the necessary commands directly. The install script has the advantage that it automatically connects Opencast to the configured ActiveMQ instance available at tcp://activemq:61616 . Since the Opencast code is shared, any change from an IDE is directly visible within the container. Multi-node Opencast Development Development with multi-node Opencast environments are also supported using a different compose file. Instead of downloading the single-node Docker compose file, download the multi-node version: $ curl -o docker-compose.yml https://raw.githubusercontent.com/opencast/opencast-docker/<version>/docker-compose/docker-compose.multiserver.build.yml This file defines a three node (admin, presentation, worker) cluster for use in testing, with all of the appropriate ports exported. To access the a node run docker-compose exec --user opencast-builder opencast-$nodetype bash . For example, to access the presentation node run docker-compose exec --user opencast-builder opencast-presentation bash . Available commands are otherwise identical. Attaching a Remote Debugger to Karaf By default, the compose file sets the necessary variables to enable remote debugging. The network port is published by the container so that you can connect the remote debugger of your IDE to the port 5005 on localhost . For multi-node setups each node has its debug port exposed: admin lives on 5005 , presentation on 5006 , and worker on 5007 .","title":"With Docker"},{"location":"development-environment-docker/#development-environment-with-docker","text":"Setting up and maintaining a proper Opencast build environment can be challenging. The quay.io/opencast/build Docker image, developed by the University of M\u00fcnster, provides such a build environment already configured and ready to use. In fact, because of Docker's isolation functionality, multiple environments can be operated side by side on a single machine.","title":"Development Environment with Docker"},{"location":"development-environment-docker/#setting-up-a-docker-build-environment","text":"Two docker-compose files are provided to start up different development environments. You also need the ActiveMQ configuration (see \"Testing Locally with Docker\" guide in the administration documentation). First, download the support assets: $ mkdir assets $ curl -o assets/activemq.xml https://raw.githubusercontent.com/opencast/opencast-docker/<version>/docker-compose/assets/activemq.xml Now create a folder where the Opencast repository should be located, and expose its path as an environment variable. You must also create the local Maven repository if it does not already exist. $ mkdir -p opencast ~/.m2 $ export OPENCAST_SRC=$PWD/opencast The OPENCAST_SRC variable is used in the compose file to set up a Docker volume so that the host and Docker container can share the Opencast codebase. Similarly, the local Maven repository is shared in order to persist Maven artifacts beyond the lifetime of the Docker container. If you do not want to use the default path ~/.m2 you can set the M2_REPO variable to any other directory on the host system. Next, you should specify your UID and GID. A matching user will then be created within the container so that all new files can also be accessed from the host. If these variables remain unset, both default to 1000. $ export OPENCAST_BUILD_USER_UID=$(id -u) $ export OPENCAST_BUILD_USER_GID=$(id -g)","title":"Setting up a Docker build environment"},{"location":"development-environment-docker/#single-node-opencast-development","text":"Now download the Docker compose file: $ curl -o docker-compose.yml https://raw.githubusercontent.com/opencast/opencast-docker/<version>/docker-compose/docker-compose.build.yml With this you are ready to start up the build environment: $ docker-compose up -d You can enter the Opencast build environment with the exec command. Omitting the --user opencast-builder argument would give you a root shell, but that is not necessary because the user opencast-builder can use sudo within the container. $ docker-compose exec --user opencast-builder opencast bash There are multiple helper scripts available within the container: # Clone the Opencast source code to the shared volume. $ oc_clone # Build Opencast. $ oc_build # Install Opencast in the same way as it would be installed in the other Opencast Docker images. $ oc_install <distribution> # Run the installed Opencast $ oc_run # Uninstall Opencast. $ oc_uninstall # Remove all Opencast files (database, media packages, etc.). $ oc_clean_data These scripts are provided to automate common tasks, but you can also run the necessary commands directly. The install script has the advantage that it automatically connects Opencast to the configured ActiveMQ instance available at tcp://activemq:61616 . Since the Opencast code is shared, any change from an IDE is directly visible within the container.","title":"Single node Opencast Development"},{"location":"development-environment-docker/#multi-node-opencast-development","text":"Development with multi-node Opencast environments are also supported using a different compose file. Instead of downloading the single-node Docker compose file, download the multi-node version: $ curl -o docker-compose.yml https://raw.githubusercontent.com/opencast/opencast-docker/<version>/docker-compose/docker-compose.multiserver.build.yml This file defines a three node (admin, presentation, worker) cluster for use in testing, with all of the appropriate ports exported. To access the a node run docker-compose exec --user opencast-builder opencast-$nodetype bash . For example, to access the presentation node run docker-compose exec --user opencast-builder opencast-presentation bash . Available commands are otherwise identical.","title":"Multi-node Opencast Development"},{"location":"development-environment-docker/#attaching-a-remote-debugger-to-karaf","text":"By default, the compose file sets the necessary variables to enable remote debugging. The network port is published by the container so that you can connect the remote debugger of your IDE to the port 5005 on localhost . For multi-node setups each node has its debug port exposed: admin lives on 5005 , presentation on 5006 , and worker on 5007 .","title":"Attaching a Remote Debugger to Karaf"},{"location":"development-environment/","text":"Development Environment Developer Builds Besides the default dist Maven profile, the assemblies project defines a second dev profile which will cause only one allinone distribution to be created. It is already unpacked and ready to be started. Activate the profile using: mvn clean install -Pdev The administrative user interface needs nodejs to build and phantomjs for testing purposes. These will be downloaded as prebuilt binaries during the maven build process. If there are no prebuilt binaries for your operating system, you can build the tools manually and then build opencast using the frontend-no-prebuilt maven profile: mvn clean install -Pdev,frontend-no-prebuilt Logging During Builds While building Opencast, the default log level for Opencast modules is WARN . To increase logging for development, edit the log level configuration in docs/log4j/log4j.properties . Building single modules When working on a single Opencast module, it can be extremely helpful to watch the newly built version and include it automatically in the Opencast OSGi infrastructure. This can be done through the bundle:watch command in Karaf. The workflow would be as follows: Start Opencast and use la -u in the Karaf console to list all installed bundles/modules. Note down the IDs of the bundles you want to watch. Use bundle:watch IDs to watch the desired modules, e.g. bundle:watch 190 199 Make your changes and rebuild the module (e.g. execute mvn clean install in the module folder). Watch how Karaf automatically redeploys the changed jars from your local Maven repository. You can verify that everything went smoothly by checking the log with log:tail . To see this technique in action, you can watch the following short video: Opencast development: Watch and reload modules The updated bundles are only available in the currently running Karaf instance. To create a Opencast version that has this changes permanently, you have to run mvn clean install in the the assemblies directory again. Your current instance will be deleted by the new assembly! In several cases the bundle:watch can bring Karaf in an unstable condition, as dependencies between bundles will not correctly be restored, after the new bundle has been deployed. Attaching a Remote Debugger to Karaf To debug a running Opencast system, you can attach a remote debugger in your IDE (Eclipse or NetBeans, i.e.). For that you have to enable the remote debugging in Karaf OSGI server that runs Opencast. You have to add \"debug\" as an additional paramenter to the Opencast start script: bin/start-opencast debug If you want to enable debug permanently you can export the variable in the shell: export DEFAULT_JAVA_DEBUG_OPTS='-Xdebug -Xnoagent -Djava.compiler=NONE -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=5005' You can connect the remote debugger of your IDE on port 5005 . For more information on remote debugging with Karaf you can visit this site. It is not recommended to enable remote debugging on production systems!","title":"Overview"},{"location":"development-environment/#development-environment","text":"","title":"Development Environment"},{"location":"development-environment/#developer-builds","text":"Besides the default dist Maven profile, the assemblies project defines a second dev profile which will cause only one allinone distribution to be created. It is already unpacked and ready to be started. Activate the profile using: mvn clean install -Pdev The administrative user interface needs nodejs to build and phantomjs for testing purposes. These will be downloaded as prebuilt binaries during the maven build process. If there are no prebuilt binaries for your operating system, you can build the tools manually and then build opencast using the frontend-no-prebuilt maven profile: mvn clean install -Pdev,frontend-no-prebuilt","title":"Developer Builds"},{"location":"development-environment/#logging-during-builds","text":"While building Opencast, the default log level for Opencast modules is WARN . To increase logging for development, edit the log level configuration in docs/log4j/log4j.properties .","title":"Logging During Builds"},{"location":"development-environment/#building-single-modules","text":"When working on a single Opencast module, it can be extremely helpful to watch the newly built version and include it automatically in the Opencast OSGi infrastructure. This can be done through the bundle:watch command in Karaf. The workflow would be as follows: Start Opencast and use la -u in the Karaf console to list all installed bundles/modules. Note down the IDs of the bundles you want to watch. Use bundle:watch IDs to watch the desired modules, e.g. bundle:watch 190 199 Make your changes and rebuild the module (e.g. execute mvn clean install in the module folder). Watch how Karaf automatically redeploys the changed jars from your local Maven repository. You can verify that everything went smoothly by checking the log with log:tail . To see this technique in action, you can watch the following short video: Opencast development: Watch and reload modules The updated bundles are only available in the currently running Karaf instance. To create a Opencast version that has this changes permanently, you have to run mvn clean install in the the assemblies directory again. Your current instance will be deleted by the new assembly! In several cases the bundle:watch can bring Karaf in an unstable condition, as dependencies between bundles will not correctly be restored, after the new bundle has been deployed.","title":"Building single modules"},{"location":"development-environment/#attaching-a-remote-debugger-to-karaf","text":"To debug a running Opencast system, you can attach a remote debugger in your IDE (Eclipse or NetBeans, i.e.). For that you have to enable the remote debugging in Karaf OSGI server that runs Opencast. You have to add \"debug\" as an additional paramenter to the Opencast start script: bin/start-opencast debug If you want to enable debug permanently you can export the variable in the shell: export DEFAULT_JAVA_DEBUG_OPTS='-Xdebug -Xnoagent -Djava.compiler=NONE -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=5005' You can connect the remote debugger of your IDE on port 5005 . For more information on remote debugging with Karaf you can visit this site. It is not recommended to enable remote debugging on production systems!","title":"Attaching a Remote Debugger to Karaf"},{"location":"development-process/","text":"Development Process Development Process Contributing Code GitHub Accepting Criteria for Patches in Different Versions Reviews Pull Request Guidelines Git Repository Branching Model Release Process Preparations Release Schedule Release Branch Tags Maintenance Releases Quality Assurance Reporting Bugs Security Issues Unit Tests User Tests Test Server This document defines rules and recommendations for Opencast development. In particular, it defines how patches can be contributed, how they are merged and how releases are done. If this document does not answer all of your questions, here is how you can get further help: Ask on the Opencast Development List Chat with developers on IRC (#opencast on Freenode) Join our weekly technical meeting (see lists or IRC) Contributing Code Opencast sources can be found on GitHub . The easiest way to contribute code to the project is by creating a pull request against the project's official repository. More details about the structure of this repository are explained later in this guide. GitHub Opencast uses GitHub for tracking issues. Each pull request should be accompanied by a ticket in GitHub unless it is a very small fix. The issue identifier should also be in the description of the pull request, which will automatically close the issue (if any) when the PR is merged. See here for more details. Creating a GitHub issue is usually the first step when fixing something. Opencast uses GitHub for code hosting. Please fork the official repository on GitHub to create pull requests from your repository which will show up on the project's list of open pull requests. Accepting Criteria for Patches in Different Versions Updates between minor versions should be as smooth as possible and should usually not need manual intervention. That is why patches may only be accepted into releases branches ( r/?.x ) if they meet the following criteria: Patches must not modify existing database tables Patches must not modify the indexes or otherwise cause re-indexing Patches must not require a different ActiveMQ configuration Patches must not modify existing translation keys Patches must work with the same configuration within a major version Patches which do not meet these criteria should target the branch develop to become part of the next major version. To determine the acceptance of patches, all pull requests will be discussed in the technical meeting. This protects against inclusion of controversial changes with no broader consent among committers. Reviews Before a patch is merged, it needs to be reviewed. The reviewer tries to make sure that the patch merges without conflicts, that it works as expected and that it does not break anything else. If the reviewer discovers any kind of issue, he should comment on the pull request in GitHub, so that the author can fix the problem. For more details about the review and merge process, have a look at Reviewing, Merging and Declining Pull Requests . Pull Request Guidelines When reviewing a pull request, it is always easier if the reviewer knows what the ticket is about, and has a rough idea of what work has been done. To this end, there are a few expectations for all pull requests: The GitHub issue title should match the pull request title The pull request description should contain a summary of the work done, along with reasoning for any major change The GitHub issue should contain the same information Pull request should include appropriate documentation The pull request should have a clean commit history In the case of major user interface changes, it is good practice to include screenshots of the change Any actions that would be required for a version upgrade (e.g: from 3.x to 4.x) must be documented in docs/guides/admin/docs/upgrade.md The commands mvn clean install , mvn javadoc:javadoc javadoc:aggregate , and mvn site should all succeed The licenses of any external libraries used in the pull request comply with the licensing rules both in terms of the license itself as well as its listing in NOTICES While a committer may accept a patch even if it does not meet these expectations, it is encouraged that anyone filing a pull request ensures that they meet these expectations. Git Repository Branching Model While the Opencast repository and branching model is inspired by GitFlow , there have been some distinct changes to how release branches are used and releases are tagged. The purpose of this is mainly to support multiple, simultaneous versions and maintenance releases. Swift overview: The develop branch represents the latest state of development. Features may be merged into this branch and into this branch only. Release branches are branched off from develop . It is basically the preparation for the next big release at all times. The release branches are named r/<a>.x (e.g. r/6.x ). They are the latest state of development for a specific major version. All minor releases are created from these branches. The branches live on as long as there may be additional maintenance releases for a given version. Git tags in the form of a.b are created to indicate official releases. To get a closer look at the branching model, let us consider a simple example with a single release: As described above, develop is the branch used for preparing the next version. At some point marked in the release schedule, the release branch is cut from develop . This action also marks the feature freeze for that version since features may be merged only into the develop branch. After the release branch is cut, the development on the develop branch may continue as before. Features can (and should) be merged without waiting for the next version to be released. Thus, the creation of a release branch also marks the beginning of the development for the next version. In contrast to that, only bug fixes may be merged into the release branch. This branch should be tested with care, so that bugs can be identified and fixed before the release. During the whole process the release manager will regularly merge back the release branch into develop or, if existent, the next active release branch. The releases themselves are not part of the release branch. Instead, the release manager branches off, makes the necessary changes to the pom files (and possibly the UI) and creates a separately tagged commit. Finally, after a release is done, more bug fixes may be added to the release branch. The release manager should identify if there are enough commits to be put into a maintenance release. Even after an Opencast version has been released, more bugs may be found and fixes for them merged into the release branch. When the release manager considers that the number or importance of such bug fixes is sufficient, he may decide to create a new maintenance release. The version 6.1 above is an example of that. With Opencast supporting two major releases, you may find not one, but up to three active release branches. Mostly, this is just the same as the simpler model from before. The branches exist separately from each other and only interact through merges from older to newer versions so that bug fixes from a release branch will automatically become part of the next Opencast versions (and develop ), without having to create additional pull requests. For example, a pull request may be merged into r/7.x , r/7.x will then be merged into develop or, if it already exists, r/8.x and from there into develop . That way patches bubble through all newer versions and finally end up in develop . Release Process As indicated above, the release cycle of a new Opencast version starts when a release branch is cut. Patches merged into develop after the cut will be part of the next version, but not the one just cut. This is why the position of release manager for the next Opencast version should be assigned at this point. The current release manager should therefore ask for volunteers in the mailing lists. For more details about the rights and responsibilities of a release manager, please have a look at the Release Manager Guide . Preparations The first phase of the release consists of adding new features and defining the release schedule. It is the duty of the release manager to orchestrate this. This does not necessarily mean that release managers merge or review pull requests, but that they talk to developers and ensure the merge process is driven forward. Release Schedule Releases should happen twice a year, usually within a time span of 9.5 months between the cut of the previous release branch and the final release. The release manager should create a release schedule as soon as possible, identifying when the release branch is cut and when the final release will happen. Additionally, he should coordinate with the QA manager to identify phases for internal and public testing. Usually, a release schedule will look like this: Date Phase May 15th Feature Freeze May 24th Translation week May 31st Public QA phase June 15th Release of Opencast 7.0 Release Branch The release branch is created from develop . The release branch is named r/A.x (e.g. r/7.x ) to indicate that it is the origin of all releases with the major version of A . The creation of the release branch marks the feature freeze for a given version, as no more features can be merged into a release branch. To ensure that all fixes that go into the release branch will become part of develop (and thus part of the next version of Opencast) with a minimum amount of work, the release manager will merge the release branch into develop on a regular basis. He may request assistance from certain developers in case of merge conflicts. This process continues until the next release branch is cut. Tags Git tags are used to mark Opencast releases. Here is how a release looks like in the history: To create a version based on a given state of the release branch (commit A ), the release manager will branch off from this commit, make the necessary version changes to all pom.xml files and create a commit which is then finally tagged. This tag is then pushed to the community repository. For more details about how to create a release, have a look at the Release Manager Guide . Maintenance Releases After a final release, additional issues may show up. These issues may be fixed on the ongoing release branch and at some point released as maintenance release. Maintenance releases will be cut monthly for the latest stable release. For legacy releases, it is up to the release manager to decide when it is worthwhile to make the cut. Quality Assurance As any piece of software, Opencast may contain bugs. It is the duty of the whole community to identify these bugs, report them and possibly fix them to improve Opencast as product. Additionally, before releasing a new version of Opencast, the current release manager and quality assurance manager will coordinate test phases dedicated to new releases in order to identify possible problems ahead of time. The whole community will be requested to participate in this testing. Reporting Bugs If you identify any bugs, please report them on Github !. Please make sure to describe in detail how to reproduce the problem, and which version of Opencast you are experiencing the issue on. Security Issues If you discover a problem that has severe implications for system security, please do not publish this information on list. Instead, send a report of the problem to security@opencast.org . The message will be forwarded to the private committers list, where the issue will be discussed. Once a patch for the problem is ready, a security notice will be released along with it. Unit Tests All Opencast modules should have built-in unit tests to check that they are actually doing what they are supposed to do and that code patches do not break the existing functionality. These tests are automatically run whenever the project is built. If building repeatedly fails due to test failures, then something is most likely wrong. Please report this as a severe bug. User Tests Before each major release, the release and quality assurance managers will ask the whole community to participate in the execution of a set of manual tests. These tests are designed to check that important functionalities of Opencast work as expected even if users are in slightly different environments or choose different methods to achieve a certain goal. Such a call for participation will usually be raised both on the lists, the technical and the adopters meeting. If it is possible for you to participate, please do so. Identifying possible problems early will immensely benefit the release process. Test Server Some institutions provide public testing infrastructure for Opencast. Use them to try out the most recent development version of Opencast. They are meant for testing. Do not fear to break them. They are meant for testing. For a list of test servers, take a look at the infrastructure documentation .","title":"Development"},{"location":"development-process/#development-process","text":"Development Process Contributing Code GitHub Accepting Criteria for Patches in Different Versions Reviews Pull Request Guidelines Git Repository Branching Model Release Process Preparations Release Schedule Release Branch Tags Maintenance Releases Quality Assurance Reporting Bugs Security Issues Unit Tests User Tests Test Server This document defines rules and recommendations for Opencast development. In particular, it defines how patches can be contributed, how they are merged and how releases are done. If this document does not answer all of your questions, here is how you can get further help: Ask on the Opencast Development List Chat with developers on IRC (#opencast on Freenode) Join our weekly technical meeting (see lists or IRC)","title":"Development Process"},{"location":"development-process/#contributing-code","text":"Opencast sources can be found on GitHub . The easiest way to contribute code to the project is by creating a pull request against the project's official repository. More details about the structure of this repository are explained later in this guide.","title":"Contributing Code"},{"location":"development-process/#github","text":"Opencast uses GitHub for tracking issues. Each pull request should be accompanied by a ticket in GitHub unless it is a very small fix. The issue identifier should also be in the description of the pull request, which will automatically close the issue (if any) when the PR is merged. See here for more details. Creating a GitHub issue is usually the first step when fixing something. Opencast uses GitHub for code hosting. Please fork the official repository on GitHub to create pull requests from your repository which will show up on the project's list of open pull requests.","title":"GitHub"},{"location":"development-process/#accepting-criteria-for-patches-in-different-versions","text":"Updates between minor versions should be as smooth as possible and should usually not need manual intervention. That is why patches may only be accepted into releases branches ( r/?.x ) if they meet the following criteria: Patches must not modify existing database tables Patches must not modify the indexes or otherwise cause re-indexing Patches must not require a different ActiveMQ configuration Patches must not modify existing translation keys Patches must work with the same configuration within a major version Patches which do not meet these criteria should target the branch develop to become part of the next major version. To determine the acceptance of patches, all pull requests will be discussed in the technical meeting. This protects against inclusion of controversial changes with no broader consent among committers.","title":"Accepting Criteria for Patches in Different Versions"},{"location":"development-process/#reviews","text":"Before a patch is merged, it needs to be reviewed. The reviewer tries to make sure that the patch merges without conflicts, that it works as expected and that it does not break anything else. If the reviewer discovers any kind of issue, he should comment on the pull request in GitHub, so that the author can fix the problem. For more details about the review and merge process, have a look at Reviewing, Merging and Declining Pull Requests .","title":"Reviews"},{"location":"development-process/#pull-request-guidelines","text":"When reviewing a pull request, it is always easier if the reviewer knows what the ticket is about, and has a rough idea of what work has been done. To this end, there are a few expectations for all pull requests: The GitHub issue title should match the pull request title The pull request description should contain a summary of the work done, along with reasoning for any major change The GitHub issue should contain the same information Pull request should include appropriate documentation The pull request should have a clean commit history In the case of major user interface changes, it is good practice to include screenshots of the change Any actions that would be required for a version upgrade (e.g: from 3.x to 4.x) must be documented in docs/guides/admin/docs/upgrade.md The commands mvn clean install , mvn javadoc:javadoc javadoc:aggregate , and mvn site should all succeed The licenses of any external libraries used in the pull request comply with the licensing rules both in terms of the license itself as well as its listing in NOTICES While a committer may accept a patch even if it does not meet these expectations, it is encouraged that anyone filing a pull request ensures that they meet these expectations.","title":"Pull Request Guidelines"},{"location":"development-process/#git-repository-branching-model","text":"While the Opencast repository and branching model is inspired by GitFlow , there have been some distinct changes to how release branches are used and releases are tagged. The purpose of this is mainly to support multiple, simultaneous versions and maintenance releases. Swift overview: The develop branch represents the latest state of development. Features may be merged into this branch and into this branch only. Release branches are branched off from develop . It is basically the preparation for the next big release at all times. The release branches are named r/<a>.x (e.g. r/6.x ). They are the latest state of development for a specific major version. All minor releases are created from these branches. The branches live on as long as there may be additional maintenance releases for a given version. Git tags in the form of a.b are created to indicate official releases. To get a closer look at the branching model, let us consider a simple example with a single release: As described above, develop is the branch used for preparing the next version. At some point marked in the release schedule, the release branch is cut from develop . This action also marks the feature freeze for that version since features may be merged only into the develop branch. After the release branch is cut, the development on the develop branch may continue as before. Features can (and should) be merged without waiting for the next version to be released. Thus, the creation of a release branch also marks the beginning of the development for the next version. In contrast to that, only bug fixes may be merged into the release branch. This branch should be tested with care, so that bugs can be identified and fixed before the release. During the whole process the release manager will regularly merge back the release branch into develop or, if existent, the next active release branch. The releases themselves are not part of the release branch. Instead, the release manager branches off, makes the necessary changes to the pom files (and possibly the UI) and creates a separately tagged commit. Finally, after a release is done, more bug fixes may be added to the release branch. The release manager should identify if there are enough commits to be put into a maintenance release. Even after an Opencast version has been released, more bugs may be found and fixes for them merged into the release branch. When the release manager considers that the number or importance of such bug fixes is sufficient, he may decide to create a new maintenance release. The version 6.1 above is an example of that. With Opencast supporting two major releases, you may find not one, but up to three active release branches. Mostly, this is just the same as the simpler model from before. The branches exist separately from each other and only interact through merges from older to newer versions so that bug fixes from a release branch will automatically become part of the next Opencast versions (and develop ), without having to create additional pull requests. For example, a pull request may be merged into r/7.x , r/7.x will then be merged into develop or, if it already exists, r/8.x and from there into develop . That way patches bubble through all newer versions and finally end up in develop .","title":"Git Repository Branching Model"},{"location":"development-process/#release-process","text":"As indicated above, the release cycle of a new Opencast version starts when a release branch is cut. Patches merged into develop after the cut will be part of the next version, but not the one just cut. This is why the position of release manager for the next Opencast version should be assigned at this point. The current release manager should therefore ask for volunteers in the mailing lists. For more details about the rights and responsibilities of a release manager, please have a look at the Release Manager Guide .","title":"Release Process"},{"location":"development-process/#preparations","text":"The first phase of the release consists of adding new features and defining the release schedule. It is the duty of the release manager to orchestrate this. This does not necessarily mean that release managers merge or review pull requests, but that they talk to developers and ensure the merge process is driven forward.","title":"Preparations"},{"location":"development-process/#release-schedule","text":"Releases should happen twice a year, usually within a time span of 9.5 months between the cut of the previous release branch and the final release. The release manager should create a release schedule as soon as possible, identifying when the release branch is cut and when the final release will happen. Additionally, he should coordinate with the QA manager to identify phases for internal and public testing. Usually, a release schedule will look like this: Date Phase May 15th Feature Freeze May 24th Translation week May 31st Public QA phase June 15th Release of Opencast 7.0","title":"Release Schedule"},{"location":"development-process/#release-branch","text":"The release branch is created from develop . The release branch is named r/A.x (e.g. r/7.x ) to indicate that it is the origin of all releases with the major version of A . The creation of the release branch marks the feature freeze for a given version, as no more features can be merged into a release branch. To ensure that all fixes that go into the release branch will become part of develop (and thus part of the next version of Opencast) with a minimum amount of work, the release manager will merge the release branch into develop on a regular basis. He may request assistance from certain developers in case of merge conflicts. This process continues until the next release branch is cut.","title":"Release Branch"},{"location":"development-process/#tags","text":"Git tags are used to mark Opencast releases. Here is how a release looks like in the history: To create a version based on a given state of the release branch (commit A ), the release manager will branch off from this commit, make the necessary version changes to all pom.xml files and create a commit which is then finally tagged. This tag is then pushed to the community repository. For more details about how to create a release, have a look at the Release Manager Guide .","title":"Tags"},{"location":"development-process/#maintenance-releases","text":"After a final release, additional issues may show up. These issues may be fixed on the ongoing release branch and at some point released as maintenance release. Maintenance releases will be cut monthly for the latest stable release. For legacy releases, it is up to the release manager to decide when it is worthwhile to make the cut.","title":"Maintenance Releases"},{"location":"development-process/#quality-assurance","text":"As any piece of software, Opencast may contain bugs. It is the duty of the whole community to identify these bugs, report them and possibly fix them to improve Opencast as product. Additionally, before releasing a new version of Opencast, the current release manager and quality assurance manager will coordinate test phases dedicated to new releases in order to identify possible problems ahead of time. The whole community will be requested to participate in this testing.","title":"Quality Assurance"},{"location":"development-process/#reporting-bugs","text":"If you identify any bugs, please report them on Github !. Please make sure to describe in detail how to reproduce the problem, and which version of Opencast you are experiencing the issue on.","title":"Reporting Bugs"},{"location":"development-process/#security-issues","text":"If you discover a problem that has severe implications for system security, please do not publish this information on list. Instead, send a report of the problem to security@opencast.org . The message will be forwarded to the private committers list, where the issue will be discussed. Once a patch for the problem is ready, a security notice will be released along with it.","title":"Security Issues"},{"location":"development-process/#unit-tests","text":"All Opencast modules should have built-in unit tests to check that they are actually doing what they are supposed to do and that code patches do not break the existing functionality. These tests are automatically run whenever the project is built. If building repeatedly fails due to test failures, then something is most likely wrong. Please report this as a severe bug.","title":"Unit Tests"},{"location":"development-process/#user-tests","text":"Before each major release, the release and quality assurance managers will ask the whole community to participate in the execution of a set of manual tests. These tests are designed to check that important functionalities of Opencast work as expected even if users are in slightly different environments or choose different methods to achieve a certain goal. Such a call for participation will usually be raised both on the lists, the technical and the adopters meeting. If it is possible for you to participate, please do so. Identifying possible problems early will immensely benefit the release process.","title":"User Tests"},{"location":"development-process/#test-server","text":"Some institutions provide public testing infrastructure for Opencast. Use them to try out the most recent development version of Opencast. They are meant for testing. Do not fear to break them. They are meant for testing. For a list of test servers, take a look at the infrastructure documentation .","title":"Test Server"},{"location":"documentation/","text":"Documentation One important measurement for code quality is its documentation. This is especially the case for open source projects that count on the support of external developers and a strong community around the code base. With that in mind, here are some of the goals you should keep in mind when working within the Opencast codebase. Package Level Documentation Package documentation should cover the general concepts that are being used throughout the package, along with the principal interfaces and classes that are being used. A good example is provided by the Java 6 sql package description . Where does the documentation go? The package information should be put into a file called package-info.java and reside in the root of the package. Generally, the package-info is a java class itself consisting of documentation and package declaration only. Documentation can be found here . Checklist In order to achieve decent package documentation, make sure your documentation answers the following questions: What problem domain does your package deal with? Are there any concepts that the developer should understand? What are the different strategies that your package might implement? What are the entry points into your package in terms of interfaces and classes? What best practices are there to use the package? What major requirements need to be met before being able to use the package? Is there related external documentation (rfc, specifications) that should be linked? Classes By looking at the documentation of a class or an interface, the developer should have a clear idea as to how the class can be used, what pitfalls there might be in terms of threading, performance etc. Again, a good example can be found in the Url class of the Java Development Kit. Documentation Body Has a one-sentence description been given on what the class does or describe? Is there a description of concepts that should be known to users of the class? Are there possible pitfalls in the area of threading, transactions or performance? Are any other issues covered that might arise when using the class? Have any constraints be documented, like special behaviour of the equals() method? Are best practices on how to use the class documented? Tags Are there related classes or external resources that should be linked using @see ? Methods As with type documentation, the notes on a method should consist of instructions on what to expect in terms of behaviour when the method is called. Also, the user should be informed about any constraints that might apply to the objects state, the format of input parameters etc. As an example, take a look at the constructor for the URL class of the Java Development Kit. Documentation Body In general, what does the method do? Are there any requirements regarding the format of the input parameters? What does the method return with respect to input parameter values or state? If applicable, are there any constraints regarding an objects state? How are special parameter values (e.g. null ) handled? Does the user know what exceptions are thrown under which circumstances? Tags Are the input parameters documented using the @param tag? Is the return value documented using the @return tag? Are there related methods that should be linked using @see ? Are the exceptions documented with @throws ? REST Endpoints All Opencast REST endpoints must display HTML formatted documentation at the /docs path. Please be sure that your documentation includes: An entry for each path pattern, categorized as read and write paths. Descriptions for each path pattern, including: The HTTP method ( GET , POST , PUT , DELETE , \u2026) Required and optional parameters The expected input, including data formats and headers The expected output, including data formats, headers, and HTTP response codes","title":"Documentation"},{"location":"documentation/#documentation","text":"One important measurement for code quality is its documentation. This is especially the case for open source projects that count on the support of external developers and a strong community around the code base. With that in mind, here are some of the goals you should keep in mind when working within the Opencast codebase.","title":"Documentation"},{"location":"documentation/#package-level-documentation","text":"Package documentation should cover the general concepts that are being used throughout the package, along with the principal interfaces and classes that are being used. A good example is provided by the Java 6 sql package description .","title":"Package Level Documentation"},{"location":"documentation/#where-does-the-documentation-go","text":"The package information should be put into a file called package-info.java and reside in the root of the package. Generally, the package-info is a java class itself consisting of documentation and package declaration only. Documentation can be found here .","title":"Where does the documentation go?"},{"location":"documentation/#checklist","text":"In order to achieve decent package documentation, make sure your documentation answers the following questions: What problem domain does your package deal with? Are there any concepts that the developer should understand? What are the different strategies that your package might implement? What are the entry points into your package in terms of interfaces and classes? What best practices are there to use the package? What major requirements need to be met before being able to use the package? Is there related external documentation (rfc, specifications) that should be linked?","title":"Checklist"},{"location":"documentation/#classes","text":"By looking at the documentation of a class or an interface, the developer should have a clear idea as to how the class can be used, what pitfalls there might be in terms of threading, performance etc. Again, a good example can be found in the Url class of the Java Development Kit.","title":"Classes"},{"location":"documentation/#documentation-body","text":"Has a one-sentence description been given on what the class does or describe? Is there a description of concepts that should be known to users of the class? Are there possible pitfalls in the area of threading, transactions or performance? Are any other issues covered that might arise when using the class? Have any constraints be documented, like special behaviour of the equals() method? Are best practices on how to use the class documented?","title":"Documentation Body"},{"location":"documentation/#tags","text":"Are there related classes or external resources that should be linked using @see ?","title":"Tags"},{"location":"documentation/#methods","text":"As with type documentation, the notes on a method should consist of instructions on what to expect in terms of behaviour when the method is called. Also, the user should be informed about any constraints that might apply to the objects state, the format of input parameters etc. As an example, take a look at the constructor for the URL class of the Java Development Kit.","title":"Methods"},{"location":"documentation/#documentation-body_1","text":"In general, what does the method do? Are there any requirements regarding the format of the input parameters? What does the method return with respect to input parameter values or state? If applicable, are there any constraints regarding an objects state? How are special parameter values (e.g. null ) handled? Does the user know what exceptions are thrown under which circumstances?","title":"Documentation Body"},{"location":"documentation/#tags_1","text":"Are the input parameters documented using the @param tag? Is the return value documented using the @return tag? Are there related methods that should be linked using @see ? Are the exceptions documented with @throws ?","title":"Tags"},{"location":"documentation/#rest-endpoints","text":"All Opencast REST endpoints must display HTML formatted documentation at the /docs path. Please be sure that your documentation includes: An entry for each path pattern, categorized as read and write paths. Descriptions for each path pattern, including: The HTTP method ( GET , POST , PUT , DELETE , \u2026) Required and optional parameters The expected input, including data formats and headers The expected output, including data formats, headers, and HTTP response codes","title":"REST Endpoints"},{"location":"governance/","text":"Opencast Governance Updated October 2017 The Opencast community is a collaboration of individuals, higher education institutions and other organizations working together to explore, discuss, define, develop and document best practices and technologies for the management of audiovisual content in academia. The community shares experience with existing technologies and practices, identifies future requirements and collaborates towards possible solutions. To this end, the Opencast community supports community-driven projects to solve common issues in managing academic video by developing respective open source software products. Today, the community\u2019s main software product is Opencast, an open source software system for lecture capture and video management. At the same time, there are other, technically related or non-related projects originating from the community (LectureSight, Paella Player, pyCA etc.). While the latter will be self-determined and autonomous (and have their own project governance therefore), they are part of the Opencast governance in that the Opencast board seeks to make them flourish as part of a healthy Opencast ecosystem. The governance of the Opencast community is stipulated in this document, with detailed expectations and practices of the governing bodies, i.e. the board and committers. Collectively, the governance structure has four key purposes: Ensure the sustainability and health of the community and its projects. Support growth in the form of new adopters, contributors, committers and resources. Support effective development and maintenance of Opencast. Allow both the community and the projects to achieve their goals of enhancing the open source management of audiovisual content in academia. In order to address the full range of governance the document includes three segments: Community governance Project governance Coordination and review, which span both the community and the projects. 1. Opencast community governance \u2013 the board The community governance centres on a board elected by the Opencast community that is designed to represent the will of the community while providing leadership to support community health long-term. The board is composed of three categories of individuals, each with a unique selection process and term of service: A minimum of three members who are elected by the community to serve a two-year term. One member selected by the board to serve a two-year term based on his/her ability to address current needs of the community and/or the board; this seat is optional. One member elected by the committers to serve a one-year term (has to be a committer). Board elections to appoint at least three members will be held every other year at the occasion of the annual community gathering (\u201cOpencast summit\u201d). They will be managed by one board member and one external individual from the community invited by the board. This election committee will communicate the upcoming election at least one month before the summit, ask current board members whether they would like to continue their term, invite community members interested in joining the board to nominate themselves for election, identify and implement an online voting process to elect individual members, and communicate nominees and stipulate the voting procedure in situ and/or online. Each nominee will be voted upon individually with a \u201cyes\u201d or \u201cno\u201d; nominees with more \u201cyes\u201d votes than \u201cno\u201d votes are being elected to serve the board for another two years. The board is being voted upon by individuals representing the community, defined by their subscription to the Opencast users\u2019 mailing list (users@opencast.org); board members have a right to participate in these elections. The election committee will also verify the right to vote, count all votes, communicate which board members have been elected, and keep the detailed voting results confidential. It is the responsibility of the board to promote the current and future health of the community. This will require the board to assess the current state and identify areas of focus. Regardless of changing needs, the board has the following ongoing responsibilities. Drive strategic planning for Opencast. Actively engage with institutions, organisations, and individuals forming the community. Draw new organizations into the community. Identify and execute processes to secure needed resources of any kind, funding, staff, etc. Act on behalf of the Opencast community in creating partnerships that serve the goals of the community and/or its projects. Actively participate in community communications activities and community building efforts; in particular, the board is to organize the annual Opencast summit. Select and invite those board members that require nomination and selection by the board. Drive and monitor relevant communication channels (website, social media). Effectively manage financial contributions made to the community as well as expenditures; the board has to report on both at the annual meeting. The board is not responsible for making project governance decisions, nor for dictating priorities to the committers. The primary focus areas of the board will shift over time with the evolving needs of the community. As such, board members must have a strong commitment to the success of the community and both the willingness and the ability to fill multiple roles based on the needs of the community. 2. Project governance \u2013 contributors and committers Projects thrive and prosper thanks to the contributions made by individuals: Anyone taking an interest in Opencast can become a contributor therefore, simply by participating in discussions, reporting bugs, editing documentation, supporting new users, fixing bugs, or enhancing the project\u2019s health in other forms. With a sufficient level of participation and commitment to the project, contributors may be nominated by existing committers to join the committer body according to the governance process described below; this is the project\u2019s interpretation of meritocracy . Committership allows individuals to participate in the product-related decision-making process as the committer body is driving, managing, and governing the product. They make releases and determine the quality and feature-set of Opencast. We define a committer as an individual contributing actively to the project in a way that deserves participation in the product-related decision-making process. As responsibilities of a committer , this entails contributing time to support the common good of the project, supporting the review and release process, quality assurance and testing processes for the project as stipulated in https://docs.opencast.org/latest/developer, being active and engaging in the broader community, including supporting adopters and providing prompt feedback, and fostering a collegial environment between the group of committers. All committers must sign an individual licence agreement (ICLA), the institution they represent a corporate contributor license agreement (CCLA); details are fleshed out at https://www.apereo.org/licensing/agreements . The privileges and rights of being a committer do not come into effect until the agreements have been received and processed. In return, committers have the following rights : Make changes to the Opencast source code and other key resources keeping in mind the responsibilities outlined above as well as the working culture of the committer body. Put forth proposals to the committer body to influence project direction and participate in resulting decision-making processes according to the project governance stipulated below. Committers agree to the following approach to project governance: Project business will be conducted on an open mail list (dev@opencast.org). Committer proposals will be voted upon according to https://docs.opencast.org/latest/developer/proposal-log/. Details of this decision-making process can be fleshed out by committers adhering to 1-2 above. By being absent or inactive for a period of six months or more, or by request, a committer enters emeritus status. No vote is required to enter this state, and committer status can be restored just by making an inquiry to the project infrastructure group. While in emeritus, committers lose access to project resources, as well as lose the ability to vote and engage in discussion on the committers mailing list. Emeritus committers can become active again upon request, and do not require any additional work prior to reactivation. Revocation of Commit Privileges. A committer may make a proposal to remove an individual from the committer body. Discussion and voting happen on the confidential committers mailing list, and the committer in question is not entitled to a vote (though they do get to participate in the discussion). 3. Coordinating and Review Process It is important to note that the board does not own the content of the review, but is responsible for ensuring that the review occurs and for facilitating a process that effectively engages committers and community members. To this end, the community will be called by the board to convene annually (\u201cOpencast summit\u201d). During these meetings the community will: Share updates that span projects and community status and progress. Review status and progress of the projects and the community as well as governance processes with the objective of validating that existing processes are serving the project and community well, or enhancing the processes to improve results. Review the project roadmap, discuss their vision for the product, thus bringing together the views of committers and the broader Opencast community. Communicate and seek to align community and development priorities. Capture future development and collaboration opportunities beyond the current roadmap. All discussion and recommended decisions resulting from the coordinating and review process will follow the defined governance processes and must be communicated on list for greater participation and feedback.","title":"Governance"},{"location":"governance/#opencast-governance","text":"Updated October 2017 The Opencast community is a collaboration of individuals, higher education institutions and other organizations working together to explore, discuss, define, develop and document best practices and technologies for the management of audiovisual content in academia. The community shares experience with existing technologies and practices, identifies future requirements and collaborates towards possible solutions. To this end, the Opencast community supports community-driven projects to solve common issues in managing academic video by developing respective open source software products. Today, the community\u2019s main software product is Opencast, an open source software system for lecture capture and video management. At the same time, there are other, technically related or non-related projects originating from the community (LectureSight, Paella Player, pyCA etc.). While the latter will be self-determined and autonomous (and have their own project governance therefore), they are part of the Opencast governance in that the Opencast board seeks to make them flourish as part of a healthy Opencast ecosystem. The governance of the Opencast community is stipulated in this document, with detailed expectations and practices of the governing bodies, i.e. the board and committers. Collectively, the governance structure has four key purposes: Ensure the sustainability and health of the community and its projects. Support growth in the form of new adopters, contributors, committers and resources. Support effective development and maintenance of Opencast. Allow both the community and the projects to achieve their goals of enhancing the open source management of audiovisual content in academia. In order to address the full range of governance the document includes three segments: Community governance Project governance Coordination and review, which span both the community and the projects.","title":"Opencast Governance"},{"location":"governance/#1-opencast-community-governance-the-board","text":"The community governance centres on a board elected by the Opencast community that is designed to represent the will of the community while providing leadership to support community health long-term. The board is composed of three categories of individuals, each with a unique selection process and term of service: A minimum of three members who are elected by the community to serve a two-year term. One member selected by the board to serve a two-year term based on his/her ability to address current needs of the community and/or the board; this seat is optional. One member elected by the committers to serve a one-year term (has to be a committer). Board elections to appoint at least three members will be held every other year at the occasion of the annual community gathering (\u201cOpencast summit\u201d). They will be managed by one board member and one external individual from the community invited by the board. This election committee will communicate the upcoming election at least one month before the summit, ask current board members whether they would like to continue their term, invite community members interested in joining the board to nominate themselves for election, identify and implement an online voting process to elect individual members, and communicate nominees and stipulate the voting procedure in situ and/or online. Each nominee will be voted upon individually with a \u201cyes\u201d or \u201cno\u201d; nominees with more \u201cyes\u201d votes than \u201cno\u201d votes are being elected to serve the board for another two years. The board is being voted upon by individuals representing the community, defined by their subscription to the Opencast users\u2019 mailing list (users@opencast.org); board members have a right to participate in these elections. The election committee will also verify the right to vote, count all votes, communicate which board members have been elected, and keep the detailed voting results confidential. It is the responsibility of the board to promote the current and future health of the community. This will require the board to assess the current state and identify areas of focus. Regardless of changing needs, the board has the following ongoing responsibilities. Drive strategic planning for Opencast. Actively engage with institutions, organisations, and individuals forming the community. Draw new organizations into the community. Identify and execute processes to secure needed resources of any kind, funding, staff, etc. Act on behalf of the Opencast community in creating partnerships that serve the goals of the community and/or its projects. Actively participate in community communications activities and community building efforts; in particular, the board is to organize the annual Opencast summit. Select and invite those board members that require nomination and selection by the board. Drive and monitor relevant communication channels (website, social media). Effectively manage financial contributions made to the community as well as expenditures; the board has to report on both at the annual meeting. The board is not responsible for making project governance decisions, nor for dictating priorities to the committers. The primary focus areas of the board will shift over time with the evolving needs of the community. As such, board members must have a strong commitment to the success of the community and both the willingness and the ability to fill multiple roles based on the needs of the community.","title":"1. Opencast community governance \u2013 the board"},{"location":"governance/#2-project-governance-contributors-and-committers","text":"Projects thrive and prosper thanks to the contributions made by individuals: Anyone taking an interest in Opencast can become a contributor therefore, simply by participating in discussions, reporting bugs, editing documentation, supporting new users, fixing bugs, or enhancing the project\u2019s health in other forms. With a sufficient level of participation and commitment to the project, contributors may be nominated by existing committers to join the committer body according to the governance process described below; this is the project\u2019s interpretation of meritocracy . Committership allows individuals to participate in the product-related decision-making process as the committer body is driving, managing, and governing the product. They make releases and determine the quality and feature-set of Opencast. We define a committer as an individual contributing actively to the project in a way that deserves participation in the product-related decision-making process. As responsibilities of a committer , this entails contributing time to support the common good of the project, supporting the review and release process, quality assurance and testing processes for the project as stipulated in https://docs.opencast.org/latest/developer, being active and engaging in the broader community, including supporting adopters and providing prompt feedback, and fostering a collegial environment between the group of committers. All committers must sign an individual licence agreement (ICLA), the institution they represent a corporate contributor license agreement (CCLA); details are fleshed out at https://www.apereo.org/licensing/agreements . The privileges and rights of being a committer do not come into effect until the agreements have been received and processed. In return, committers have the following rights : Make changes to the Opencast source code and other key resources keeping in mind the responsibilities outlined above as well as the working culture of the committer body. Put forth proposals to the committer body to influence project direction and participate in resulting decision-making processes according to the project governance stipulated below. Committers agree to the following approach to project governance: Project business will be conducted on an open mail list (dev@opencast.org). Committer proposals will be voted upon according to https://docs.opencast.org/latest/developer/proposal-log/. Details of this decision-making process can be fleshed out by committers adhering to 1-2 above. By being absent or inactive for a period of six months or more, or by request, a committer enters emeritus status. No vote is required to enter this state, and committer status can be restored just by making an inquiry to the project infrastructure group. While in emeritus, committers lose access to project resources, as well as lose the ability to vote and engage in discussion on the committers mailing list. Emeritus committers can become active again upon request, and do not require any additional work prior to reactivation. Revocation of Commit Privileges. A committer may make a proposal to remove an individual from the committer body. Discussion and voting happen on the confidential committers mailing list, and the committer in question is not entitled to a vote (though they do get to participate in the discussion).","title":"2. Project governance \u2013 contributors and committers"},{"location":"governance/#3-coordinating-and-review-process","text":"It is important to note that the board does not own the content of the review, but is responsible for ensuring that the review occurs and for facilitating a process that effectively engages committers and community members. To this end, the community will be called by the board to convene annually (\u201cOpencast summit\u201d). During these meetings the community will: Share updates that span projects and community status and progress. Review status and progress of the projects and the community as well as governance processes with the objective of validating that existing processes are serving the project and community well, or enhancing the processes to improve results. Review the project roadmap, discuss their vision for the product, thus bringing together the views of committers and the broader Opencast community. Communicate and seek to align community and development priorities. Capture future development and collaboration opportunities beyond the current roadmap. All discussion and recommended decisions resulting from the coordinating and review process will follow the defined governance processes and must be communicated on list for greater participation and feedback.","title":"3. Coordinating and Review Process"},{"location":"license/","text":"Licenses and Legal Matters This is a guide from developers for developers and therefore not an official legal document. We try to be as accurate as possible but cannot guarantee that there are no mistakes. If in doubt, please send a question to the Opencast developer mailing list, to the Opencast Board or to the Apereo Foundation, preferably in that order. Which Licenses May I Use for Opencast? All libraries that are used in Opencast need to be compatible with the Educational Community License version 2.0. In short, if the license is on: the Apereo or Apache Category-A list, it is fine to use the library. the Apereo or Apache Category-B list, we may use it unmodified. Please avoid Category-B if possible. the Apereo or Apache Category-X list, we cannot use it. Everything else needs to go to the Apereo foundation for approval. These lists, along with a more detailed explanation, can be found on the Apereo page about practices on third-party licenses Here is the slightly longer section from the \u201eApereo Licensing & Intellectual Property Practices\u201c page: How To Label Third Party Libraries Third party libraries that are used within an Opencast system in production need to be listed in the NOTICES file, which can be found in the root directory of our code repository. Libraries and tools that are only used for testing or building the project, or are run via system call, do not need to be listed (e.g. maven, maven plugins, junit, ...). JavaScript Libraries For JavaScript libraries, list the file or folder that is included and give a short statement about the copyright and license. This can usually be taken from the copyright header at the top of the library. Example: modules/admin-ui/src/main/webapp/lib/angular/* AngularJS v1.3.6 (c) 2010-2014 Google, Inc. http://angularjs.org License: MIT Java Libraries / Maven Dependencies Java dependencies are listed pre module. Maven provides some helpful tools to list dependencies and even report libraries. Have a look at the output of mvn dependency:list and mvn dependency:tree or generate a full report for a module using: mvn -s settings.xml project-info-reports:dependencies This will create a file target/site/dependencies.html containing a full report, including the library versions and licenses. Finally, add the library and license to the NOTICES file in the form: rn-workflow-service-remote GroupId ArtifactId License commons-io commons-io The Apache Software License, Version 2.0","title":"Licenses and Legal Matters"},{"location":"license/#licenses-and-legal-matters","text":"This is a guide from developers for developers and therefore not an official legal document. We try to be as accurate as possible but cannot guarantee that there are no mistakes. If in doubt, please send a question to the Opencast developer mailing list, to the Opencast Board or to the Apereo Foundation, preferably in that order.","title":"Licenses and Legal Matters"},{"location":"license/#which-licenses-may-i-use-for-opencast","text":"All libraries that are used in Opencast need to be compatible with the Educational Community License version 2.0. In short, if the license is on: the Apereo or Apache Category-A list, it is fine to use the library. the Apereo or Apache Category-B list, we may use it unmodified. Please avoid Category-B if possible. the Apereo or Apache Category-X list, we cannot use it. Everything else needs to go to the Apereo foundation for approval. These lists, along with a more detailed explanation, can be found on the Apereo page about practices on third-party licenses Here is the slightly longer section from the \u201eApereo Licensing & Intellectual Property Practices\u201c page:","title":"Which Licenses May I Use for Opencast?"},{"location":"license/#how-to-label-third-party-libraries","text":"Third party libraries that are used within an Opencast system in production need to be listed in the NOTICES file, which can be found in the root directory of our code repository. Libraries and tools that are only used for testing or building the project, or are run via system call, do not need to be listed (e.g. maven, maven plugins, junit, ...).","title":"How To Label Third Party Libraries"},{"location":"license/#javascript-libraries","text":"For JavaScript libraries, list the file or folder that is included and give a short statement about the copyright and license. This can usually be taken from the copyright header at the top of the library. Example: modules/admin-ui/src/main/webapp/lib/angular/* AngularJS v1.3.6 (c) 2010-2014 Google, Inc. http://angularjs.org License: MIT","title":"JavaScript Libraries"},{"location":"license/#java-libraries-maven-dependencies","text":"Java dependencies are listed pre module. Maven provides some helpful tools to list dependencies and even report libraries. Have a look at the output of mvn dependency:list and mvn dependency:tree or generate a full report for a module using: mvn -s settings.xml project-info-reports:dependencies This will create a file target/site/dependencies.html containing a full report, including the library versions and licenses. Finally, add the library and license to the NOTICES file in the form: rn-workflow-service-remote GroupId ArtifactId License commons-io commons-io The Apache Software License, Version 2.0","title":"Java Libraries / Maven Dependencies"},{"location":"local-cluster/","text":"Configuring a Local Cluster for Testing A list of commands to quickly build and configure a local Opencast cluster. First, build and extract distributions: mvn clean install cd build tar xf opencast-dist-admin-10-SNAPSHOT.tar.gz tar xf opencast-dist-presentation-10-SNAPSHOT.tar.gz tar xf opencast-dist-worker-10-SNAPSHOT.tar.gz Then configure a share storage space: ln -s ../../opencast-dist-admin/data/opencast opencast-dist-presentation/data/opencast ln -s ../../opencast-dist-admin/data/opencast opencast-dist-worker/data/opencast Configure different network ports ( 8080 -> admin , 8081 -> presentation , 8082 -> worker ) for the distributions: sed -i 's/8080/8081/' opencast-dist-presentation/etc/org.ops4j.pax.web.cfg sed -i 's/8080/8081/' opencast-dist-presentation/etc/custom.properties sed -i 's/8080/8082/' opencast-dist-worker/etc/org.ops4j.pax.web.cfg sed -i 's/8080/8082/' opencast-dist-worker/etc/custom.properties sed -i 's_#prop.org.opencastproject.engage.ui.url=.*$_prop.org.opencastproject.engage.ui.url=http://localhost:8081_' */etc/org.opencastproject.organization-mh_default_org.cfg sed -i 's_#prop.org.opencastproject.admin.ui.url=.*$_prop.org.opencastproject.admin.ui.url=http://localhost:8080_' */etc/org.opencastproject.organization-mh_default_org.cfg Configure a MariaDB database: sed -i 's/#org.opencastproject.db.jdbc.driver/org.opencastproject.db.jdbc.driver/' */etc/custom.properties sed -i 's/#org.opencastproject.db.jdbc.url/org.opencastproject.db.jdbc.url/' */etc/custom.properties sed -i 's/#org.opencastproject.db.jdbc.user/org.opencastproject.db.jdbc.user/' */etc/custom.properties sed -i 's/#org.opencastproject.db.jdbc.pass/org.opencastproject.db.jdbc.pass/' */etc/custom.properties Start Opencast: cd opencast-dist-admin ./bin/start-opencast cd opencast-dist-presentation ./bin/start-opencast cd opencast-dist-worker ./bin/start-opencast","title":"Configure a Local Cluster for Testing"},{"location":"local-cluster/#configuring-a-local-cluster-for-testing","text":"A list of commands to quickly build and configure a local Opencast cluster. First, build and extract distributions: mvn clean install cd build tar xf opencast-dist-admin-10-SNAPSHOT.tar.gz tar xf opencast-dist-presentation-10-SNAPSHOT.tar.gz tar xf opencast-dist-worker-10-SNAPSHOT.tar.gz Then configure a share storage space: ln -s ../../opencast-dist-admin/data/opencast opencast-dist-presentation/data/opencast ln -s ../../opencast-dist-admin/data/opencast opencast-dist-worker/data/opencast Configure different network ports ( 8080 -> admin , 8081 -> presentation , 8082 -> worker ) for the distributions: sed -i 's/8080/8081/' opencast-dist-presentation/etc/org.ops4j.pax.web.cfg sed -i 's/8080/8081/' opencast-dist-presentation/etc/custom.properties sed -i 's/8080/8082/' opencast-dist-worker/etc/org.ops4j.pax.web.cfg sed -i 's/8080/8082/' opencast-dist-worker/etc/custom.properties sed -i 's_#prop.org.opencastproject.engage.ui.url=.*$_prop.org.opencastproject.engage.ui.url=http://localhost:8081_' */etc/org.opencastproject.organization-mh_default_org.cfg sed -i 's_#prop.org.opencastproject.admin.ui.url=.*$_prop.org.opencastproject.admin.ui.url=http://localhost:8080_' */etc/org.opencastproject.organization-mh_default_org.cfg Configure a MariaDB database: sed -i 's/#org.opencastproject.db.jdbc.driver/org.opencastproject.db.jdbc.driver/' */etc/custom.properties sed -i 's/#org.opencastproject.db.jdbc.url/org.opencastproject.db.jdbc.url/' */etc/custom.properties sed -i 's/#org.opencastproject.db.jdbc.user/org.opencastproject.db.jdbc.user/' */etc/custom.properties sed -i 's/#org.opencastproject.db.jdbc.pass/org.opencastproject.db.jdbc.pass/' */etc/custom.properties Start Opencast: cd opencast-dist-admin ./bin/start-opencast cd opencast-dist-presentation ./bin/start-opencast cd opencast-dist-worker ./bin/start-opencast","title":"Configuring a Local Cluster for Testing"},{"location":"localization/","text":"Localization Introduction The Opencast project uses the Crowdin Localization Management Platform for translating Opencast into a variety of languages. The English translation (en-US) acts as source language that is translated to other languages using Crowdin. While all translation files are located in the Opencast code repository, only the English translation should be modified in the code repository - all other translation files are downloaded from Crowdin. Important: All translation files for languages other than English (en-US) are downloaded from Crowdin. Modifications to the translation files in the Opencast code repository will be regularly overwritten and therefore will be lost! Note that Crowdin managers take care of uploading the English sources (and possibly translations) to Crowdin and download the others translations from Crowdin. I would like Opencast to support my language. Is this possible? Yes, absolutely! If you are willing to take the effort to provide the translation, we are happy to include your favorite language in Opencast! How can I provide a language translation? We use the Crowdin Localization Management Platform - an easy to use web service for localization management. To provide a language translation, please perform the following steps: Create a free account on Crowdin Visit the Opencast project on Crowdin and issue a join request Translate Opencast on Crowdin Once the translation reaches at least 90% (prefarable 100%), please read the section about include and exclusion of translations just below. In case you have questions, we are happy to answer them on the Opencast Users mailing list. Inclusion and Exclusion of Translations Opencast supports a number of languages right out-of-the-box. Please find the criteria for inclusion and exlusion of language translations in Opencast releases below: A not yet supported translation is included into the next major release if it is translated at least 90% at the time when the release branch is cut. The release managers will take the review if no other reviewer can be found. A not yet supported translation may be included in the current release branch anytime if it is translated to 100% and a reviewer is found. It will then be part of the next minor release and major release if feasible An endangered translation is a supported translation that is translated less than 80% at the time when the release branch of the next major release is cut. The release managers will publish a list of endangered languages if any An endangered translation will be removed with the next major release if it is not saved. The release managers take care of the removal in case no other person will An endangered translation may be saved by reaching at least 90% translated until at least two weeks before the release date of the next major release and a reviewer is found Note that Crowdin is displaying the percentage translated for each language. It is the percentages shown on that page that act as reference. Considering the dates when releases branch are cut, the respective releases schedules act as reference. Crowdin Management And Administration Crowdin managers are persons with privileged access to Crowdin needed to upload new files to be translated to Crowdin. The rest of document should help future Crowdin managers to get familiar with Crowdin quickly. Accepting Translators We ask that Crowdin users who wish to help translate Opencast send a brief, understandable sentence regarding why they wish to help translate Opencast. Users who do not send this in should be asked via the Crowdin messaging system. Something as simple as 'I want to help translate $project into [language]' would more than suffice. Versioning Crowdin supports versions management by allowing the management of multiple branches. The relation of Opencast code repository branches to Crowdin branches follows the following convention: The Opencast branch r/a.x corresponds to Crowdin branch a.x . Crowdin does automatically detect equal strings across branches so there is no need to configure anything when a new branch is created. Right after a cut of a new Opencast release branch r/a.x (from develop ), the following actions must be performed: Download the translations from Crowdin branch develop Commit the downloaded translations into the Opencast branch r/a.x Respect the rules described in the section Inclusion and Exclusion of Translations Important: Do not upload translations to Crowdin! To keep the Opencast code repository in sync with Crowdin, the translation sources should be uploaded to Crowdin as soon as possible. Working with Crowdin CLI The Crowdin CLI command line tool is used to synchronize the source language files and translations between the Opencast code repository and the Crowdin project Opencast. The Crowdin CLI configuration can be found in /.crowdin.yaml Please perform the following steps to get the tool running on your local host: Install Crowdin CLI tool Get the API key for the project Opencast Add the following line to your local Crowdin configuration file ( ~/.crowdin.yaml ): the first line: api_key: <secret key> Now you can use the Crowdin CLI command line tool to upload source language files and download translations. To upload the translation sources from the Opencast code repository to Crowdin, use the following command: crowdin --config .crowdin.yaml upload sources -b <branch> Note that the branch <branch> will be automatically created if it is not yet existing. To download the translations from Crowdin, use the following command: crowdin --config .crowdin.yaml download -b <branch> To show a list of files in the current project, use the following command: crowdin --config .crowdin.yaml list project -b <branch> Note: Do not upload translations to Crowdin. Our current subscription plan will exceeded otherwise. To consider when adding a translation Add a SVG with the flag under modules/admin-ui-frontend/app/img/lang/ Add a case to the switch-case in modules/engage-theodul-core/src/main/resources/ui/js/engage/core.js modules/engage-ui/src/main/resources/ui/js/app/engage-ui.js Further Information Crowdin Opencast Project Crowdin CLI Documentation Crowdin Versions Management Documentation Crowdin Language Codes","title":"Localization"},{"location":"localization/#localization","text":"","title":"Localization"},{"location":"localization/#introduction","text":"The Opencast project uses the Crowdin Localization Management Platform for translating Opencast into a variety of languages. The English translation (en-US) acts as source language that is translated to other languages using Crowdin. While all translation files are located in the Opencast code repository, only the English translation should be modified in the code repository - all other translation files are downloaded from Crowdin. Important: All translation files for languages other than English (en-US) are downloaded from Crowdin. Modifications to the translation files in the Opencast code repository will be regularly overwritten and therefore will be lost! Note that Crowdin managers take care of uploading the English sources (and possibly translations) to Crowdin and download the others translations from Crowdin.","title":"Introduction"},{"location":"localization/#i-would-like-opencast-to-support-my-language-is-this-possible","text":"Yes, absolutely! If you are willing to take the effort to provide the translation, we are happy to include your favorite language in Opencast!","title":"I would like Opencast to support my language. Is this possible?"},{"location":"localization/#how-can-i-provide-a-language-translation","text":"We use the Crowdin Localization Management Platform - an easy to use web service for localization management. To provide a language translation, please perform the following steps: Create a free account on Crowdin Visit the Opencast project on Crowdin and issue a join request Translate Opencast on Crowdin Once the translation reaches at least 90% (prefarable 100%), please read the section about include and exclusion of translations just below. In case you have questions, we are happy to answer them on the Opencast Users mailing list.","title":"How can I provide a language translation?"},{"location":"localization/#inclusion-and-exclusion-of-translations","text":"Opencast supports a number of languages right out-of-the-box. Please find the criteria for inclusion and exlusion of language translations in Opencast releases below: A not yet supported translation is included into the next major release if it is translated at least 90% at the time when the release branch is cut. The release managers will take the review if no other reviewer can be found. A not yet supported translation may be included in the current release branch anytime if it is translated to 100% and a reviewer is found. It will then be part of the next minor release and major release if feasible An endangered translation is a supported translation that is translated less than 80% at the time when the release branch of the next major release is cut. The release managers will publish a list of endangered languages if any An endangered translation will be removed with the next major release if it is not saved. The release managers take care of the removal in case no other person will An endangered translation may be saved by reaching at least 90% translated until at least two weeks before the release date of the next major release and a reviewer is found Note that Crowdin is displaying the percentage translated for each language. It is the percentages shown on that page that act as reference. Considering the dates when releases branch are cut, the respective releases schedules act as reference.","title":"Inclusion and Exclusion of Translations"},{"location":"localization/#crowdin-management-and-administration","text":"Crowdin managers are persons with privileged access to Crowdin needed to upload new files to be translated to Crowdin. The rest of document should help future Crowdin managers to get familiar with Crowdin quickly.","title":"Crowdin Management And Administration"},{"location":"localization/#accepting-translators","text":"We ask that Crowdin users who wish to help translate Opencast send a brief, understandable sentence regarding why they wish to help translate Opencast. Users who do not send this in should be asked via the Crowdin messaging system. Something as simple as 'I want to help translate $project into [language]' would more than suffice.","title":"Accepting Translators"},{"location":"localization/#versioning","text":"Crowdin supports versions management by allowing the management of multiple branches. The relation of Opencast code repository branches to Crowdin branches follows the following convention: The Opencast branch r/a.x corresponds to Crowdin branch a.x . Crowdin does automatically detect equal strings across branches so there is no need to configure anything when a new branch is created. Right after a cut of a new Opencast release branch r/a.x (from develop ), the following actions must be performed: Download the translations from Crowdin branch develop Commit the downloaded translations into the Opencast branch r/a.x Respect the rules described in the section Inclusion and Exclusion of Translations Important: Do not upload translations to Crowdin! To keep the Opencast code repository in sync with Crowdin, the translation sources should be uploaded to Crowdin as soon as possible.","title":"Versioning"},{"location":"localization/#working-with-crowdin-cli","text":"The Crowdin CLI command line tool is used to synchronize the source language files and translations between the Opencast code repository and the Crowdin project Opencast. The Crowdin CLI configuration can be found in /.crowdin.yaml Please perform the following steps to get the tool running on your local host: Install Crowdin CLI tool Get the API key for the project Opencast Add the following line to your local Crowdin configuration file ( ~/.crowdin.yaml ): the first line: api_key: <secret key> Now you can use the Crowdin CLI command line tool to upload source language files and download translations. To upload the translation sources from the Opencast code repository to Crowdin, use the following command: crowdin --config .crowdin.yaml upload sources -b <branch> Note that the branch <branch> will be automatically created if it is not yet existing. To download the translations from Crowdin, use the following command: crowdin --config .crowdin.yaml download -b <branch> To show a list of files in the current project, use the following command: crowdin --config .crowdin.yaml list project -b <branch> Note: Do not upload translations to Crowdin. Our current subscription plan will exceeded otherwise.","title":"Working with Crowdin CLI"},{"location":"localization/#to-consider-when-adding-a-translation","text":"Add a SVG with the flag under modules/admin-ui-frontend/app/img/lang/ Add a case to the switch-case in modules/engage-theodul-core/src/main/resources/ui/js/engage/core.js modules/engage-ui/src/main/resources/ui/js/app/engage-ui.js","title":"To consider when adding a translation"},{"location":"localization/#further-information","text":"Crowdin Opencast Project Crowdin CLI Documentation Crowdin Versions Management Documentation Crowdin Language Codes","title":"Further Information"},{"location":"packaging/","text":"Packaging Guidelines This page is intended as a guideline for packagers. It may help to figure out where to place parts of Opencast. The locations, etc. proposed here should never overrule the official packaging guides for a specific operating system or distribution. If in doubt follow the guides for your distribution like for example the Fedora Packaging Guidelines Introduction In a Unix file system there are different places for different types of data. Executables, for example, may be placed in /usr/bin , libraries in /usr/lib , etc. These places are defined by the operating system distributor and the Filesystem Hierarchy Standard . Latter is followed by almost every major distributor, but not everything in there is clearly defined. Especially software which is installed automatically\u2013for example software from RPM or DEB repositories\u2013should follow these rules so conflicts are minimized and the user will have one place to look for one kind of data. For example if you are searching for a system-wide configuration file for any software on Linux every user will always look in /etc . If you want to package Opencast use the following documentations to decide where to place files: Distribution guidelines like the Fedora Packaging Guidelines Filesystem Hierarchy Standard This Guide Locations To Use The following locations should be used for Opencast and its related data: /usr/share/opencast : Software and data not modified by Opencast. This includes felix, the Opencast modules and external libraries. /etc/opencast : Opencast related configuration files (Felix and service configuration, workflows, encoding profiles, etc.) /var/log/opencast : The Opencast logfiles. Consider to enable logrotate for this directory. /srv/opencast or /var/lib/opencast : Opencast storage, including the recordings, the archive, the Solr indexes, etc. You may use one of these directories or both. For more details have a look at the explanation below and the discussion in the comments. /tmp/opencast : Temporary data which are not necessarily preserved between reboots. This includes the felix-cache and other temporary data. /usr/sbin/opencast : Opencast startscript /etc/init.d/opencast SysV-Initscript (if necessary) Reasoning for these Locations /usr/share/opencast \u2013 Opencast Software Components The Filesystem Hierarchy Standard states that \u201c The /usr/share hierarchy is for all read-only architecture independent data files. \u201d and that \u201c Any program or package which contains or requires data that does not need to be modified should store that data in /usr/share \u201d. It is also used for this purpose by cups, emacs, cmake, pulseaudio, gimp, \u2026 It sould be used for felix.jar and all the modules (lib directory) /etc/opencast \u2013 Opencast Configuration The Filesystem Hierarchy Standard states that \u201c The /etc hierarchy contains configuration files. A \"configuration file\" is a local file used to control the operation of a program; it must be static and cannot be an executable binary. \u201d /var/log/opencast/ \u2013 Opencast Logs The Filesystem Hierarchy Standard states that \u201c This directory contains miscellaneous log files. Most logs must be written to this directory or an appropriate subdirectory. \u201d /srv/opencast and/or /var/lib/opencast/ \u2013 Data modified by Opencast About this the Filesystem Hierarchy Standard says that \u201c This hierarchy holds state information pertaining to an application or the system. State information is data that programs modify while they run, \u2026 \u201d also \u201c /var/lib/ is the location that must be used for all distribution packaging support\u2026 \u201d Why Not Use /opt For Packages While it is ok to place software in /opt if you install the manually as /opt is intended to be used for \u201c Add-on application software \u201d by the Filesystem Hierarchy Standard, it should never be used for automatic installations (RPMs Debian packages, \u2026).. The Fedora Packaging Guidelines for example are pretty clear about this: \u201c*No Files or Directories under /srv, /opt, or /usr/local [\u2026] In addition, no Fedora package can have any files or directories under /opt or /usr/local, as these directories are not permitted to be used by Distributions in the FHS. The reason for this is that the FHS is handing control of the directory structure under /opt to the system administrator by stating that \u201c Distributions [\u2026] must not modify or delete software installed by the local system administrator \u2026 \u201d. That is something you cannot guarantee with automatic installations. For example if you use RPMs, the only way to do this would be to mark every single file (binaries, modules, assets, \u2026) as configuration files which are not to be replaced in case they are modified. It is quite obvious that this would be a a really bad idea leading to a number of further problems. Notice For System Operators This guide is supposed to defines default locations for an Opencast system. It does not restrict your own system configuration. For a Opencast system it is for example quite common to mount an external storage (NFS, \u2026) and use it as storage for Opencast. You do not have to mount it to /var/lib/opencast if you do not want to. Instead, mount it in /media or wherever you want\u2013it is your system afterall\u2013and either change the Opencast configuration to use the directory of your directly, or put appropriate symlinks in /var/lib/opencast . This is, however, system specific and should not be done for packages.","title":"Packaging"},{"location":"packaging/#packaging-guidelines","text":"This page is intended as a guideline for packagers. It may help to figure out where to place parts of Opencast. The locations, etc. proposed here should never overrule the official packaging guides for a specific operating system or distribution. If in doubt follow the guides for your distribution like for example the Fedora Packaging Guidelines","title":"Packaging Guidelines"},{"location":"packaging/#introduction","text":"In a Unix file system there are different places for different types of data. Executables, for example, may be placed in /usr/bin , libraries in /usr/lib , etc. These places are defined by the operating system distributor and the Filesystem Hierarchy Standard . Latter is followed by almost every major distributor, but not everything in there is clearly defined. Especially software which is installed automatically\u2013for example software from RPM or DEB repositories\u2013should follow these rules so conflicts are minimized and the user will have one place to look for one kind of data. For example if you are searching for a system-wide configuration file for any software on Linux every user will always look in /etc . If you want to package Opencast use the following documentations to decide where to place files: Distribution guidelines like the Fedora Packaging Guidelines Filesystem Hierarchy Standard This Guide","title":"Introduction"},{"location":"packaging/#locations-to-use","text":"The following locations should be used for Opencast and its related data: /usr/share/opencast : Software and data not modified by Opencast. This includes felix, the Opencast modules and external libraries. /etc/opencast : Opencast related configuration files (Felix and service configuration, workflows, encoding profiles, etc.) /var/log/opencast : The Opencast logfiles. Consider to enable logrotate for this directory. /srv/opencast or /var/lib/opencast : Opencast storage, including the recordings, the archive, the Solr indexes, etc. You may use one of these directories or both. For more details have a look at the explanation below and the discussion in the comments. /tmp/opencast : Temporary data which are not necessarily preserved between reboots. This includes the felix-cache and other temporary data. /usr/sbin/opencast : Opencast startscript /etc/init.d/opencast SysV-Initscript (if necessary)","title":"Locations To Use"},{"location":"packaging/#reasoning-for-these-locations","text":"","title":"Reasoning for these Locations"},{"location":"packaging/#usrshareopencast-opencast-software-components","text":"The Filesystem Hierarchy Standard states that \u201c The /usr/share hierarchy is for all read-only architecture independent data files. \u201d and that \u201c Any program or package which contains or requires data that does not need to be modified should store that data in /usr/share \u201d. It is also used for this purpose by cups, emacs, cmake, pulseaudio, gimp, \u2026 It sould be used for felix.jar and all the modules (lib directory)","title":"/usr/share/opencast \u2013 Opencast Software Components"},{"location":"packaging/#etcopencast-opencast-configuration","text":"The Filesystem Hierarchy Standard states that \u201c The /etc hierarchy contains configuration files. A \"configuration file\" is a local file used to control the operation of a program; it must be static and cannot be an executable binary. \u201d","title":"/etc/opencast \u2013 Opencast Configuration"},{"location":"packaging/#varlogopencast-opencast-logs","text":"The Filesystem Hierarchy Standard states that \u201c This directory contains miscellaneous log files. Most logs must be written to this directory or an appropriate subdirectory. \u201d","title":"/var/log/opencast/ \u2013 Opencast Logs"},{"location":"packaging/#srvopencast-andor-varlibopencast-data-modified-by-opencast","text":"About this the Filesystem Hierarchy Standard says that \u201c This hierarchy holds state information pertaining to an application or the system. State information is data that programs modify while they run, \u2026 \u201d also \u201c /var/lib/ is the location that must be used for all distribution packaging support\u2026 \u201d","title":"/srv/opencast and/or /var/lib/opencast/ \u2013 Data modified by Opencast"},{"location":"packaging/#why-not-use-opt-for-packages","text":"While it is ok to place software in /opt if you install the manually as /opt is intended to be used for \u201c Add-on application software \u201d by the Filesystem Hierarchy Standard, it should never be used for automatic installations (RPMs Debian packages, \u2026).. The Fedora Packaging Guidelines for example are pretty clear about this: \u201c*No Files or Directories under /srv, /opt, or /usr/local [\u2026] In addition, no Fedora package can have any files or directories under /opt or /usr/local, as these directories are not permitted to be used by Distributions in the FHS. The reason for this is that the FHS is handing control of the directory structure under /opt to the system administrator by stating that \u201c Distributions [\u2026] must not modify or delete software installed by the local system administrator \u2026 \u201d. That is something you cannot guarantee with automatic installations. For example if you use RPMs, the only way to do this would be to mark every single file (binaries, modules, assets, \u2026) as configuration files which are not to be replaced in case they are modified. It is quite obvious that this would be a a really bad idea leading to a number of further problems.","title":"Why Not Use /opt For Packages"},{"location":"packaging/#notice-for-system-operators","text":"This guide is supposed to defines default locations for an Opencast system. It does not restrict your own system configuration. For a Opencast system it is for example quite common to mount an external storage (NFS, \u2026) and use it as storage for Opencast. You do not have to mount it to /var/lib/opencast if you do not want to. Instead, mount it in /media or wherever you want\u2013it is your system afterall\u2013and either change the Opencast configuration to use the directory of your directly, or put appropriate symlinks in /var/lib/opencast . This is, however, system specific and should not be done for packages.","title":"Notice For System Operators"},{"location":"proposal-log/","text":"Opencast Proposals All important decisions for Opencast have to be made on list. For more details, please have a look at out documentation about decision making . The following list contains a list of passed proposals for reference. Passed Proposals Release Process Update Proposed by Lars Kiesow lkiesow@uos.de , passed on Thu, 24 Dec 2020 Hi everyone, based on the discussion on list and in the technical meeting, Lukas and I have created a proposal for changing our release process and applying a new set of rules. You might notice that we have deliberately not included all of what we discussed but hopefully only what can get consensus for now. Apart from some minor changes, the main difference in here is that we transition from making a distinction between feature and bug fix to defining a set of rules that hopefully ensure smooth minor updates. If this proposal passes, we can work for there and refine this but we hope that this is a good starting point. ## Release Schedule - Major releases happen every half year - Minor stable releases are cut monthly monthly - Minor legacy releases are cut on demand - Urgent minor releases may be cut if necessary - Feature freeze for a major version should be about a month before the release ## Accepting patches for minor releases - All patches need to be discussed in the technical meeting - Minor changes should not take much time - Protects against problematic changes in minor releases - Exceptions may be discussed if necessary (e.g. version bumps for libraries on develop) - Patches for minor releases must not - Modify any existing database tables - Modify the indexes or otherwise cause re-indexing - Require a different ActiveMQ configuration - Modify existing translations - Patches for minor releases must - Work with the same configuration within a major version ## Be Pragmatic - If everything is broken, don't let rules hold you back\u2026 - There is always a special case: If in doubt talk to the community and find a solution that works. - Communication is key: If you think you need to break a rule, coordinate that with the community. ## Additional suggestions - Patches should be applied to the latest release branch if possible - Avoid the risk of breaking legacy due to less testing - Avoid unnecessary merge conflicts - Patches should avoid unnecessarily large changes in any release branch As usual, if no one objects, this proposal passes in three days. Best regards, Lars Relocate build infrastructure to main repository Proposed by Lars Kiesow lkiesow@uos.de , passed on Thu, 17 Dec 2020 Hi everyone, as you are all aware there are multiple community members working on and maintaining infrastructure for different binary builds. For example, Greg is doing the Debian packages. All these are currently maintained in separate repositories\u2026 somewhere. Greg and I would like to bring these back to the main repositories to not loose track of what's available and where things are living. But maintaining these sometimes requires quick actions and we cannot really create a pull request every time a release is cut and then wait a week or two before it's merged until we can release the packaged version. That is why I #propose to allow for maintainers of these areas to work on these directly without running through our usual pull request and review rules. For example, that could mean that I could quickly update the RPM spec file if necessary while I would not be allowed to just modify the codebase (e.g. modules/*) on my own. Initially, I #propose the following areas and maintainers: Debian build architecture Greg Logan RPM build architecture Lars Kiesow docs.opencast.org build architecture Lars Kiesow This doesn't really change anything at the moment since we are already in control of these parts right now. The difference would be that it's not maintained somewhere any longer but everyone knows where to find things and (potentially) how contribute. Of course, we can extend this whenever we need to to include more/less scripts. Though I suggest that we keep this to what we officially support. As usual, this proposal passes unless someone vetos it in the next 72h. Best regards, Lars JDK Support Proposed by Greg Logan gregorydlogan@gmail.com , passed on Wed, 11 Nov 2020 Hi all, We have some old rules[1] around which JDK is supported by which version of Opencast, but obviously they are somewhat out of date at this point. With that in mind, here's what I'm proposing: Opencast 8: Formal support for JDK 8, and nothing else Opencast 9: Formal support for JDKs 8 and 11, with a provisional \"it should work\" for newer versions[2] Opencast 10: Formal support for JDK 11, with a provisional \"it should work\" for newer versions Going forward, I would like to see a given version of Opencast support the latest two LTS JDKs as a general rule. #proposal passes if no objections are raised by EOD 2020-11-11 G 1: https://docs.opencast.org/develop/developer/#proposal-log/#requiring-java-18-for-30 2: Note that JDK 11 is currently broken, at least until https://github.com/opencast/opencast/pull/2009 gets merged OSGi Annotations and Configuration Proposed by Lars Kiesow lkiesow@uos.de , passed on Wed, 7 Oct 2020 Hi everyone, I hereby #propose that for all new code added to opencast - OSGi annotations should be used instead of XML files for specifying components and dependencies. Reason: This is far less error prone. Even on today's Review Friday, we had problems with the XML files again. No wonder if you write these manually. - @Activate and @Modified should be used for reading and updating configuration instead of implementing a Managed Service. Reason: Managed services are usually unnecessary and often make runtime updates to services problematic (see webinar below) If there is a reason to go against these rules, it must be clearly stated on a pull request. OSGi component annotation example: https://vt.uos.de/e5msw If you want to know more about the service configuration and see annotations in action, watch the \u201cOpencast OSGI Configuration\u201c webinar: https://video.ethz.ch/events/opencast/webinars/7261ea70-ce36-4e17-8634-963966311028.html This proposal passes on Wednesday evening if no one objects. Best regards, Lars JPA, PostgreSQL and DDL-Scripts Proposed by Lars Kiesow lkiesow@uos.de , passed on Wed, 30 Oct 2019 Hi everyone, tl;dr \u2013 we want to replace the manually maintained database set-up scripts with JPA's auto-generation capabilities, properly including optimizations like indices, constraints, \u2026 while re-introducing proper support for PostgreSQL. We recently looked into re-introducing proper PostgreSQL support for Opencast, updating the driver [1], \u2026 \u2013 thanks to Beuth University \u2013 One task left now is to properly initialize the database. While we could now write our own initialization script for PostgreSQL, much like `docs/scripts/ddl/mysql5.sql `, we would like to spare us as community the double-effort and instead tackle an alternative route we have been discussing in the community for a long time now: Making Opencast generate a proper database schema on its own. To give you a few more details, if you do not use the DDL scripts, but just configure a database and start Opencast, you will already see a database schema being auto-generated and everything (kind of) magically works. However, due to a mix of historical reasons and some deliberate negligence on our side as developers \u2013 why do the JPA stuff properly if we have to write the DDL script anyway \u2013 the auto-generated schema is problematic for production since a lot of optimization is missing. We did already take a look at this, evaluating the amount of work [2] and fixing a few of the problems [3] but there is much more work to do. The first pull request [2] also outlines in much more detail the current state, what the problems are and what we have to do \u2026if you are interested. Now, finally, we would like to continue this work. First, we are seeking help from others, fixing the issues, but more importantly, testing the work with your database set-ups. Additionally, for us, this work only makes sense if we get a proper PostgreSQL support out of it. Overall, I hope this will make supporting all databases easier since we do not need to write set-up SQL anymore, but for now, we do still need to write upgrade SQL statements and officially re-adding support for PostreSQL, this means that we need to write upgrade scripts for that as well \u2013 we are looking into fixing that problem as well, but that's a separate project ;-) Does anyone have objections to this plan? Does anyone want to help (with development or testing)? Best regards, Lars [1] https://github.com/opencast/opencast/pull/1103 [2] https://github.com/opencast/opencast/pull/1105 [3] https://github.com/opencast/opencast/pull/1133 Pull request reviews Proposed by Lars Kiesow lkiesow@uos.de , passed on Thu, 7 Feb 2019 Hi everyone, right now, we have 43 open pull requests. 23 of these are open for over a month. 12 are open for over two month. I find this a bit problematic. That is why I #propose to relax the requirement of reviewers as follows: Pull requests with no major modification and no review for over a month may be reviewed by developers from the same or related organizations. Reasoning: I find our review process to be very valuable since it prevents a lot of bugs from getting into the code base in the first place. The state of develop is proof that it works great: Most of the time, you can spin up develop and use it without major issues. This state is completely different from Opencast develop back in 1.4 days. Nevertheless, it also puts a burden on the community since it sometimes makes it very hard to fix problems. Not only do you need to write the patch, but you also need to find someone completely unrelated to review this. This can be quite hard in some scenarios. Example: SWITCH tasks me with fixing a bug. This means that SWITCH is out of the loop for reviews and ELAN e.V. is out as well. But not only that. We are related to Osnabr\u00fcck University so e.g. R\u00fcdiger cannot review my patch and also e.g. Michael is working for plapadoo and ELAN so he is out as well. In short: ~80% of the regular reviewers are forbidden from taking the review. This example shows that our current rules can be problematic. This is not generally the case, but this can be the case. Going back to the original reasoning for this rule, it was an intentional decision to prevent a specific institution from just forcing code into Opencast without community involvement by creating a pull request and having a colleague simply merge it. Lifting the embargo after a month should have more or less the same effect. The community has a month to notice and complain about any given patch. Hence, no institution should be able to just push any code into Opencast. If no one complained or commented for a month to raise their concerns, it's unlikely that anyone really objects to the change and it just becomes a matter of a technical review (does the code have any problems). For that, any second developer should hopefully suffice. Please let me know if you have any objections to this change. As usual, this proposal will pass in 72h if no one objects. Best regards, Lars Automate translation merges Proposed by Lars Kiesow lkiesow@uos.de , passed on Wed, 29 Sept 2018 Hi everyone, a few weeks ago we discussed on the technical meeting that it would be great if we could automatically merge back translations on a regular basis. That is why I hereby #propose: Updates to translations for existing languages may be pushed into Opencast's repository automatically. Note that adding or removing languages will still need a regular pull request and a review If this proposal passes, I can implement the automation. \u2013Lars No more merge tickets Proposed by Lars Kiesow lkiesow@uos.de , passed on Thu, 17 May 2018 Hi everyone, tl;dr I hereby #propose to drop the practice of keeping a Jira ticket for synchronizing merges Today we had a short internal discussion about Opencast's merge tickets where we found that all of us here at Osnabr\u00fcck think that we should drop the practice of keeping them. The original goal for those was to prevent two developers to merge things simultaneously causing conflicts for each other. Nowadays most people use Github's merge button anyway which makes this far less problematic since developers do not need to keep track of the upstream branch but can just merge which will just magically work as long as there is no conflict. Some people still merge via command line but they are usually those who can handle conflicts anyway ;-) What also plays into this proposal is that we do not have that many merges. So this is not that big a problem in the first place. And for the few occasions where there are many merges (e.g. I remember some merge sprints before a feature freeze) we always coordinated those efforts anyway (who is taking which review, what's the progress, \u2026) so that not having a merge ticket wouldn't be a problem here either. From experience I can say that even for cutting a release I probably could have worked without a merge ticket with no problem: I pulled the version I was cutting into my local branch anyway, so I could work there and additional merges would not have interfered. Finally, if there is a rare case where it actually makes sense to block a branch, I deem us flexible enough to shout out on list, which may even work better since the message is not drowned by hundreds of similar messages :) Best regards, Lars Migrate Docker Images to Quay.io Proposed by Matthias Neugebauer matthias.neugebauer@uni-muenster.de , passed on Tue, 1 May 2018 Hi, as you might have noticed, there are currently no images available on Docker Hub for Opencast 4.3. The problem is, that Docker Hub itself uses an old version of Docker to build new images (version 17.06.1 is from mid 2017). When Opencast 4.3 was released, I prepared new Dockerfiles and also fixed an issue that resulted in unnecessary large images. Now the problem is, that this requires (only for building images) a feature that was only added to Docker 17.09 in September 2017. In addition, I found Docker Hub to be really slow and unreliable. Builds start minutes after triggering and take a long time to complete. And there are times when nearly all builds simply fail, e.g. because the base image could not be downloaded or the machine used for building run out of disk space. All things that should not happen leaving me quite frustrated with Docker Hub. While Docker Hub is the \"official\" (more like default) image registry, there exist multiple alternatives. Quay (https://quay.io/), for example, is another bigger registry now run by CoreOS (owned by Red Hat). The service is free for public images and offers some additional features compared to Docker Hub (e.g. image vulnerability scanning). In my initial tests, I was really pleased. I don't know what they are doing, but image builds start quick and take under 6 minutes! My local builds take 15-20 minutes :D To come to the point: I herby #propose to further test out Quay and, if this service performs well, migrate the Docker images to this registry. For the tests, I would need to connect the opencast-docker repository to Quay, for which I don't have the permissions. Also, the migration would leave the already existing images on Docker Hub, but users would be advised to use the new repository (\"quay.io/opencast/allinone\" instead of \"opencast/allinone\"). Best regards Matthias Drop Undocumented Workflow Handler Proposed by Lars Kiesow lkiesow@uos.de , passed on Fri, 10 Nov 2017 Hi, lately I discovered a couple of undocumented and probably unused workflow operation handler, for example the 'failing' operation to name one of them. All of these are not really useful in their current undocumented form. That is why I hereby #propose to drop all workflow operation handler still undocumented at the end of the year. If this proposal passes, I will create and publish a list of all operations which would be dropped in their current state. If someone still wants to keep any of them, the only thing they need to do is to write a short documentation page for those operations. A task easily done. That way we may get rid of some unused, unnecessary old operations while ensuring that all of the actually useful ones are documented and thus usable without special inner knowledge of Opencast. Regards, Lars Changing Translation Sources Proposed by Sven Stauber sven.stauber@switch.ch , passed on December 20, 2017 Dear Opencast Developers I hereby #propose to add an additional rule to our development process as described on [1]: Adding or changing translation sources is not allowed in release branches (implying that pull requests doing so need to be directed to the branch develop). Best regards Sven [1] https://docs.opencast.org/develop/developer/development-process/ Crowdin Acceptance Policy Proposed by Greg Logan gregorydlogan@gmail.com , passed on November 17, 2017 Hi all, Per the discussion in the meeting today, we need to set a policy regarding what is expected of our Crowdin translators prior to joining the translation team. My proposal is that they must write a brief, understandable sentence regarding why they want to help translate Opencast via the Crowdin UI. This is an optional field in the workflow where they request to be a translator (ie, no new tools or fields) which is sometimes filled in, but mostly left blank. Something like 'I want to help translate $project into [language]' would be sufficient. This filters out the bots, yet is simple enough that someone with Google translate ought to be able to work something out. Once this passes I will update the Crowdin and Opencast docs regarding the requirements, and then we should be good to go. Proposal closes EOD 2017-11-17. Rename Matterhorn Repository To Opencast Proposed by Lars Kiesow lkiesow@uos.de , passed on July 13, 2017 Hi everyone, I think we have reached a point where people are wondering what the hell matterhorn is ;-D That is why I #propose to rename our official repository from matterhorn to opencast: old: https://bitbucket.org/opencast-community/matterhorn/ new: https://bitbucket.org/opencast-community/opencast/ This proposal will end on Thu Jul 13 16:00 CEST 2017 Regards, Lars Criteria For Inclusion Of Translations Proposed by Sven Stauber sven.stauber@switch.ch , passed on April 28, 2017 Dear all, There are currently no rules about the criteria needed for a translation to be included or excluded from the official Opencast releases. I hereby propose the following rules: 1. A not yet supported translation is included into the next major release if it is translated to at least 90% at the time when the release branch is cut. The release managers will take the review if no other reviewer can be found. 2. A not yet supported translation may be included in the current release branch anytime if it is translated to 100% and a reviewer is found. It will then be part of the next minor release and major release if feasible 3. An endangered translation is a supported translation that is translated less than 80% at the time when the release branch of the next major release is cut. The release managers will publish a list of endangered languages if any 4. An endangered translation will be removed with the next major release if it is not saved. The release managers take care of the removal in case no other person will 5. An endangered translation may be saved by reaching at least 90% translated until at least two weeks before the release date of the next major release and a reviewer is found 6. Considering the percentages of being translated, Crowdin acts as reference 7. Considering the dates of the release cuts of major releases, the respective releases schedules act as reference Best, Sven Make Maintenance Releases Easier Proposed by Lars Kiesow lkiesow@uos.de , passed on April 24, 2017 Hi everyone, over the last years, I have cut a lot of Opencast maintenance releases. The process is to announce that a release will be cut, create a release candidate and wait 72h without veto to actually release. I have always sent out the voting mail as required by our release process and I can always count on the usual response: No reply. This is actually not very surprising since for example now for the 2.3.3 release, people have either already tested the latest state of r/2.3.3 or are involved in the next big release already. In short this means that we always have a three day waiting period in which basically nothing happens. That is why I would like to change the process for *maintenance releases* in the following way: A release manager may cut new maintenance releases at any time without prior release candidate. He should openly announce the date for a new release a week before the release or at any earlier point in time. Note that this will also allow a release manager to release as fast as possible if necessary (e.g. security fix) since the announcement is not strictly required but only a strong advise. This should lessen the work for the a release managers and will enable more agile release processes. We also should not really loose any QA work since everyone knows when releases will happen and people can always test the latest state of a release branch which will become the new release. This proposal will not affect major releases where release candidates with three days testing period would still be required. I hope you agree with this change, Lars Minor documentation changes do not require JIRA issues or PRs Proposed by Stephen Marquard stephen.marquard@uct.ac.za , passed on June 9, 2017 To reduce the overhead involved in improving our documentation, I #propose that minor fixes to documentation may be committed to either maintenance branches or develop without requiring a JIRA issue or pull request. Markdown docs can be edited directly on bitbucket (and git should we move to that), which is a very fast and convenient way for developers to fix documentation. Constraints: documentation fixes committed in this way should be minor changes only; for example fixing typos, layout, formatting, links or small changes to existing content, but no significant new content (which should continue to go through the usual review process). Requiring Java 1.8 for 3.0 Proposed by Greg Logan gregorydlogan@gmail.com , passed on June 12, 2017 Hi folks, For those following along, James Perrin has identified an issue where 3.0 requires Java 1.8 at runtime. We haven't formally included that requirement for 3.0 yet (it's already required for 4.0), but I hereby propose that we do. No one seems to have noticed this requirement was already present in 3.0 (not even me!), even at this late in the release cycle which speaks, I think, to the already widespread adoption of Java 1.8. We would also have to go back and redo all of our testing were we to change the problematic jar to an earlier version, which would be unfortunate for our release timelines. This proposal closes EOD 2017-06-12 UTC -6, at which point I should be able to cut the release. G Change version numbers scheme Proposed by R\u00fcdiger Rolf rrolf@uni-osnabrueck.de >, passed on Mar 23, 2017 Hi all, as we currently approach a new release, I would like to raise a question when it comes to our version numbers: du we need a version number that consists of three parts? At the moment we have <main-version-number>.<major-release-number>.<minor-release-number>. With our current release process, with a new release every 6 month we would always increase the <major-release-number>. Additional to this we have the <minor-release-number> for bug-fix-releases, whenever they are needed. But we do not have a process for increasing the <main-version-number>. Okay 2 years ago we were lucky enough that two long running sub-projects that replaced all UIs in one release were finished. That was an obvious reason to increase the main version. But will we ever be that lucky again? Is only replacing all UIs justifying a main version increase? If I look at the project history we had several milestones that could have justified a new main version, like a nearly complete refactoring of the backend in 1.4, the video-editor in 1.6, the Karaf update in 2.1, the External API in 2.3. *So my #proposal would be to remove the first part of the version number for all upcoming releases. So our next release would be 3.0 and the release at the end of the year it would be 4.0. * We would follow other projects like Sakai in this change - although without the confusing part of going from 2.9.3 to 10.0, where they removed the first number. What are your thoughts? Regards R\u00fcdiger Officially declare the Admin UI Facade as internal API for exclusive use by the module matterhorn-adminui-ng Proposed by Sven Stauber sven.stauber@switch.ch , passed on December 16, 2016 Dear all, I hereby propose to officially declare the Admin UI Facade as internal API for exclusive use by the module matterhorn-adminui-ng. Reason: The Admin UI Facade is essentially the backend of the Admin UI. While it would be technically possible to use this API for other purposes, this would introduce dependencies to components other than the Admin UI. Allowing such dependencies to come into existence would cause changes to the Admin UI Facade to potentially break other (possibly unknown external) components. Hence, we would need to announce, coordinate and discuss changes to this API to not break dependencies to components we potentially don't even know. This would unnecessarily slow down the future development of the Admin UI. In addition, Opencast 2.3 introduces the External API which has been explicitly designed to meet the requirements of an API used to integrate other components. Changes needed: The documentation needs to reflect that the Admin UI Facade is an internal API that will be changed without prior announcement whenever needed without respecting dependencies other than the Admin UI itself and therefore people shall not use this API for integration purposes. Best, Sven Opencast Next: Code Cleanup Proposed by Lars Kiesow lkiesow@uos.de , passed on Thu, 7 July 2016 15:21:19 UTC Hi everyone, a while ago we discussed on the technical meeting that we would like to remove some old code from Opencast since these parts do not work properly (sometimes not at all) or are unused. Why cleaning up? To name some reasons: - Less code to run (less memory, faster start-up) - Less things to compile (faster build) - Less dependencies - People do not accidentally stumble upon broken things - Less work for maintenance And now here is what I #propose to remove and a reason why I think this should be removed. I already took the comments people made in the first draft [1] into account, although I still dared to include the two last items but this time, hopefully with a convincing reason for why they should be removed. 1. Old Administrative User Interface (matterhorn-admin-ui) The reason for this should be obvious: We got a new one. The old one has not been tested for the last three releases, is not linked anywhere anymore and is partly buggy due to changes to Opencast. To maintain two interfaces for one thing do not make sense. 2. Hold-state Workflow Operations These do not work with the new interface any longer and the concept has since been replaced by the actions you can perform on archived material. 3. CleanSessionsFilter Old temporary bug fix. For more details read the thread on our developer list. 4. Republish Workflow Operation Handler It can be removed since it has been replaced by a flag on the publish operation in 2.x. 5. Old workflows + encodings We got new ones. These were only left because of the old ui. 6. Old player (Flash in engage ui) Flash is dead. We have the new player and Paella. 7. Most of shared_ressources Almost everything in here belongs to old user interfaces. 8. matterhorn-engage-player This is the old player Flex project. Iam not even sure it can still be compiled. 9. matterhorn-test-harness Old integration tests 10. matterhorn-mediapackage-ui Old UI ressources 11. matterhorn-manager-* Old, outdated configuration modification via web ui. This was never used and would need a major update to get it working again at all. 12. matterhorn-load-test* Some tests. I have never seen them executed by anyone. 13. matterhorn-holdstate-workflowoperation Workflow operations requiring a hold state which does not exist anymore with the new admin interface. 14. matterhorn-deprecated-workflowoperation The name says everything. This includes the download DVD operation. 15. matterhorn-annotation-* This should not work with either of the current players anymore. 16. docs/jmeter, docs/scripts/load_testing Configuration for a performance testing tool. Not used for a long time and not up-to-date. 17. Everything unused from: https://data.lkiesow.de/opencast/apidocs/deprecated-list.html E.g. FunctionException and ProcessExecutor(Exception) 18. matterhorn-webconsole Karaf comes with a web console. We do not use our old implementation anymore. 19. matterhorn-mediapackage-manipulator Rest endpoint for media package manipulation. It's not used anymore except by components to be removed. 20. matterhorn-search-service-feeds Broken implementation for RSS/Atom feeds 21. matterhorn-caption-* and embed operation Service for converting different subtitle formats and operation to embed these subtitles into the media files. This is *not* player caption support. If required, FFmpeg can be used for conversion between several subtitle formats. Asked on list [2], no one uses this. As indicated before, points 20 and 21 had some comments for leaving them in which did not convince me to not propose this. \u201cInstead of removing it, fix it\u201d is an easy thing to say but sadly requires ressources. Keeping it, announcing it as features and then tell people that it is not working only afterwards is a bad thing and I would like to avoid that. Note that all the code is still in our history so that we loose nothing if we want the old code back. Please feel free to indicate if this action is fine for you or if you want to keep some of the marked code. Please provide a reason if you do. Best regards, Lars [1] http://bit.ly/28YOEZ1 [2] http://bit.ly/28Ztlt8 This proposal has passed with these additional corrections: Hi, we discussed this on today's technical meeting and I'm slightly changing the proposal: 20. Let's remove matterhorn-search-service-feeds only after September 1st which is a realistic time to get things into the next Opencast release. If someone has fixed the issue by them, we will, of course, keep it. This change takes into account that some people have said they are interested into fixing that module, but will make sure that it's removed if no one fixes it to not have an advertised but broken feature. 21. I will be looking into adding subtitle support in a sensible way before removing the matterhorn-caption-* modules or at least clarify if they can still be used. Regards, Lars Hi James, a couple of days, I talked to someone saying that he will soon provide a patch adding exactly this functionality. The holdstate operations are definitely broken due to their UI. My suggestion for a compromise here: - Remove them if that patch for archiving the options is released - Remove them if no one fixes them in time (September 1st) for 2.3 If you want to bring them back later, we always keep the code in our history. Regards, Lars > Hi, > I would like to keep 2 and presumably 13. Both Manchester and AFAIK > Cape Town have use cases for hold states since there is still no > mechanism for passing WF configuration options from one WF to another. > Regards > James The patch has already been published . Opencast Community Repository Owners Proposed by Lars Kiesow lkiesow@uos.de , passed on Fri, 13 May 2016 18:41:52 UTC Hi, today, in the technical meeting, we shortly discussed how to handle requests, problems, etc regarding the other repositories we are hosting under the umbrella of the Opencast community: https://bitbucket.org/opencast-community/profile/repositories While we have people who care about the official Opencast repository as well as rules about what may be merged, who may merge things, \u2026 we do not have that for other repositories and for some it's very unclear. That is why I would like to propose that every repository under the umbrella of the Opencast community needs to have a \u201cproject owner\u201d being responsible for that repository. Usually it should be the one requesting that repository, but of course it can be someone else known in the community. I would also like to propose that if there is no one willing to take up the responsibility to take care of a repository (ownership) if an old owner leaves, the repository should either be removed or marked as deprecated and moved to a separate section if so requested. Finally, I would like to propose that we use the new \u201cproject\u201d feature of BitBucket to group the repositories into the groups: - Opencast - Contrib - Adopters - Deprecated (<- to be created if needed) Currently, all repositories are in one big project. Regards, Lars Rename Opencast Mailing Lists Proposed by Lars Kiesow lkiesow@uos.de , passed on Thu, 14 Apr 2016 00:00:00 UTC Hi everyone, traditionally, we have the three mailing lists: - matterhorn@opencast.org (development list) - matterhorn-users@opencast.org (user list) - community@opencast.org (more or less announcements) Recently, though, we have seen especially the last two list being used for user questions and problems. That is not surprising as we dropped the name \u201cMatterhorn\u201d and new users do not know what that the list matterhorn-users is meant for questions about Opencast. That is why I would like to rename these lists to - dev@opencast.org or development@opencast.org (I prefer the short name but don't have very strong feelings about that) - users@opencast.org - announcements@opencast.org Together with the already existing security-notices list, this gives these lists a very clear meaning. It would also have the benefit that users only interested in general announcements could subscribe to one list only which would likely be a very low-traffic mailing list. Additionally, this would make it sufficient to send announcements to one list, instead of sending it to all three lists. To prevent general questions on the announcements list, I suggest we grant posting rights to board members, committers or other people who have or had a role in our community only. I don't think we need to be too strict here but should make sure that people understand what this list is for. Finally, for the sake of our current members, I would suggest that we forward the mails to the old addresses for at least until the end of the year, if that is possible. Best regards, Lars Documentation Pull Request Merge Order Proposed by Lars Kiesow lkiesow@uos.de , passed on Thu, 25 Feb 2016 20:52:00 UTC Hi everyone, as discussed in this weeks technical meeting, I hereby #propose to allow out-of-order merges of documentation pull requests in the same way we have this exception for bug-fixes. to be precise, I #propose to change the development process docs for reviewing and merging [1] in the following way: [old] - Pull requests for bug fixes (t/MH-XXXXX-...) may be reviewed and merged out of order. [new] - Pull requests for bug fixes or documentation may be reviewed and merged out of order. Regards, Lars [1] https://docs.opencast.org/develop/developer/reviewing-and-merging/ Removing instances of print statements with a style rule #proposal Proposed by Greg Logan gregorydlogan@gmail.com , passed on Wed, 12 Feb 2016 12:00:00 UTC Hi folks, I noticed in a recently review that there are still System.out.println statements in use in our codebase. I was surprised, because thought we had previously implemented a checkstyle rule which would have banned those statements! I hereby #propose that we implement the changes outlined in https://opencast.jira.com/browse/MH-11222, and remove these statements in favour of logger statements. I also propose that we add this rule to the checkstyle ruleset so that we don't have to deal with this again going forward. Proposal closes EOD 2016-02-03. G How to release a new Opencast version\u2026 Proposed by Lars Kiesow lkiesow@uos.de , passed on Fri, 14 Aug 2015 12:54:51 UTC Hi everyone, serving as co-release manager for two versions of Opencast, I noticed that our current release process has some aspects of the release defined in a way that is more hindering than helpful and I want to #propose a slight change to these recommendations. I hereby #propose: 1. Get rid of the `master` branch, make `develop` the main branch. 2. Do not use the --no-ff flags for merges 3. Do not create versions/tags in a release branch. Separate them. Reasoning: 1. The short explanation whould be: When did you explicitely checked out `master` last time? People rarely do that. If I want a specific version, I use the tag, if not I want the release branch or `develop`. If you think about it, then the whole reason for `master` in GitFlow is to always provide the last stable version to users who just check out the repository and do nothing else. The problem with Opencast is, that we support multiple versions at the same time. If in a couple of weeks 1.6.2 is being released, it is the latest stable. Is it? If I check out `master`, however, I will still get 2.0 as we cannot merge 1.6.x afterwards. While you can grasp the reasons behind this, it is a bit confusing for users and it is much easier to just tell them to use the tag to check out a specific version. That it, if they do not use the tarballs from BitBucket anyway. 2. First of all, most people seem to be using BitBucket for auto-merging and it does not use --no-ff. So we are not really consistent anyway. Being consistent and using --no-ff would mean to forbid the usage of the BitBucket merge. Second, have a look at the confusing mess that are the current branches (I tried to find something in the visualization a while ago but gave up). It would be much cleaner to try using fast-forward merges. So instead of using non-fast-forward commits I would argue that we should instead try to use as many fast-forward commits as possible. 3. Once we decided to have the tags in our branches like this: ---- A ---- B (tagged) ----- C ---- D --> A is the commit containing the version that is decided to be released. B is the tagged version. It is exactly the same code as A except for the pom.xml versions that are modified. Finally C then reverts B as the modified version should not be part of the release branch, .... After C, the code is basically A again except for the history (which we later need to merge which can be problematic). D would then be the next \u201creal\u201d commit, meaning the next fix. Much easier to handle would be the following structure: ---- A ---- D --> \\ B (tagged) You do not have to revert that commit, you do not need to merge the easily conflicting pom.xml changes and in the end, you would anyway check out the tag using git checkout <tag> if you want that specific version Branching structure: To have a complete overview, this is what the new branching structure would look like: develop --*--*--*--*--*----*--------*--------*----> \\ / / r/x.y.z *--*--*---*--*--*--*--*--*----> \\ \\ * x.y.z-beta1 * x.y.z-rc1 Regards, Lars Moving away from the 3rd party scripts Proposed by Greg Logan gregorydlogan@gmail.com , passed by Fri, 24 Jul 2015 16:45:40 UTC Hi folks, As it stands right now we depend on the 3rd party tool script to install a great many of our 3rd party dependencies. These are utilities like tesseract, ffmpeg, sox, etc. This script is maintained by Matjaz, in his own time. I'd like to take a moment to thank him for a doing a great job on a particularly annoying aspect of supporting our work! I know it hasn't been easy, especially supporting vast number of different OS versions! With the release of 2.0 I noticed that our 3rd party tool script is becoming both a little out of date, and difficult to maintain. I took a quick look around and it seems like *most* of our dependencies are available from normal distribution repositories for Debian based systems, and I'm told that there is a similar situation for Redhat based systems. I am unsure of how many of our users are running Matterhorn on Mac, but I would hope that our developers who are working on Mac would be able to provide instructions and/or binaries for those users. The only dependency where there might be a universal sticking point is ffmpeg (due to patent concerns), however ffmpeg builds a full static binary with each release, so I assume we can either depend on this and/or cache them somewhere. What this means is that we can potentially remove the 3rd party script from our repository. I hereby #propose we find a way to do that, which would remove the 3rd party script from the repository and replace it with a number of new steps in the install documentation. G Status of youtube in 2.0 and #proposal to change the default workflow Proposed by R\u00fcdiger Rolf rrolf@Uni-Osnabrueck.DE , passed on Sat, 13 Jun 2015 14:15:55 UTC Hi list! There was some discussion in the DevOps meeting yesterday if the Youtube distribution would work or not. I offered to check this. The good news first: IT WORKS! Just follow this manual and your Matterhorn - ups Opencast - is ready to distribute to Youtube. http://docs.opencast.org/r/2.0.x/admin/modules/youtubepublication/ The bad news: The default workflow definition does not really support the publishing on Youtube, as only one video file could be published by the current WOH. https://opencast.jira.com/browse/MH-10920 The reason is simple and the fix would be too. But there are some options to fix this: 1. Remove the option to distribute to Youtube from the default workflow definition, as the complicated configuration would have to come first anyway. 2. Only let \"presenter\" or \"presentation\" be published to Youtube. We would need a new youtube tag and add this to the compose operation and the youtube operation. 3. Introduce the composite operation to the workflow definition and publish only the resulting single stream to Youtube. 4. Upgrade the WOH to support publishing of multiple files. I would say that option 4 could be 2.1 goal, but not for 2.0. I would #propose to go for option 1, as nobody can use Youtube out-of-the-box anyway. And the admin could then setup an appropriate Youtube workflow for their needs too. Regards R\u00fcdiger Episode DublinCore Catalog Proposed by Karen Dolan kdolan@dce.harvard.edu , Passed on Sat, 30 May 2015 12:39:05 UTC Dear Opencast-ees, The following proposal addresses MH-10821[1]. An issue that exposes a know long time ambiguity regarding metadata and the ingest service. The reason that its a proposal is that it normalizes the handling of inbound episode catalog metadata in the ingest service. 1) A new configuration parameter, boolean, for the Ingest Service. The config param identifies if episode metadata precedence is for Ingestee (i.e. Opencast system) or the Ingester (i.e. Capture Agent). For example: at our site, the scheduling entity is the metadata authority. All updates are made to the Scheduling endpoint. The Capture Agent always has stale episode catalog metadata. At other sites, updates are made on the Capture Agent directly. The community default can be for priority to the Capture Agent. 2) All Ingest endpoints perform the same consistent process to ensure that an episode catalog will exist, manually or automatically provided. 3) The process performs the following... 3.1. Gather data - Check if inbound media package contain a reference to an Episode DublinCore catalog and if that catalog contains a title. - Check if the inbound media package contains a title attribute. - Check if the Workflow service has a reference to the mediapackage's Episode Dublin Core catalog - Check if the Scheduler service retained a reference to the event's Episode Dublin Core catalog 3.2. Use config param to prioritize action on acquiring an Episode dc catalog for the media package If Capture Agent metadata takes precedence: - Take the inbound Episode dc catalog, if it exists - Take the Episode dc catalog from the workflow service, if it exists - Take the Episode dc catalog from the scheduler service, if it exists - Create an Episode dc catalog from the title in the media package,, if it exists - Create an Episode dc catalog using a default title (i.e. \"Recording-1234556XYZ\") If Opencast metadata takes precedence: - Take the Episode dc catalog from the workflow service, if it exists - Take the Episode dc catalog from the scheduler service, if it exists - Take the inbound Episode dc catalog if it exists - Create an Episode dc catalog from the title in the media package, if it exists - Create an Episode dc catalog using a default title (i.e. \"Recording-1234556XYZ\") I'll start a pull for the above, and appreciate any thoughts. Regards, Karen [1] https://opencast.jira.com/browse/MH-10821 Dropping Taglines Proposed by Greg Logan gregorydlogan@gmail.com , Passed on Fri, 29 May 2015 16:19:09 UTC Hi folks, I hereby propose that we drop the practice of having taglines. I propose this because we don't have a place in the new admin UI to put them, nor have I ever heard any of the adopters make use of it. I know we don't use it as a committing group, which means that *no one* is using them. G Wiki Cleanup Proposed by Lars Kiesow lkiesow@uos.de , Passed on Fri, 24 May 2015 11:36:49 UTC Hi everyone, since we partly switched to our new documentation [1] I would like to make sure that the old and mostly outdated documentation goes away so that no one stumbles upon that. When I had a look at the wikis we currently have I noticed that most of our 17(!) wikis have not been touched in years and can probably go away. Here is a list of our wikis and what I #propose to do with/to them: Keep (maybe clean-up a bit): - Matterhorn Adopter Guides - Matterhorn Developer Wiki - Opencast Matterhorn D/A/CH - Opencast Matterhorn Espa\u00f1ol - LectureSight Export as PDF to archive the contents and then delete: - Matterhorn Release Docs - 1.0 - Matterhorn Release Docs - 1.1 - Matterhorn Release Docs - 1.2 - Matterhorn Release Docs - 1.3 - Matterhorn Release Docs - 1.4 - Matterhorn Release Docs - 1.5 - Matterhorn Release Docs - TRUNK Keep until 2.1 is out then export as PDF and delete: - Matterhorn Release Docs - 1.6 Just delete: - Analytic video annotation - Infra - Matterhorn Documents - Opencast Community Please let me know if you agree or disagree with this proposal. Regards, Lars [1] http://documentation.opencast.org Jira Clean-Up Proposed by Lars Kiesow lkiesow@uos.de , Passed on Fri, 8 May 2015 11:52:16 UTC Hi everyone, as discussed in the technical meeting, I hereby #propose: The \u201cBlocker\u201d and \u201cRelease Blocker\u201d severity status are more or less redundant. As part of cleaning up Jira, let us remove the \u201cRelease Blocker\u201d severity in favor of \u201cBlocker\u201d. As footnote, some statistics: Since the beginning of 2014, 70 Release Blockers have been files in Jira while mere *8* Blockers have been files. Regards, Lars Opencast Documentation Proposed by R\u00fcdiger Rolf rrolf@uni-osnabrueck.de , Passed on Sat, 02 May 2015 14:43:28 UTC Hi all, Tobias, Basil, Lars and I discussed status of the current migration of the Opencast (Matterhorn) documentation to GIT. We still see some open issues that need clarification so we would like to propose the following points: *1. Formating and Hosting of the Documentation * We want to use https://readthedocs.org to or a similar service create a more appealing HTML version from the Markdown of the documentation. The documentation will be versioned there so that for older versions the documentation is still available. By default the \"latest\" version is shown. The versions of the documenation will be generated based on the release branches. *2. Structure of the Documentation* We see the documentation in*Git *separating into 3 sections: - /Administration Guide/: with information about the installation, configuration, customization and integration. This will be the part of information by an administrator to setup Opencast. - /Developer Guide/: All information related to implementation details of Opencast, so that this will be updated in a pull request (API changes, module descriptions, architecture). The development process documents should also go here as only committers usually should change these. - /User Guide/: Documentation of the (new) Admin UI that was already started by Entwine and the Engage UI (especially Theodul Player). This guide should only describe options available on the UIs. Within the *Wiki* we still see the need for 2 sections: - /Developer Wiki/: Proposals, working documents and meeting notes will be kept here so that anybody can edit these. So information not to close to any existing implementation that might still be in a process of discussion can be found here. - /Adopters Wiki/: This can be the place where adopters share their best practises, configurations, hardware recommendations, third-party software documentation etc. Again anyone can contribute to this wiki. The difference between the Wiki and Git is in the first line that the Git documentation should become a quality assured ressource for Opencast users. The Git documentation should be reviewed within the release process and it will be part of the review process of a pull request, to make sure that the needed documentation changes have been contributed too. The Wikis on the other hand should be a more open platform where everybody can contribute and users might find cookbooks to enhance their system, or they can share ideas. So now we would like to get your opinion on this proposal. Thank you, R\u00fcdiger Requirement Specification Proposed by Lars Kiesow lkiesow@uos.de , Passed on Thu, 16 Apr 2015 15:55:31 UTC On list or IRC we often see that people do not really know the current requirements for a specific version of Opencast Matterhorn. Of course there are the pom.xml files specifying internal dependencies, but there is nothing for 3rd-party-tools, ... It would be nice to add a file specifying these requirements in a format that is easy to parse and can hence be used for automatic scripts to generate dependency lists, ... That is why I hereby #propose to add a requirements.xml file that specifies the requirements for Opencast Matterhorn: - Required tools including versions - Which modules require which tools - Which modules conflict with each other (negative requirement) This is mainly what is not specified by the pom.xml files yet. Jira Clean-Up (Tags VS Labels) Proposed by Lars Kiesow lkiesow@uos.de , Passed on Thu, 19. Mar 2015 15:43:20 UTC \u2026then hereby I officially #propose removing the labels from Jira. For more details, have a look at the mail thread at: https://groups.google.com/a/opencast.org/forum/#!topic/matterhorn/vIdWQkZmbdQ FFmpeg Update Proposed by Lars Kiesow lkiesow@uos.de , Passed on Sat, 14 Mar 2015 22:12:18 UTC Looking at the FFmpeg project for the last two years, you will notice that they developed a pretty stable release cycle with a release of a new stable version approximately every three month. To stop us from having to propose an update again and again, I hereby propose the following general rule for our support of FFmpeg: A Matterhorn release will oficially support the latest stable version of FFmpeg released at the time the release branch is cut and all other FFmpeg versions with the same major version number released afterwards. For example, for Matterhorn 2 this would mean that we will officially support FFmpeg 2.5.4 and all later 2.x versions like 2.6 which has been released on the 7th of March or a possible 2.7 onece it is released. We would, however, not necessarily support an FFmpeg 3 as it *might* come with an interface change that *could* break compatibility. That obviously does not mean that older versions of FFmpeg just stop working. In fact, most parts of the default Matterhorn configuration should at the moment still work with FFmpeg 1.x but we will not test or fix compatibility problems. Proposal Log Proposed by Lars Kiesow lkiesow@uos.de , Passed on Sat, 14 Mar 2015 16:35:08 UTC It would be wonderful if we had a central place to look up the proposals that have passed. That is why I hereby propose that: - We create a proposal log in our new documentation containing all proposals that have passed on list. - A proposal will become effective only after it is written down in that log. That should usually be done by the person who sent out that proposal. This will, of course, not affect the existing decision making rules (proposal on list, marked with #proposal, lazy consensus after three days, no -1, ...)","title":"Proposal Log"},{"location":"proposal-log/#opencast-proposals","text":"All important decisions for Opencast have to be made on list. For more details, please have a look at out documentation about decision making . The following list contains a list of passed proposals for reference.","title":"Opencast Proposals"},{"location":"proposal-log/#passed-proposals","text":"","title":"Passed Proposals"},{"location":"proposal-log/#release-process-update","text":"Proposed by Lars Kiesow lkiesow@uos.de , passed on Thu, 24 Dec 2020 Hi everyone, based on the discussion on list and in the technical meeting, Lukas and I have created a proposal for changing our release process and applying a new set of rules. You might notice that we have deliberately not included all of what we discussed but hopefully only what can get consensus for now. Apart from some minor changes, the main difference in here is that we transition from making a distinction between feature and bug fix to defining a set of rules that hopefully ensure smooth minor updates. If this proposal passes, we can work for there and refine this but we hope that this is a good starting point. ## Release Schedule - Major releases happen every half year - Minor stable releases are cut monthly monthly - Minor legacy releases are cut on demand - Urgent minor releases may be cut if necessary - Feature freeze for a major version should be about a month before the release ## Accepting patches for minor releases - All patches need to be discussed in the technical meeting - Minor changes should not take much time - Protects against problematic changes in minor releases - Exceptions may be discussed if necessary (e.g. version bumps for libraries on develop) - Patches for minor releases must not - Modify any existing database tables - Modify the indexes or otherwise cause re-indexing - Require a different ActiveMQ configuration - Modify existing translations - Patches for minor releases must - Work with the same configuration within a major version ## Be Pragmatic - If everything is broken, don't let rules hold you back\u2026 - There is always a special case: If in doubt talk to the community and find a solution that works. - Communication is key: If you think you need to break a rule, coordinate that with the community. ## Additional suggestions - Patches should be applied to the latest release branch if possible - Avoid the risk of breaking legacy due to less testing - Avoid unnecessary merge conflicts - Patches should avoid unnecessarily large changes in any release branch As usual, if no one objects, this proposal passes in three days. Best regards, Lars","title":"Release Process Update"},{"location":"proposal-log/#relocate-build-infrastructure-to-main-repository","text":"Proposed by Lars Kiesow lkiesow@uos.de , passed on Thu, 17 Dec 2020 Hi everyone, as you are all aware there are multiple community members working on and maintaining infrastructure for different binary builds. For example, Greg is doing the Debian packages. All these are currently maintained in separate repositories\u2026 somewhere. Greg and I would like to bring these back to the main repositories to not loose track of what's available and where things are living. But maintaining these sometimes requires quick actions and we cannot really create a pull request every time a release is cut and then wait a week or two before it's merged until we can release the packaged version. That is why I #propose to allow for maintainers of these areas to work on these directly without running through our usual pull request and review rules. For example, that could mean that I could quickly update the RPM spec file if necessary while I would not be allowed to just modify the codebase (e.g. modules/*) on my own. Initially, I #propose the following areas and maintainers: Debian build architecture Greg Logan RPM build architecture Lars Kiesow docs.opencast.org build architecture Lars Kiesow This doesn't really change anything at the moment since we are already in control of these parts right now. The difference would be that it's not maintained somewhere any longer but everyone knows where to find things and (potentially) how contribute. Of course, we can extend this whenever we need to to include more/less scripts. Though I suggest that we keep this to what we officially support. As usual, this proposal passes unless someone vetos it in the next 72h. Best regards, Lars","title":"Relocate build infrastructure to main repository"},{"location":"proposal-log/#jdk-support","text":"Proposed by Greg Logan gregorydlogan@gmail.com , passed on Wed, 11 Nov 2020 Hi all, We have some old rules[1] around which JDK is supported by which version of Opencast, but obviously they are somewhat out of date at this point. With that in mind, here's what I'm proposing: Opencast 8: Formal support for JDK 8, and nothing else Opencast 9: Formal support for JDKs 8 and 11, with a provisional \"it should work\" for newer versions[2] Opencast 10: Formal support for JDK 11, with a provisional \"it should work\" for newer versions Going forward, I would like to see a given version of Opencast support the latest two LTS JDKs as a general rule. #proposal passes if no objections are raised by EOD 2020-11-11 G 1: https://docs.opencast.org/develop/developer/#proposal-log/#requiring-java-18-for-30 2: Note that JDK 11 is currently broken, at least until https://github.com/opencast/opencast/pull/2009 gets merged","title":"JDK Support"},{"location":"proposal-log/#osgi-annotations-and-configuration","text":"Proposed by Lars Kiesow lkiesow@uos.de , passed on Wed, 7 Oct 2020 Hi everyone, I hereby #propose that for all new code added to opencast - OSGi annotations should be used instead of XML files for specifying components and dependencies. Reason: This is far less error prone. Even on today's Review Friday, we had problems with the XML files again. No wonder if you write these manually. - @Activate and @Modified should be used for reading and updating configuration instead of implementing a Managed Service. Reason: Managed services are usually unnecessary and often make runtime updates to services problematic (see webinar below) If there is a reason to go against these rules, it must be clearly stated on a pull request. OSGi component annotation example: https://vt.uos.de/e5msw If you want to know more about the service configuration and see annotations in action, watch the \u201cOpencast OSGI Configuration\u201c webinar: https://video.ethz.ch/events/opencast/webinars/7261ea70-ce36-4e17-8634-963966311028.html This proposal passes on Wednesday evening if no one objects. Best regards, Lars","title":"OSGi Annotations and Configuration"},{"location":"proposal-log/#jpa-postgresql-and-ddl-scripts","text":"Proposed by Lars Kiesow lkiesow@uos.de , passed on Wed, 30 Oct 2019 Hi everyone, tl;dr \u2013 we want to replace the manually maintained database set-up scripts with JPA's auto-generation capabilities, properly including optimizations like indices, constraints, \u2026 while re-introducing proper support for PostgreSQL. We recently looked into re-introducing proper PostgreSQL support for Opencast, updating the driver [1], \u2026 \u2013 thanks to Beuth University \u2013 One task left now is to properly initialize the database. While we could now write our own initialization script for PostgreSQL, much like `docs/scripts/ddl/mysql5.sql `, we would like to spare us as community the double-effort and instead tackle an alternative route we have been discussing in the community for a long time now: Making Opencast generate a proper database schema on its own. To give you a few more details, if you do not use the DDL scripts, but just configure a database and start Opencast, you will already see a database schema being auto-generated and everything (kind of) magically works. However, due to a mix of historical reasons and some deliberate negligence on our side as developers \u2013 why do the JPA stuff properly if we have to write the DDL script anyway \u2013 the auto-generated schema is problematic for production since a lot of optimization is missing. We did already take a look at this, evaluating the amount of work [2] and fixing a few of the problems [3] but there is much more work to do. The first pull request [2] also outlines in much more detail the current state, what the problems are and what we have to do \u2026if you are interested. Now, finally, we would like to continue this work. First, we are seeking help from others, fixing the issues, but more importantly, testing the work with your database set-ups. Additionally, for us, this work only makes sense if we get a proper PostgreSQL support out of it. Overall, I hope this will make supporting all databases easier since we do not need to write set-up SQL anymore, but for now, we do still need to write upgrade SQL statements and officially re-adding support for PostreSQL, this means that we need to write upgrade scripts for that as well \u2013 we are looking into fixing that problem as well, but that's a separate project ;-) Does anyone have objections to this plan? Does anyone want to help (with development or testing)? Best regards, Lars [1] https://github.com/opencast/opencast/pull/1103 [2] https://github.com/opencast/opencast/pull/1105 [3] https://github.com/opencast/opencast/pull/1133","title":"JPA, PostgreSQL and DDL-Scripts"},{"location":"proposal-log/#pull-request-reviews","text":"Proposed by Lars Kiesow lkiesow@uos.de , passed on Thu, 7 Feb 2019 Hi everyone, right now, we have 43 open pull requests. 23 of these are open for over a month. 12 are open for over two month. I find this a bit problematic. That is why I #propose to relax the requirement of reviewers as follows: Pull requests with no major modification and no review for over a month may be reviewed by developers from the same or related organizations. Reasoning: I find our review process to be very valuable since it prevents a lot of bugs from getting into the code base in the first place. The state of develop is proof that it works great: Most of the time, you can spin up develop and use it without major issues. This state is completely different from Opencast develop back in 1.4 days. Nevertheless, it also puts a burden on the community since it sometimes makes it very hard to fix problems. Not only do you need to write the patch, but you also need to find someone completely unrelated to review this. This can be quite hard in some scenarios. Example: SWITCH tasks me with fixing a bug. This means that SWITCH is out of the loop for reviews and ELAN e.V. is out as well. But not only that. We are related to Osnabr\u00fcck University so e.g. R\u00fcdiger cannot review my patch and also e.g. Michael is working for plapadoo and ELAN so he is out as well. In short: ~80% of the regular reviewers are forbidden from taking the review. This example shows that our current rules can be problematic. This is not generally the case, but this can be the case. Going back to the original reasoning for this rule, it was an intentional decision to prevent a specific institution from just forcing code into Opencast without community involvement by creating a pull request and having a colleague simply merge it. Lifting the embargo after a month should have more or less the same effect. The community has a month to notice and complain about any given patch. Hence, no institution should be able to just push any code into Opencast. If no one complained or commented for a month to raise their concerns, it's unlikely that anyone really objects to the change and it just becomes a matter of a technical review (does the code have any problems). For that, any second developer should hopefully suffice. Please let me know if you have any objections to this change. As usual, this proposal will pass in 72h if no one objects. Best regards, Lars","title":"Pull request reviews"},{"location":"proposal-log/#automate-translation-merges","text":"Proposed by Lars Kiesow lkiesow@uos.de , passed on Wed, 29 Sept 2018 Hi everyone, a few weeks ago we discussed on the technical meeting that it would be great if we could automatically merge back translations on a regular basis. That is why I hereby #propose: Updates to translations for existing languages may be pushed into Opencast's repository automatically. Note that adding or removing languages will still need a regular pull request and a review If this proposal passes, I can implement the automation. \u2013Lars","title":"Automate translation merges"},{"location":"proposal-log/#no-more-merge-tickets","text":"Proposed by Lars Kiesow lkiesow@uos.de , passed on Thu, 17 May 2018 Hi everyone, tl;dr I hereby #propose to drop the practice of keeping a Jira ticket for synchronizing merges Today we had a short internal discussion about Opencast's merge tickets where we found that all of us here at Osnabr\u00fcck think that we should drop the practice of keeping them. The original goal for those was to prevent two developers to merge things simultaneously causing conflicts for each other. Nowadays most people use Github's merge button anyway which makes this far less problematic since developers do not need to keep track of the upstream branch but can just merge which will just magically work as long as there is no conflict. Some people still merge via command line but they are usually those who can handle conflicts anyway ;-) What also plays into this proposal is that we do not have that many merges. So this is not that big a problem in the first place. And for the few occasions where there are many merges (e.g. I remember some merge sprints before a feature freeze) we always coordinated those efforts anyway (who is taking which review, what's the progress, \u2026) so that not having a merge ticket wouldn't be a problem here either. From experience I can say that even for cutting a release I probably could have worked without a merge ticket with no problem: I pulled the version I was cutting into my local branch anyway, so I could work there and additional merges would not have interfered. Finally, if there is a rare case where it actually makes sense to block a branch, I deem us flexible enough to shout out on list, which may even work better since the message is not drowned by hundreds of similar messages :) Best regards, Lars","title":"No more merge tickets"},{"location":"proposal-log/#migrate-docker-images-to-quayio","text":"Proposed by Matthias Neugebauer matthias.neugebauer@uni-muenster.de , passed on Tue, 1 May 2018 Hi, as you might have noticed, there are currently no images available on Docker Hub for Opencast 4.3. The problem is, that Docker Hub itself uses an old version of Docker to build new images (version 17.06.1 is from mid 2017). When Opencast 4.3 was released, I prepared new Dockerfiles and also fixed an issue that resulted in unnecessary large images. Now the problem is, that this requires (only for building images) a feature that was only added to Docker 17.09 in September 2017. In addition, I found Docker Hub to be really slow and unreliable. Builds start minutes after triggering and take a long time to complete. And there are times when nearly all builds simply fail, e.g. because the base image could not be downloaded or the machine used for building run out of disk space. All things that should not happen leaving me quite frustrated with Docker Hub. While Docker Hub is the \"official\" (more like default) image registry, there exist multiple alternatives. Quay (https://quay.io/), for example, is another bigger registry now run by CoreOS (owned by Red Hat). The service is free for public images and offers some additional features compared to Docker Hub (e.g. image vulnerability scanning). In my initial tests, I was really pleased. I don't know what they are doing, but image builds start quick and take under 6 minutes! My local builds take 15-20 minutes :D To come to the point: I herby #propose to further test out Quay and, if this service performs well, migrate the Docker images to this registry. For the tests, I would need to connect the opencast-docker repository to Quay, for which I don't have the permissions. Also, the migration would leave the already existing images on Docker Hub, but users would be advised to use the new repository (\"quay.io/opencast/allinone\" instead of \"opencast/allinone\"). Best regards Matthias","title":"Migrate Docker Images to Quay.io"},{"location":"proposal-log/#drop-undocumented-workflow-handler","text":"Proposed by Lars Kiesow lkiesow@uos.de , passed on Fri, 10 Nov 2017 Hi, lately I discovered a couple of undocumented and probably unused workflow operation handler, for example the 'failing' operation to name one of them. All of these are not really useful in their current undocumented form. That is why I hereby #propose to drop all workflow operation handler still undocumented at the end of the year. If this proposal passes, I will create and publish a list of all operations which would be dropped in their current state. If someone still wants to keep any of them, the only thing they need to do is to write a short documentation page for those operations. A task easily done. That way we may get rid of some unused, unnecessary old operations while ensuring that all of the actually useful ones are documented and thus usable without special inner knowledge of Opencast. Regards, Lars","title":"Drop Undocumented Workflow Handler"},{"location":"proposal-log/#changing-translation-sources","text":"Proposed by Sven Stauber sven.stauber@switch.ch , passed on December 20, 2017 Dear Opencast Developers I hereby #propose to add an additional rule to our development process as described on [1]: Adding or changing translation sources is not allowed in release branches (implying that pull requests doing so need to be directed to the branch develop). Best regards Sven [1] https://docs.opencast.org/develop/developer/development-process/","title":"Changing Translation Sources"},{"location":"proposal-log/#crowdin-acceptance-policy","text":"Proposed by Greg Logan gregorydlogan@gmail.com , passed on November 17, 2017 Hi all, Per the discussion in the meeting today, we need to set a policy regarding what is expected of our Crowdin translators prior to joining the translation team. My proposal is that they must write a brief, understandable sentence regarding why they want to help translate Opencast via the Crowdin UI. This is an optional field in the workflow where they request to be a translator (ie, no new tools or fields) which is sometimes filled in, but mostly left blank. Something like 'I want to help translate $project into [language]' would be sufficient. This filters out the bots, yet is simple enough that someone with Google translate ought to be able to work something out. Once this passes I will update the Crowdin and Opencast docs regarding the requirements, and then we should be good to go. Proposal closes EOD 2017-11-17.","title":"Crowdin Acceptance Policy"},{"location":"proposal-log/#rename-matterhorn-repository-to-opencast","text":"Proposed by Lars Kiesow lkiesow@uos.de , passed on July 13, 2017 Hi everyone, I think we have reached a point where people are wondering what the hell matterhorn is ;-D That is why I #propose to rename our official repository from matterhorn to opencast: old: https://bitbucket.org/opencast-community/matterhorn/ new: https://bitbucket.org/opencast-community/opencast/ This proposal will end on Thu Jul 13 16:00 CEST 2017 Regards, Lars","title":"Rename Matterhorn Repository To Opencast"},{"location":"proposal-log/#criteria-for-inclusion-of-translations","text":"Proposed by Sven Stauber sven.stauber@switch.ch , passed on April 28, 2017 Dear all, There are currently no rules about the criteria needed for a translation to be included or excluded from the official Opencast releases. I hereby propose the following rules: 1. A not yet supported translation is included into the next major release if it is translated to at least 90% at the time when the release branch is cut. The release managers will take the review if no other reviewer can be found. 2. A not yet supported translation may be included in the current release branch anytime if it is translated to 100% and a reviewer is found. It will then be part of the next minor release and major release if feasible 3. An endangered translation is a supported translation that is translated less than 80% at the time when the release branch of the next major release is cut. The release managers will publish a list of endangered languages if any 4. An endangered translation will be removed with the next major release if it is not saved. The release managers take care of the removal in case no other person will 5. An endangered translation may be saved by reaching at least 90% translated until at least two weeks before the release date of the next major release and a reviewer is found 6. Considering the percentages of being translated, Crowdin acts as reference 7. Considering the dates of the release cuts of major releases, the respective releases schedules act as reference Best, Sven","title":"Criteria For Inclusion Of Translations"},{"location":"proposal-log/#make-maintenance-releases-easier","text":"Proposed by Lars Kiesow lkiesow@uos.de , passed on April 24, 2017 Hi everyone, over the last years, I have cut a lot of Opencast maintenance releases. The process is to announce that a release will be cut, create a release candidate and wait 72h without veto to actually release. I have always sent out the voting mail as required by our release process and I can always count on the usual response: No reply. This is actually not very surprising since for example now for the 2.3.3 release, people have either already tested the latest state of r/2.3.3 or are involved in the next big release already. In short this means that we always have a three day waiting period in which basically nothing happens. That is why I would like to change the process for *maintenance releases* in the following way: A release manager may cut new maintenance releases at any time without prior release candidate. He should openly announce the date for a new release a week before the release or at any earlier point in time. Note that this will also allow a release manager to release as fast as possible if necessary (e.g. security fix) since the announcement is not strictly required but only a strong advise. This should lessen the work for the a release managers and will enable more agile release processes. We also should not really loose any QA work since everyone knows when releases will happen and people can always test the latest state of a release branch which will become the new release. This proposal will not affect major releases where release candidates with three days testing period would still be required. I hope you agree with this change, Lars","title":"Make Maintenance Releases Easier"},{"location":"proposal-log/#minor-documentation-changes-do-not-require-jira-issues-or-prs","text":"Proposed by Stephen Marquard stephen.marquard@uct.ac.za , passed on June 9, 2017 To reduce the overhead involved in improving our documentation, I #propose that minor fixes to documentation may be committed to either maintenance branches or develop without requiring a JIRA issue or pull request. Markdown docs can be edited directly on bitbucket (and git should we move to that), which is a very fast and convenient way for developers to fix documentation. Constraints: documentation fixes committed in this way should be minor changes only; for example fixing typos, layout, formatting, links or small changes to existing content, but no significant new content (which should continue to go through the usual review process).","title":"Minor documentation changes do not require JIRA issues or PRs"},{"location":"proposal-log/#requiring-java-18-for-30","text":"Proposed by Greg Logan gregorydlogan@gmail.com , passed on June 12, 2017 Hi folks, For those following along, James Perrin has identified an issue where 3.0 requires Java 1.8 at runtime. We haven't formally included that requirement for 3.0 yet (it's already required for 4.0), but I hereby propose that we do. No one seems to have noticed this requirement was already present in 3.0 (not even me!), even at this late in the release cycle which speaks, I think, to the already widespread adoption of Java 1.8. We would also have to go back and redo all of our testing were we to change the problematic jar to an earlier version, which would be unfortunate for our release timelines. This proposal closes EOD 2017-06-12 UTC -6, at which point I should be able to cut the release. G","title":"Requiring Java 1.8 for 3.0"},{"location":"proposal-log/#change-version-numbers-scheme","text":"Proposed by R\u00fcdiger Rolf rrolf@uni-osnabrueck.de >, passed on Mar 23, 2017 Hi all, as we currently approach a new release, I would like to raise a question when it comes to our version numbers: du we need a version number that consists of three parts? At the moment we have <main-version-number>.<major-release-number>.<minor-release-number>. With our current release process, with a new release every 6 month we would always increase the <major-release-number>. Additional to this we have the <minor-release-number> for bug-fix-releases, whenever they are needed. But we do not have a process for increasing the <main-version-number>. Okay 2 years ago we were lucky enough that two long running sub-projects that replaced all UIs in one release were finished. That was an obvious reason to increase the main version. But will we ever be that lucky again? Is only replacing all UIs justifying a main version increase? If I look at the project history we had several milestones that could have justified a new main version, like a nearly complete refactoring of the backend in 1.4, the video-editor in 1.6, the Karaf update in 2.1, the External API in 2.3. *So my #proposal would be to remove the first part of the version number for all upcoming releases. So our next release would be 3.0 and the release at the end of the year it would be 4.0. * We would follow other projects like Sakai in this change - although without the confusing part of going from 2.9.3 to 10.0, where they removed the first number. What are your thoughts? Regards R\u00fcdiger","title":"Change version numbers scheme"},{"location":"proposal-log/#officially-declare-the-admin-ui-facade-as-internal-api-for-exclusive-use-by-the-module-matterhorn-adminui-ng","text":"Proposed by Sven Stauber sven.stauber@switch.ch , passed on December 16, 2016 Dear all, I hereby propose to officially declare the Admin UI Facade as internal API for exclusive use by the module matterhorn-adminui-ng. Reason: The Admin UI Facade is essentially the backend of the Admin UI. While it would be technically possible to use this API for other purposes, this would introduce dependencies to components other than the Admin UI. Allowing such dependencies to come into existence would cause changes to the Admin UI Facade to potentially break other (possibly unknown external) components. Hence, we would need to announce, coordinate and discuss changes to this API to not break dependencies to components we potentially don't even know. This would unnecessarily slow down the future development of the Admin UI. In addition, Opencast 2.3 introduces the External API which has been explicitly designed to meet the requirements of an API used to integrate other components. Changes needed: The documentation needs to reflect that the Admin UI Facade is an internal API that will be changed without prior announcement whenever needed without respecting dependencies other than the Admin UI itself and therefore people shall not use this API for integration purposes. Best, Sven","title":"Officially declare the Admin UI Facade as internal API for exclusive use by the module matterhorn-adminui-ng"},{"location":"proposal-log/#opencast-next-code-cleanup","text":"Proposed by Lars Kiesow lkiesow@uos.de , passed on Thu, 7 July 2016 15:21:19 UTC Hi everyone, a while ago we discussed on the technical meeting that we would like to remove some old code from Opencast since these parts do not work properly (sometimes not at all) or are unused. Why cleaning up? To name some reasons: - Less code to run (less memory, faster start-up) - Less things to compile (faster build) - Less dependencies - People do not accidentally stumble upon broken things - Less work for maintenance And now here is what I #propose to remove and a reason why I think this should be removed. I already took the comments people made in the first draft [1] into account, although I still dared to include the two last items but this time, hopefully with a convincing reason for why they should be removed. 1. Old Administrative User Interface (matterhorn-admin-ui) The reason for this should be obvious: We got a new one. The old one has not been tested for the last three releases, is not linked anywhere anymore and is partly buggy due to changes to Opencast. To maintain two interfaces for one thing do not make sense. 2. Hold-state Workflow Operations These do not work with the new interface any longer and the concept has since been replaced by the actions you can perform on archived material. 3. CleanSessionsFilter Old temporary bug fix. For more details read the thread on our developer list. 4. Republish Workflow Operation Handler It can be removed since it has been replaced by a flag on the publish operation in 2.x. 5. Old workflows + encodings We got new ones. These were only left because of the old ui. 6. Old player (Flash in engage ui) Flash is dead. We have the new player and Paella. 7. Most of shared_ressources Almost everything in here belongs to old user interfaces. 8. matterhorn-engage-player This is the old player Flex project. Iam not even sure it can still be compiled. 9. matterhorn-test-harness Old integration tests 10. matterhorn-mediapackage-ui Old UI ressources 11. matterhorn-manager-* Old, outdated configuration modification via web ui. This was never used and would need a major update to get it working again at all. 12. matterhorn-load-test* Some tests. I have never seen them executed by anyone. 13. matterhorn-holdstate-workflowoperation Workflow operations requiring a hold state which does not exist anymore with the new admin interface. 14. matterhorn-deprecated-workflowoperation The name says everything. This includes the download DVD operation. 15. matterhorn-annotation-* This should not work with either of the current players anymore. 16. docs/jmeter, docs/scripts/load_testing Configuration for a performance testing tool. Not used for a long time and not up-to-date. 17. Everything unused from: https://data.lkiesow.de/opencast/apidocs/deprecated-list.html E.g. FunctionException and ProcessExecutor(Exception) 18. matterhorn-webconsole Karaf comes with a web console. We do not use our old implementation anymore. 19. matterhorn-mediapackage-manipulator Rest endpoint for media package manipulation. It's not used anymore except by components to be removed. 20. matterhorn-search-service-feeds Broken implementation for RSS/Atom feeds 21. matterhorn-caption-* and embed operation Service for converting different subtitle formats and operation to embed these subtitles into the media files. This is *not* player caption support. If required, FFmpeg can be used for conversion between several subtitle formats. Asked on list [2], no one uses this. As indicated before, points 20 and 21 had some comments for leaving them in which did not convince me to not propose this. \u201cInstead of removing it, fix it\u201d is an easy thing to say but sadly requires ressources. Keeping it, announcing it as features and then tell people that it is not working only afterwards is a bad thing and I would like to avoid that. Note that all the code is still in our history so that we loose nothing if we want the old code back. Please feel free to indicate if this action is fine for you or if you want to keep some of the marked code. Please provide a reason if you do. Best regards, Lars [1] http://bit.ly/28YOEZ1 [2] http://bit.ly/28Ztlt8 This proposal has passed with these additional corrections: Hi, we discussed this on today's technical meeting and I'm slightly changing the proposal: 20. Let's remove matterhorn-search-service-feeds only after September 1st which is a realistic time to get things into the next Opencast release. If someone has fixed the issue by them, we will, of course, keep it. This change takes into account that some people have said they are interested into fixing that module, but will make sure that it's removed if no one fixes it to not have an advertised but broken feature. 21. I will be looking into adding subtitle support in a sensible way before removing the matterhorn-caption-* modules or at least clarify if they can still be used. Regards, Lars Hi James, a couple of days, I talked to someone saying that he will soon provide a patch adding exactly this functionality. The holdstate operations are definitely broken due to their UI. My suggestion for a compromise here: - Remove them if that patch for archiving the options is released - Remove them if no one fixes them in time (September 1st) for 2.3 If you want to bring them back later, we always keep the code in our history. Regards, Lars > Hi, > I would like to keep 2 and presumably 13. Both Manchester and AFAIK > Cape Town have use cases for hold states since there is still no > mechanism for passing WF configuration options from one WF to another. > Regards > James The patch has already been published .","title":"Opencast Next: Code Cleanup"},{"location":"proposal-log/#opencast-community-repository-owners","text":"Proposed by Lars Kiesow lkiesow@uos.de , passed on Fri, 13 May 2016 18:41:52 UTC Hi, today, in the technical meeting, we shortly discussed how to handle requests, problems, etc regarding the other repositories we are hosting under the umbrella of the Opencast community: https://bitbucket.org/opencast-community/profile/repositories While we have people who care about the official Opencast repository as well as rules about what may be merged, who may merge things, \u2026 we do not have that for other repositories and for some it's very unclear. That is why I would like to propose that every repository under the umbrella of the Opencast community needs to have a \u201cproject owner\u201d being responsible for that repository. Usually it should be the one requesting that repository, but of course it can be someone else known in the community. I would also like to propose that if there is no one willing to take up the responsibility to take care of a repository (ownership) if an old owner leaves, the repository should either be removed or marked as deprecated and moved to a separate section if so requested. Finally, I would like to propose that we use the new \u201cproject\u201d feature of BitBucket to group the repositories into the groups: - Opencast - Contrib - Adopters - Deprecated (<- to be created if needed) Currently, all repositories are in one big project. Regards, Lars","title":"Opencast Community Repository Owners"},{"location":"proposal-log/#rename-opencast-mailing-lists","text":"Proposed by Lars Kiesow lkiesow@uos.de , passed on Thu, 14 Apr 2016 00:00:00 UTC Hi everyone, traditionally, we have the three mailing lists: - matterhorn@opencast.org (development list) - matterhorn-users@opencast.org (user list) - community@opencast.org (more or less announcements) Recently, though, we have seen especially the last two list being used for user questions and problems. That is not surprising as we dropped the name \u201cMatterhorn\u201d and new users do not know what that the list matterhorn-users is meant for questions about Opencast. That is why I would like to rename these lists to - dev@opencast.org or development@opencast.org (I prefer the short name but don't have very strong feelings about that) - users@opencast.org - announcements@opencast.org Together with the already existing security-notices list, this gives these lists a very clear meaning. It would also have the benefit that users only interested in general announcements could subscribe to one list only which would likely be a very low-traffic mailing list. Additionally, this would make it sufficient to send announcements to one list, instead of sending it to all three lists. To prevent general questions on the announcements list, I suggest we grant posting rights to board members, committers or other people who have or had a role in our community only. I don't think we need to be too strict here but should make sure that people understand what this list is for. Finally, for the sake of our current members, I would suggest that we forward the mails to the old addresses for at least until the end of the year, if that is possible. Best regards, Lars","title":"Rename Opencast Mailing Lists"},{"location":"proposal-log/#documentation-pull-request-merge-order","text":"Proposed by Lars Kiesow lkiesow@uos.de , passed on Thu, 25 Feb 2016 20:52:00 UTC Hi everyone, as discussed in this weeks technical meeting, I hereby #propose to allow out-of-order merges of documentation pull requests in the same way we have this exception for bug-fixes. to be precise, I #propose to change the development process docs for reviewing and merging [1] in the following way: [old] - Pull requests for bug fixes (t/MH-XXXXX-...) may be reviewed and merged out of order. [new] - Pull requests for bug fixes or documentation may be reviewed and merged out of order. Regards, Lars [1] https://docs.opencast.org/develop/developer/reviewing-and-merging/","title":"Documentation Pull Request Merge Order"},{"location":"proposal-log/#removing-instances-of-print-statements-with-a-style-rule-proposal","text":"Proposed by Greg Logan gregorydlogan@gmail.com , passed on Wed, 12 Feb 2016 12:00:00 UTC Hi folks, I noticed in a recently review that there are still System.out.println statements in use in our codebase. I was surprised, because thought we had previously implemented a checkstyle rule which would have banned those statements! I hereby #propose that we implement the changes outlined in https://opencast.jira.com/browse/MH-11222, and remove these statements in favour of logger statements. I also propose that we add this rule to the checkstyle ruleset so that we don't have to deal with this again going forward. Proposal closes EOD 2016-02-03. G","title":"Removing instances of print statements with a style rule #proposal"},{"location":"proposal-log/#how-to-release-a-new-opencast-version","text":"Proposed by Lars Kiesow lkiesow@uos.de , passed on Fri, 14 Aug 2015 12:54:51 UTC Hi everyone, serving as co-release manager for two versions of Opencast, I noticed that our current release process has some aspects of the release defined in a way that is more hindering than helpful and I want to #propose a slight change to these recommendations. I hereby #propose: 1. Get rid of the `master` branch, make `develop` the main branch. 2. Do not use the --no-ff flags for merges 3. Do not create versions/tags in a release branch. Separate them. Reasoning: 1. The short explanation whould be: When did you explicitely checked out `master` last time? People rarely do that. If I want a specific version, I use the tag, if not I want the release branch or `develop`. If you think about it, then the whole reason for `master` in GitFlow is to always provide the last stable version to users who just check out the repository and do nothing else. The problem with Opencast is, that we support multiple versions at the same time. If in a couple of weeks 1.6.2 is being released, it is the latest stable. Is it? If I check out `master`, however, I will still get 2.0 as we cannot merge 1.6.x afterwards. While you can grasp the reasons behind this, it is a bit confusing for users and it is much easier to just tell them to use the tag to check out a specific version. That it, if they do not use the tarballs from BitBucket anyway. 2. First of all, most people seem to be using BitBucket for auto-merging and it does not use --no-ff. So we are not really consistent anyway. Being consistent and using --no-ff would mean to forbid the usage of the BitBucket merge. Second, have a look at the confusing mess that are the current branches (I tried to find something in the visualization a while ago but gave up). It would be much cleaner to try using fast-forward merges. So instead of using non-fast-forward commits I would argue that we should instead try to use as many fast-forward commits as possible. 3. Once we decided to have the tags in our branches like this: ---- A ---- B (tagged) ----- C ---- D --> A is the commit containing the version that is decided to be released. B is the tagged version. It is exactly the same code as A except for the pom.xml versions that are modified. Finally C then reverts B as the modified version should not be part of the release branch, .... After C, the code is basically A again except for the history (which we later need to merge which can be problematic). D would then be the next \u201creal\u201d commit, meaning the next fix. Much easier to handle would be the following structure: ---- A ---- D --> \\ B (tagged) You do not have to revert that commit, you do not need to merge the easily conflicting pom.xml changes and in the end, you would anyway check out the tag using git checkout <tag> if you want that specific version Branching structure: To have a complete overview, this is what the new branching structure would look like: develop --*--*--*--*--*----*--------*--------*----> \\ / / r/x.y.z *--*--*---*--*--*--*--*--*----> \\ \\ * x.y.z-beta1 * x.y.z-rc1 Regards, Lars","title":"How to release a new Opencast version\u2026"},{"location":"proposal-log/#moving-away-from-the-3rd-party-scripts","text":"Proposed by Greg Logan gregorydlogan@gmail.com , passed by Fri, 24 Jul 2015 16:45:40 UTC Hi folks, As it stands right now we depend on the 3rd party tool script to install a great many of our 3rd party dependencies. These are utilities like tesseract, ffmpeg, sox, etc. This script is maintained by Matjaz, in his own time. I'd like to take a moment to thank him for a doing a great job on a particularly annoying aspect of supporting our work! I know it hasn't been easy, especially supporting vast number of different OS versions! With the release of 2.0 I noticed that our 3rd party tool script is becoming both a little out of date, and difficult to maintain. I took a quick look around and it seems like *most* of our dependencies are available from normal distribution repositories for Debian based systems, and I'm told that there is a similar situation for Redhat based systems. I am unsure of how many of our users are running Matterhorn on Mac, but I would hope that our developers who are working on Mac would be able to provide instructions and/or binaries for those users. The only dependency where there might be a universal sticking point is ffmpeg (due to patent concerns), however ffmpeg builds a full static binary with each release, so I assume we can either depend on this and/or cache them somewhere. What this means is that we can potentially remove the 3rd party script from our repository. I hereby #propose we find a way to do that, which would remove the 3rd party script from the repository and replace it with a number of new steps in the install documentation. G","title":"Moving away from the 3rd party scripts"},{"location":"proposal-log/#status-of-youtube-in-20-and-proposal-to-change-the-default-workflow","text":"Proposed by R\u00fcdiger Rolf rrolf@Uni-Osnabrueck.DE , passed on Sat, 13 Jun 2015 14:15:55 UTC Hi list! There was some discussion in the DevOps meeting yesterday if the Youtube distribution would work or not. I offered to check this. The good news first: IT WORKS! Just follow this manual and your Matterhorn - ups Opencast - is ready to distribute to Youtube. http://docs.opencast.org/r/2.0.x/admin/modules/youtubepublication/ The bad news: The default workflow definition does not really support the publishing on Youtube, as only one video file could be published by the current WOH. https://opencast.jira.com/browse/MH-10920 The reason is simple and the fix would be too. But there are some options to fix this: 1. Remove the option to distribute to Youtube from the default workflow definition, as the complicated configuration would have to come first anyway. 2. Only let \"presenter\" or \"presentation\" be published to Youtube. We would need a new youtube tag and add this to the compose operation and the youtube operation. 3. Introduce the composite operation to the workflow definition and publish only the resulting single stream to Youtube. 4. Upgrade the WOH to support publishing of multiple files. I would say that option 4 could be 2.1 goal, but not for 2.0. I would #propose to go for option 1, as nobody can use Youtube out-of-the-box anyway. And the admin could then setup an appropriate Youtube workflow for their needs too. Regards R\u00fcdiger","title":"Status of youtube in 2.0 and #proposal to change the default workflow"},{"location":"proposal-log/#episode-dublincore-catalog","text":"Proposed by Karen Dolan kdolan@dce.harvard.edu , Passed on Sat, 30 May 2015 12:39:05 UTC Dear Opencast-ees, The following proposal addresses MH-10821[1]. An issue that exposes a know long time ambiguity regarding metadata and the ingest service. The reason that its a proposal is that it normalizes the handling of inbound episode catalog metadata in the ingest service. 1) A new configuration parameter, boolean, for the Ingest Service. The config param identifies if episode metadata precedence is for Ingestee (i.e. Opencast system) or the Ingester (i.e. Capture Agent). For example: at our site, the scheduling entity is the metadata authority. All updates are made to the Scheduling endpoint. The Capture Agent always has stale episode catalog metadata. At other sites, updates are made on the Capture Agent directly. The community default can be for priority to the Capture Agent. 2) All Ingest endpoints perform the same consistent process to ensure that an episode catalog will exist, manually or automatically provided. 3) The process performs the following... 3.1. Gather data - Check if inbound media package contain a reference to an Episode DublinCore catalog and if that catalog contains a title. - Check if the inbound media package contains a title attribute. - Check if the Workflow service has a reference to the mediapackage's Episode Dublin Core catalog - Check if the Scheduler service retained a reference to the event's Episode Dublin Core catalog 3.2. Use config param to prioritize action on acquiring an Episode dc catalog for the media package If Capture Agent metadata takes precedence: - Take the inbound Episode dc catalog, if it exists - Take the Episode dc catalog from the workflow service, if it exists - Take the Episode dc catalog from the scheduler service, if it exists - Create an Episode dc catalog from the title in the media package,, if it exists - Create an Episode dc catalog using a default title (i.e. \"Recording-1234556XYZ\") If Opencast metadata takes precedence: - Take the Episode dc catalog from the workflow service, if it exists - Take the Episode dc catalog from the scheduler service, if it exists - Take the inbound Episode dc catalog if it exists - Create an Episode dc catalog from the title in the media package, if it exists - Create an Episode dc catalog using a default title (i.e. \"Recording-1234556XYZ\") I'll start a pull for the above, and appreciate any thoughts. Regards, Karen [1] https://opencast.jira.com/browse/MH-10821","title":"Episode DublinCore Catalog"},{"location":"proposal-log/#dropping-taglines","text":"Proposed by Greg Logan gregorydlogan@gmail.com , Passed on Fri, 29 May 2015 16:19:09 UTC Hi folks, I hereby propose that we drop the practice of having taglines. I propose this because we don't have a place in the new admin UI to put them, nor have I ever heard any of the adopters make use of it. I know we don't use it as a committing group, which means that *no one* is using them. G","title":"Dropping Taglines"},{"location":"proposal-log/#wiki-cleanup","text":"Proposed by Lars Kiesow lkiesow@uos.de , Passed on Fri, 24 May 2015 11:36:49 UTC Hi everyone, since we partly switched to our new documentation [1] I would like to make sure that the old and mostly outdated documentation goes away so that no one stumbles upon that. When I had a look at the wikis we currently have I noticed that most of our 17(!) wikis have not been touched in years and can probably go away. Here is a list of our wikis and what I #propose to do with/to them: Keep (maybe clean-up a bit): - Matterhorn Adopter Guides - Matterhorn Developer Wiki - Opencast Matterhorn D/A/CH - Opencast Matterhorn Espa\u00f1ol - LectureSight Export as PDF to archive the contents and then delete: - Matterhorn Release Docs - 1.0 - Matterhorn Release Docs - 1.1 - Matterhorn Release Docs - 1.2 - Matterhorn Release Docs - 1.3 - Matterhorn Release Docs - 1.4 - Matterhorn Release Docs - 1.5 - Matterhorn Release Docs - TRUNK Keep until 2.1 is out then export as PDF and delete: - Matterhorn Release Docs - 1.6 Just delete: - Analytic video annotation - Infra - Matterhorn Documents - Opencast Community Please let me know if you agree or disagree with this proposal. Regards, Lars [1] http://documentation.opencast.org","title":"Wiki Cleanup"},{"location":"proposal-log/#jira-clean-up","text":"Proposed by Lars Kiesow lkiesow@uos.de , Passed on Fri, 8 May 2015 11:52:16 UTC Hi everyone, as discussed in the technical meeting, I hereby #propose: The \u201cBlocker\u201d and \u201cRelease Blocker\u201d severity status are more or less redundant. As part of cleaning up Jira, let us remove the \u201cRelease Blocker\u201d severity in favor of \u201cBlocker\u201d. As footnote, some statistics: Since the beginning of 2014, 70 Release Blockers have been files in Jira while mere *8* Blockers have been files. Regards, Lars","title":"Jira Clean-Up"},{"location":"proposal-log/#opencast-documentation","text":"Proposed by R\u00fcdiger Rolf rrolf@uni-osnabrueck.de , Passed on Sat, 02 May 2015 14:43:28 UTC Hi all, Tobias, Basil, Lars and I discussed status of the current migration of the Opencast (Matterhorn) documentation to GIT. We still see some open issues that need clarification so we would like to propose the following points: *1. Formating and Hosting of the Documentation * We want to use https://readthedocs.org to or a similar service create a more appealing HTML version from the Markdown of the documentation. The documentation will be versioned there so that for older versions the documentation is still available. By default the \"latest\" version is shown. The versions of the documenation will be generated based on the release branches. *2. Structure of the Documentation* We see the documentation in*Git *separating into 3 sections: - /Administration Guide/: with information about the installation, configuration, customization and integration. This will be the part of information by an administrator to setup Opencast. - /Developer Guide/: All information related to implementation details of Opencast, so that this will be updated in a pull request (API changes, module descriptions, architecture). The development process documents should also go here as only committers usually should change these. - /User Guide/: Documentation of the (new) Admin UI that was already started by Entwine and the Engage UI (especially Theodul Player). This guide should only describe options available on the UIs. Within the *Wiki* we still see the need for 2 sections: - /Developer Wiki/: Proposals, working documents and meeting notes will be kept here so that anybody can edit these. So information not to close to any existing implementation that might still be in a process of discussion can be found here. - /Adopters Wiki/: This can be the place where adopters share their best practises, configurations, hardware recommendations, third-party software documentation etc. Again anyone can contribute to this wiki. The difference between the Wiki and Git is in the first line that the Git documentation should become a quality assured ressource for Opencast users. The Git documentation should be reviewed within the release process and it will be part of the review process of a pull request, to make sure that the needed documentation changes have been contributed too. The Wikis on the other hand should be a more open platform where everybody can contribute and users might find cookbooks to enhance their system, or they can share ideas. So now we would like to get your opinion on this proposal. Thank you, R\u00fcdiger","title":"Opencast Documentation"},{"location":"proposal-log/#requirement-specification","text":"Proposed by Lars Kiesow lkiesow@uos.de , Passed on Thu, 16 Apr 2015 15:55:31 UTC On list or IRC we often see that people do not really know the current requirements for a specific version of Opencast Matterhorn. Of course there are the pom.xml files specifying internal dependencies, but there is nothing for 3rd-party-tools, ... It would be nice to add a file specifying these requirements in a format that is easy to parse and can hence be used for automatic scripts to generate dependency lists, ... That is why I hereby #propose to add a requirements.xml file that specifies the requirements for Opencast Matterhorn: - Required tools including versions - Which modules require which tools - Which modules conflict with each other (negative requirement) This is mainly what is not specified by the pom.xml files yet.","title":"Requirement Specification"},{"location":"proposal-log/#jira-clean-up-tags-vs-labels","text":"Proposed by Lars Kiesow lkiesow@uos.de , Passed on Thu, 19. Mar 2015 15:43:20 UTC \u2026then hereby I officially #propose removing the labels from Jira. For more details, have a look at the mail thread at: https://groups.google.com/a/opencast.org/forum/#!topic/matterhorn/vIdWQkZmbdQ","title":"Jira Clean-Up (Tags VS Labels)"},{"location":"proposal-log/#ffmpeg-update","text":"Proposed by Lars Kiesow lkiesow@uos.de , Passed on Sat, 14 Mar 2015 22:12:18 UTC Looking at the FFmpeg project for the last two years, you will notice that they developed a pretty stable release cycle with a release of a new stable version approximately every three month. To stop us from having to propose an update again and again, I hereby propose the following general rule for our support of FFmpeg: A Matterhorn release will oficially support the latest stable version of FFmpeg released at the time the release branch is cut and all other FFmpeg versions with the same major version number released afterwards. For example, for Matterhorn 2 this would mean that we will officially support FFmpeg 2.5.4 and all later 2.x versions like 2.6 which has been released on the 7th of March or a possible 2.7 onece it is released. We would, however, not necessarily support an FFmpeg 3 as it *might* come with an interface change that *could* break compatibility. That obviously does not mean that older versions of FFmpeg just stop working. In fact, most parts of the default Matterhorn configuration should at the moment still work with FFmpeg 1.x but we will not test or fix compatibility problems.","title":"FFmpeg Update"},{"location":"proposal-log/#proposal-log","text":"Proposed by Lars Kiesow lkiesow@uos.de , Passed on Sat, 14 Mar 2015 16:35:08 UTC It would be wonderful if we had a central place to look up the proposals that have passed. That is why I hereby propose that: - We create a proposal log in our new documentation containing all proposals that have passed on list. - A proposal will become effective only after it is written down in that log. That should usually be done by the person who sent out that proposal. This will, of course, not affect the existing decision making rules (proposal on list, marked with #proposal, lazy consensus after three days, no -1, ...)","title":"Proposal Log"},{"location":"qa-coordinator/","text":"QA Coordinator Guide Role The QA coordinator is ultimately responsible for making sure that quality assurance (QA) processes are occurring during each release window, and for other project related tasks. The QA Coordinator has the following duties: Establish and manage QA process, technologies, and methods Manage CI infrastructure Create and expand automated testing infrastructure Ensure committers are following testing procedures Ensure codebase stability Cleanup obsolete branches Help with merge issues Setup and manage automated distributable builds Code cleanup Organize community/committers Remain active on list Organize Technical meeting Encourage additional committers to join the project Manage crowdin translators - ensure join requests are dealt with Handle (security issue reports)[security.md], and notifications once handled Occasionally purge non-active committers, moving them to the emeritus list in the main pom, and removing their permissions in JIRA Expand documentation Ensure documentation is build and published Expand documentation, and/or organize volunteer documentation tasks","title":"QA Coordinator"},{"location":"qa-coordinator/#qa-coordinator-guide","text":"","title":"QA Coordinator Guide"},{"location":"qa-coordinator/#role","text":"The QA coordinator is ultimately responsible for making sure that quality assurance (QA) processes are occurring during each release window, and for other project related tasks. The QA Coordinator has the following duties: Establish and manage QA process, technologies, and methods Manage CI infrastructure Create and expand automated testing infrastructure Ensure committers are following testing procedures Ensure codebase stability Cleanup obsolete branches Help with merge issues Setup and manage automated distributable builds Code cleanup Organize community/committers Remain active on list Organize Technical meeting Encourage additional committers to join the project Manage crowdin translators - ensure join requests are dealt with Handle (security issue reports)[security.md], and notifications once handled Occasionally purge non-active committers, moving them to the emeritus list in the main pom, and removing their permissions in JIRA Expand documentation Ensure documentation is build and published Expand documentation, and/or organize volunteer documentation tasks","title":"Role"},{"location":"release-manager/","text":"Release Manager Guide The single most important duty of release managers is to keep an eye on their release, notify the community about possible problems in a timely manner and encourage community members to help out if needed. While working on Opencast's code is often done as well during the release process, for release managers this is secondary to the communication and management role and with few exceptions no requirement for this position. The community has a number of expectations for release managers, and their handling of the problems which may arise during the release cycle. The core of these expectations are: The development process should be followed or amended if required The release should be on time The release should not have any critical technical, usability or security issues This means that release managers may need to force decisions around the release, help negotiate the acceptance or rejection of contributions and provide regular updates about the release on list and during the technical and adopter meetings. It is important to note that, while release managers drive the release process, the committer body is in charge of both the work and the decision making, meaning that votes and successful proposals from this body take precedence over release manager decisions. Responsibilities While a general rule is certainly just to look out for the release, work together with the community to make the release work properly and be pragmatic about the process, there are a few tasks which really can only be done by release managers. For all of these tasks, it's generally a good idea to look at previous releases and at their solutions for the tasks. Often, completing the task is simply a matter of repeating or updating previous work (e.g. advance the previous release schedule by six months). Release Notes It's usually a good idea to create or clean the release notes page early in the release cycle. This allows for a place to put the release schedule, short descriptions of features or noteworthy configuration changes early on. The Release Schedule Releases should happen twice a year, usually within a time span of 9.5 months between the cut of the previous release branch and the final release. The release managers should create a release schedule as soon as possible, announce it on list and publish it on the release notes page. Release Branch According to the set release schedule, at one point a release branch should be cut, effectively marking a feature freeze for a given release. This branch is split off develop and should be named r/N.x (e.g. r/6.x for the Opencast 6 release branch). Example on how to create the Opencast 7 release branch: Check out develop and make sure it has the latest state (replace <remote> with your remote name for the community repository): git checkout develop git pull <remote> develop Make sure you did not modify any files. If you did, stash those changes: git status # check for modified files git stash # stash them if necessary Create and push the new release branch: git checkout -b r/7.x git push <remote> r/7.x That is it for the release branch. Now update the versions in develop in preparation for the next release: git checkout develop mvn versions:set -DnewVersion=8-SNAPSHOT versions:commit Have a look at the changes. Make sure that nothing else was modified: git diff git status | grep modified: | grep -v pom.xml # this should have no output If everything looks fine, commit the changes and push it to the community repository: git add $(git status | grep 'modified:.*pom.xml' | awk '{print $2;}') git commit -s -m 'Bumping pom.xml Version Numbers' git push <remote> develop At this point, the developer community should then be notified by writing an email like the following to the developers list: To: dev@opencast.org Subject: Release Branch for Opencast <version> Cut Hi everyone, the Opencast <version> release branch (r/<version>) has been cut. Pull requests for bug fixes may still be made against this branch but, as usual, features should go into develop instead. Remember the release schedule for this release: <release_schedule> As always, we hope to have a lot of people testing this version, especially during the public QA phase. Please report any bugs or issues you encounter. For testing, you may use https://stable.opencast.org if you do not want to set up a test server yourself. The server is reset on a daily basis and will follow the new release branch with its next rebuild. Additionally, look out for announcements regarding container and package builds for testing on list if you want to run your own system but do not want to build Opencast from source. Status of Translations After the release branch is cut, the release managers should check if there are languages to be in- or excluded for the upcoming release as specified by the criteria in the localization documentation and notify the community about the status of Opencast's translations if necessary. Example announcement for included languages: To: users@opencast.org Subject: Opencast <VERSION>: Translation Status Hi everyone, while checking the translation statuses of the languages available on Crowdin[1], we have found that the following languages meet the criteria to be included in Opencast <VERSION>: - <LANGUAGE1> (<PERCENTAGE1>) - <LANGUAGE2> (<PERCENTAGE2>) - .... [1] Opencast on Crowdin https://crowdin.com/project/opencast-community [2] Inclusion and exclusion of translations https://docs.opencast.org/develop/developer/localization/#inclusion-and-exclusion-of-translations Example announcement for endangered languages: To: users@opencast.org Subject: Opencast <VERSION>: <LANGUAGE> Translation Endangered Hi everyone, while checking the <LANGUAGE> translation status of Opencast, we have found that it is only <PERCENTAGE> translated. This is not enough to justify its inclusion in the upcoming Opencast release[1], meaning that the <LANGUAGE> translation is in danger of being removed in Opencast <VERSION> if its status stays the same. To save the <LANGUAGE> translation from removal, it needs to be translated at least 90% before <DATE>. Sincerely, Your Opencast <VERSION> Release Managers [1] Inclusion and Exclusion of Translations https://docs.opencast.org/develop/developer/localization/#inclusion-and-exclusion-of-translations A specific translation week may be announced using an email like this: To: users@opencast.org Subject: Opencast <version>: Translation Week Hi everyone, starting on <date> the Opencast <version> translation week will take place, during which we particular focus on improving Opencast's translations. Can I help? ----------- Everybody that speaks a different language or dialect and feels confident enough to participate can participate. How can I help? --------------- We use Crowdin [1] to manage translations. Please sign up and request to help with a particular language. What is the current status? --------------------------- Fully translated: - \u2026 Mostly translated (>80% translated): - \u2026 Endangered translations (\u226480% translated): - \u2026 Note that we can add any additional languages you are willing to translate. If you have any additional questions, please do not hesitate to ask. [1] https://crowdin.com/project/opencast-community Moderation of Peer Reviews Release managers should regularly check open pull requests for possible problems (no reviewers, discussions in need of moderation, issues that should be raised to community awareness, \u2026) and bring these up in the technical meeting, on the developer list or wherever appropriate. Merging Release Branches To not have to merge bug fixes into several branches and create several pull requests, the release branch should be merged down on a regular basis. Assuming, for example, that r/6.x is the latest release branch, merges should happen like this: r/5.x \u2192 r/6.x \u2192 develop While any committer may do this at any time, it is good practice for release managers to do this for their release branches on a regular basis. For example, to merge the latest release branch into develop , follow these steps: Update your local repository git fetch <remote> Update develop : git checkout develop git merge <remote>/develop # this should be a fast-forward merge Merge the release branch. Note that if large merge conflicts arise, you may ask for help from the people creating the problematic patches: git merge <remote>/r/6.x Push the updated branch into the community repository: git push <remote> develop Updating Translations Updating the localization translations is automated for existing translation files. If new files need to be added, it is something that should happen early during the release process. If files need to be removed, this needs to be done manually. Releasing The following steps outline the necessary steps for cutting the final release: Switch to and update your release branch and ensure the latest state of the previous release branch is merged: git checkout r/6.x git fetch <remote> git merge <remote>/r/6.x git merge <remote>/r/5.x Add the release notes, and update the changelog. The create-changelog helper script is a convenient tool for this. cd docs/guides/admin/docs/ vim releasenotes.md vim changelog.md git commit -S releasenotes.md changelog.md -m 'Updated Release Notes' git push <remote> r/6.x Switch to a new branch to create the release (name does not really matter): git checkout -b tmp-6.0 Make the version changes for the release: mvn versions:set -DnewVersion=6.0 versions:commit Have a look at the changes. Make sure that nothing else was modified: git diff git status | grep modified: | grep -v pom.xml # this should yield no output Commit the changes and create a release tag: git add $(git status | grep 'modified:.*pom.xml' | awk '{print $2;}') git commit -S -m 'Opencast 6.0' git tag -s 6.0 Push the tag to the community repository (you can remove the branch afterwards): git push <remote> 6.0:6.0 Push the built artifacts to Maven. Bug the QA Coordinator to do this so that he remembers to set this up from the CI servers. If you want to do this yourself please read the infra documentation . Create a new release on GitHub using the graphical user interface to upload the distribution tarballs. Finally, send a release notice to Opencast's announcement list. Note that posting to this list is restricted to those who need access to avoid general discussions on that list. In case you do not already have permissions to post on this list, please ask to be given permission. For the message, you may use the following template: To: announcements@opencast.org Subject: Opencast <VERSION> Released Hi everyone, it is my pleasure to announce that Opencast <VERSION> has been released: https://github.com/opencast/opencast/releases The documentation for this release can be found at: https://docs.opencast.org/r/<VERSION>/admin/ RPM and Debian packages as well as Docker images will be available soon. Watch for announcements on the users list. To all committers and involved contributors, thank you for all your work. This could not have happened without you and I am glad we were able to work together and get this release out. Appointment of Next Release Manager After the release branch is cut, all work on develop is effectively the preparation for the next release. At this point, the release managers should send an inquiry to the development list to identify volunteers for the next release. For that, this email template may be used: To: dev@opencast.org Subject: Opencast <NEXT_RELEASE> release managers wanted Hi everyone, the Opencast community is looking for release managers for the upcoming <NEXT_RELEASE> (Feature freeze around <DATE>, release around <DATE>). Note that the release manager's job contains very little technical work. Instead, they mostly focus on motivation and coordination of the community during the release phase. The role of release managers is described in more detail in the Opencast development documentation: https://docs.opencast.org/develop/developer/release-manager/ In the past, it has proven good practice to have two people fill this job as co-release managers to help keep up the process during vacation, sickness and in case of local emergencies. I am looking forward to your applications on list, please voice your interest until <DATE_ROUGHLY_2_WEEKS_IN_THE_FUTURE>. In the case where someone steps up and offers to fill in the role of a release manager for the upcoming release, a vote is held on the committers list to determine whether the candidates are deemed suitable for the position. This email template may be used to initiate the vote: To: committers@opencast.org Subject: [#vote] Vote on Release Managers of Opencast <NEXT_RELEASE> Hi everyone, I am happy to announce that the following community members have volunteered themselves for the position of the Opencast <NEXT_RELEASE> release manager and have expressed the intention of sharing the position: <NAME, INSTITITION> <NAME, INSTITUTION> I hereby open the vote on accepting them for this position. The vote will be open for the coming 72h. Once the voting is complete, the result should be announced on the development list: As an example: To: dev@opencast.org Subject: Release Managers of Opencast <NEXT_RELEASE> Hi everyone, it is my pleasure to announce that the following people have been elected to be the release managers for the upcoming Opencast <NEXT_RELEASE> release: <NAME, INSTITUTION> <NAME, INSTITUTION> We wish to thank them for volunteering, and hope the release goes smoothly!","title":"Release Manager"},{"location":"release-manager/#release-manager-guide","text":"The single most important duty of release managers is to keep an eye on their release, notify the community about possible problems in a timely manner and encourage community members to help out if needed. While working on Opencast's code is often done as well during the release process, for release managers this is secondary to the communication and management role and with few exceptions no requirement for this position. The community has a number of expectations for release managers, and their handling of the problems which may arise during the release cycle. The core of these expectations are: The development process should be followed or amended if required The release should be on time The release should not have any critical technical, usability or security issues This means that release managers may need to force decisions around the release, help negotiate the acceptance or rejection of contributions and provide regular updates about the release on list and during the technical and adopter meetings. It is important to note that, while release managers drive the release process, the committer body is in charge of both the work and the decision making, meaning that votes and successful proposals from this body take precedence over release manager decisions.","title":"Release Manager Guide"},{"location":"release-manager/#responsibilities","text":"While a general rule is certainly just to look out for the release, work together with the community to make the release work properly and be pragmatic about the process, there are a few tasks which really can only be done by release managers. For all of these tasks, it's generally a good idea to look at previous releases and at their solutions for the tasks. Often, completing the task is simply a matter of repeating or updating previous work (e.g. advance the previous release schedule by six months).","title":"Responsibilities"},{"location":"release-manager/#release-notes","text":"It's usually a good idea to create or clean the release notes page early in the release cycle. This allows for a place to put the release schedule, short descriptions of features or noteworthy configuration changes early on.","title":"Release Notes"},{"location":"release-manager/#the-release-schedule","text":"Releases should happen twice a year, usually within a time span of 9.5 months between the cut of the previous release branch and the final release. The release managers should create a release schedule as soon as possible, announce it on list and publish it on the release notes page.","title":"The  Release Schedule"},{"location":"release-manager/#release-branch","text":"According to the set release schedule, at one point a release branch should be cut, effectively marking a feature freeze for a given release. This branch is split off develop and should be named r/N.x (e.g. r/6.x for the Opencast 6 release branch). Example on how to create the Opencast 7 release branch: Check out develop and make sure it has the latest state (replace <remote> with your remote name for the community repository): git checkout develop git pull <remote> develop Make sure you did not modify any files. If you did, stash those changes: git status # check for modified files git stash # stash them if necessary Create and push the new release branch: git checkout -b r/7.x git push <remote> r/7.x That is it for the release branch. Now update the versions in develop in preparation for the next release: git checkout develop mvn versions:set -DnewVersion=8-SNAPSHOT versions:commit Have a look at the changes. Make sure that nothing else was modified: git diff git status | grep modified: | grep -v pom.xml # this should have no output If everything looks fine, commit the changes and push it to the community repository: git add $(git status | grep 'modified:.*pom.xml' | awk '{print $2;}') git commit -s -m 'Bumping pom.xml Version Numbers' git push <remote> develop At this point, the developer community should then be notified by writing an email like the following to the developers list: To: dev@opencast.org Subject: Release Branch for Opencast <version> Cut Hi everyone, the Opencast <version> release branch (r/<version>) has been cut. Pull requests for bug fixes may still be made against this branch but, as usual, features should go into develop instead. Remember the release schedule for this release: <release_schedule> As always, we hope to have a lot of people testing this version, especially during the public QA phase. Please report any bugs or issues you encounter. For testing, you may use https://stable.opencast.org if you do not want to set up a test server yourself. The server is reset on a daily basis and will follow the new release branch with its next rebuild. Additionally, look out for announcements regarding container and package builds for testing on list if you want to run your own system but do not want to build Opencast from source.","title":"Release Branch"},{"location":"release-manager/#status-of-translations","text":"After the release branch is cut, the release managers should check if there are languages to be in- or excluded for the upcoming release as specified by the criteria in the localization documentation and notify the community about the status of Opencast's translations if necessary. Example announcement for included languages: To: users@opencast.org Subject: Opencast <VERSION>: Translation Status Hi everyone, while checking the translation statuses of the languages available on Crowdin[1], we have found that the following languages meet the criteria to be included in Opencast <VERSION>: - <LANGUAGE1> (<PERCENTAGE1>) - <LANGUAGE2> (<PERCENTAGE2>) - .... [1] Opencast on Crowdin https://crowdin.com/project/opencast-community [2] Inclusion and exclusion of translations https://docs.opencast.org/develop/developer/localization/#inclusion-and-exclusion-of-translations Example announcement for endangered languages: To: users@opencast.org Subject: Opencast <VERSION>: <LANGUAGE> Translation Endangered Hi everyone, while checking the <LANGUAGE> translation status of Opencast, we have found that it is only <PERCENTAGE> translated. This is not enough to justify its inclusion in the upcoming Opencast release[1], meaning that the <LANGUAGE> translation is in danger of being removed in Opencast <VERSION> if its status stays the same. To save the <LANGUAGE> translation from removal, it needs to be translated at least 90% before <DATE>. Sincerely, Your Opencast <VERSION> Release Managers [1] Inclusion and Exclusion of Translations https://docs.opencast.org/develop/developer/localization/#inclusion-and-exclusion-of-translations A specific translation week may be announced using an email like this: To: users@opencast.org Subject: Opencast <version>: Translation Week Hi everyone, starting on <date> the Opencast <version> translation week will take place, during which we particular focus on improving Opencast's translations. Can I help? ----------- Everybody that speaks a different language or dialect and feels confident enough to participate can participate. How can I help? --------------- We use Crowdin [1] to manage translations. Please sign up and request to help with a particular language. What is the current status? --------------------------- Fully translated: - \u2026 Mostly translated (>80% translated): - \u2026 Endangered translations (\u226480% translated): - \u2026 Note that we can add any additional languages you are willing to translate. If you have any additional questions, please do not hesitate to ask. [1] https://crowdin.com/project/opencast-community","title":"Status of Translations"},{"location":"release-manager/#moderation-of-peer-reviews","text":"Release managers should regularly check open pull requests for possible problems (no reviewers, discussions in need of moderation, issues that should be raised to community awareness, \u2026) and bring these up in the technical meeting, on the developer list or wherever appropriate.","title":"Moderation of Peer Reviews"},{"location":"release-manager/#merging-release-branches","text":"To not have to merge bug fixes into several branches and create several pull requests, the release branch should be merged down on a regular basis. Assuming, for example, that r/6.x is the latest release branch, merges should happen like this: r/5.x \u2192 r/6.x \u2192 develop While any committer may do this at any time, it is good practice for release managers to do this for their release branches on a regular basis. For example, to merge the latest release branch into develop , follow these steps: Update your local repository git fetch <remote> Update develop : git checkout develop git merge <remote>/develop # this should be a fast-forward merge Merge the release branch. Note that if large merge conflicts arise, you may ask for help from the people creating the problematic patches: git merge <remote>/r/6.x Push the updated branch into the community repository: git push <remote> develop","title":"Merging Release Branches"},{"location":"release-manager/#updating-translations","text":"Updating the localization translations is automated for existing translation files. If new files need to be added, it is something that should happen early during the release process. If files need to be removed, this needs to be done manually.","title":"Updating Translations"},{"location":"release-manager/#releasing","text":"The following steps outline the necessary steps for cutting the final release: Switch to and update your release branch and ensure the latest state of the previous release branch is merged: git checkout r/6.x git fetch <remote> git merge <remote>/r/6.x git merge <remote>/r/5.x Add the release notes, and update the changelog. The create-changelog helper script is a convenient tool for this. cd docs/guides/admin/docs/ vim releasenotes.md vim changelog.md git commit -S releasenotes.md changelog.md -m 'Updated Release Notes' git push <remote> r/6.x Switch to a new branch to create the release (name does not really matter): git checkout -b tmp-6.0 Make the version changes for the release: mvn versions:set -DnewVersion=6.0 versions:commit Have a look at the changes. Make sure that nothing else was modified: git diff git status | grep modified: | grep -v pom.xml # this should yield no output Commit the changes and create a release tag: git add $(git status | grep 'modified:.*pom.xml' | awk '{print $2;}') git commit -S -m 'Opencast 6.0' git tag -s 6.0 Push the tag to the community repository (you can remove the branch afterwards): git push <remote> 6.0:6.0 Push the built artifacts to Maven. Bug the QA Coordinator to do this so that he remembers to set this up from the CI servers. If you want to do this yourself please read the infra documentation . Create a new release on GitHub using the graphical user interface to upload the distribution tarballs. Finally, send a release notice to Opencast's announcement list. Note that posting to this list is restricted to those who need access to avoid general discussions on that list. In case you do not already have permissions to post on this list, please ask to be given permission. For the message, you may use the following template: To: announcements@opencast.org Subject: Opencast <VERSION> Released Hi everyone, it is my pleasure to announce that Opencast <VERSION> has been released: https://github.com/opencast/opencast/releases The documentation for this release can be found at: https://docs.opencast.org/r/<VERSION>/admin/ RPM and Debian packages as well as Docker images will be available soon. Watch for announcements on the users list. To all committers and involved contributors, thank you for all your work. This could not have happened without you and I am glad we were able to work together and get this release out.","title":"Releasing"},{"location":"release-manager/#appointment-of-next-release-manager","text":"After the release branch is cut, all work on develop is effectively the preparation for the next release. At this point, the release managers should send an inquiry to the development list to identify volunteers for the next release. For that, this email template may be used: To: dev@opencast.org Subject: Opencast <NEXT_RELEASE> release managers wanted Hi everyone, the Opencast community is looking for release managers for the upcoming <NEXT_RELEASE> (Feature freeze around <DATE>, release around <DATE>). Note that the release manager's job contains very little technical work. Instead, they mostly focus on motivation and coordination of the community during the release phase. The role of release managers is described in more detail in the Opencast development documentation: https://docs.opencast.org/develop/developer/release-manager/ In the past, it has proven good practice to have two people fill this job as co-release managers to help keep up the process during vacation, sickness and in case of local emergencies. I am looking forward to your applications on list, please voice your interest until <DATE_ROUGHLY_2_WEEKS_IN_THE_FUTURE>. In the case where someone steps up and offers to fill in the role of a release manager for the upcoming release, a vote is held on the committers list to determine whether the candidates are deemed suitable for the position. This email template may be used to initiate the vote: To: committers@opencast.org Subject: [#vote] Vote on Release Managers of Opencast <NEXT_RELEASE> Hi everyone, I am happy to announce that the following community members have volunteered themselves for the position of the Opencast <NEXT_RELEASE> release manager and have expressed the intention of sharing the position: <NAME, INSTITITION> <NAME, INSTITUTION> I hereby open the vote on accepting them for this position. The vote will be open for the coming 72h. Once the voting is complete, the result should be announced on the development list: As an example: To: dev@opencast.org Subject: Release Managers of Opencast <NEXT_RELEASE> Hi everyone, it is my pleasure to announce that the following people have been elected to be the release managers for the upcoming Opencast <NEXT_RELEASE> release: <NAME, INSTITUTION> <NAME, INSTITUTION> We wish to thank them for volunteering, and hope the release goes smoothly!","title":"Appointment of Next Release Manager"},{"location":"reviewing-and-merging/","text":"Reviewing, Merging and Declining Pull Requests Before a patch is merged into an official branch, it needs to be reviewed by a committer. During a review, the reviewer tries to make sure that the patch merges without conflicts, that it works as expected and that it does not break anything else. If the reviewer discovers any kind of issue, he should leave a comment in the pull request view of GitHub and let the contributor fix the problem. Reviewer and contributor should work together to fix any problem with the pull requests before it is ready to go into the codebase. Reviewing Rules Reviews and merges need to be done by committers Reviewers should come from a different institution than the contributor Feature pull requests are only allowed to be merged into the branch develop Pull requests that change translation keys are only allowed to be merged into the branch develop After taking up a review (or being assigned to one), a basic review has to be done within the following 14 days for a bug fix or 21 days for a feature. A basic review means that either some issues should be pointed out or the pull request should be approved No pull request shall be without a reviewer for more than 14 days. If no committer is willing to take up the review on their own, the review will be assigned Any committer may merge an arbitrary set of approved pull requests, even if he is not the official reviewer, given that: The committer sends an announcement to the development list 24 hours in advance, including a list of the pull requests to be merged and no other committer objects to that The committer checks that all the patches are working (which basically means to review the patch set as a whole) A reviewer may decline a pull request if issues were pointed out but were neither fixed nor discussed for more than 14 days. It is generally suggested to try to contact the contributor before declining the pull request and to additionally bring the problem up at the technical meeting Reviewing a Pull Request There is no list of things you are required to do as reviewer of a pull request. Our primary goals in doing reviews for all pull requests are to ensure that: There are no bugs in the pull request The feature works as advertised It does not break any other features There are no obvious legal problems Licenses seem to be fine Licenses are added to the NOTICES file Feature documentation is in place An upgrade path exists (usually relevant for database changes) The exact review process heavily depends on the type, size and purpose of the pull request. Here are some things a reviewer should usually do: Check if code looks sensible (e.g. no nonsensical files, no obvious formatting errors, no large binary files, \u2026) Locally run Opencast and verify that the issue has been fixed (as described in the pull request and/or the GitHub issue) or the feature does what it is supposed Check the documentation Some changes require special attention: Folder Description etc/listproviders Changes here might need to be reflected in the static mockup data for the Admin UI facade found in modules/admin-ui/src/test/resources/app/admin-ng/resources modules/admin-ui/src/main/java In case the interface of the Admin UI facade changes, those changes need to be also reflected in the static mockup data for the Admin UI facade found in modules/admin-ui/src/test/resources/app.","title":"Reviewing, Merging and Declining Pull Requests"},{"location":"reviewing-and-merging/#reviewing-merging-and-declining-pull-requests","text":"Before a patch is merged into an official branch, it needs to be reviewed by a committer. During a review, the reviewer tries to make sure that the patch merges without conflicts, that it works as expected and that it does not break anything else. If the reviewer discovers any kind of issue, he should leave a comment in the pull request view of GitHub and let the contributor fix the problem. Reviewer and contributor should work together to fix any problem with the pull requests before it is ready to go into the codebase.","title":"Reviewing, Merging and Declining Pull Requests"},{"location":"reviewing-and-merging/#reviewing-rules","text":"Reviews and merges need to be done by committers Reviewers should come from a different institution than the contributor Feature pull requests are only allowed to be merged into the branch develop Pull requests that change translation keys are only allowed to be merged into the branch develop After taking up a review (or being assigned to one), a basic review has to be done within the following 14 days for a bug fix or 21 days for a feature. A basic review means that either some issues should be pointed out or the pull request should be approved No pull request shall be without a reviewer for more than 14 days. If no committer is willing to take up the review on their own, the review will be assigned Any committer may merge an arbitrary set of approved pull requests, even if he is not the official reviewer, given that: The committer sends an announcement to the development list 24 hours in advance, including a list of the pull requests to be merged and no other committer objects to that The committer checks that all the patches are working (which basically means to review the patch set as a whole) A reviewer may decline a pull request if issues were pointed out but were neither fixed nor discussed for more than 14 days. It is generally suggested to try to contact the contributor before declining the pull request and to additionally bring the problem up at the technical meeting","title":"Reviewing Rules"},{"location":"reviewing-and-merging/#reviewing-a-pull-request","text":"There is no list of things you are required to do as reviewer of a pull request. Our primary goals in doing reviews for all pull requests are to ensure that: There are no bugs in the pull request The feature works as advertised It does not break any other features There are no obvious legal problems Licenses seem to be fine Licenses are added to the NOTICES file Feature documentation is in place An upgrade path exists (usually relevant for database changes) The exact review process heavily depends on the type, size and purpose of the pull request. Here are some things a reviewer should usually do: Check if code looks sensible (e.g. no nonsensical files, no obvious formatting errors, no large binary files, \u2026) Locally run Opencast and verify that the issue has been fixed (as described in the pull request and/or the GitHub issue) or the feature does what it is supposed Check the documentation Some changes require special attention: Folder Description etc/listproviders Changes here might need to be reflected in the static mockup data for the Admin UI facade found in modules/admin-ui/src/test/resources/app/admin-ng/resources modules/admin-ui/src/main/java In case the interface of the Admin UI facade changes, those changes need to be also reflected in the static mockup data for the Admin UI facade found in modules/admin-ui/src/test/resources/app.","title":"Reviewing a Pull Request"},{"location":"scheduler/","text":"Scheduler Modules The scheduler service consists of the following modules: scheduler-api An API module defining the core scheduler functions and properties. scheduler-impl The default implementation of the scheduler service as an OSGi service. scheduler-remote The remote implementation of the scheduler service as an OSGi service. Database The scheduler service stores snapshots using the AssetManager and additionally uses two tables: oc_scheduled_extended_event Manages scheduled event meta data such as start date, end date, capture agent ID, and so on. oc_scheduled_last_modified Manages the last recording modification date of a status change on an event sent by the capture agent. API Here is a sample to create a single event with the scheduler Java API. public void createEvent(Event event) { schedulerService.addEvent(event.getStart(), event.getEnd(), event.getAgentId(), event.getUsers(), event.getMediaPackage(), event.getWfProperties(), event.getCaMetadata(), event.getSource(), \"organization-xyz-script\"; }","title":"Scheduler"},{"location":"scheduler/#scheduler","text":"","title":"Scheduler"},{"location":"scheduler/#modules","text":"The scheduler service consists of the following modules: scheduler-api An API module defining the core scheduler functions and properties. scheduler-impl The default implementation of the scheduler service as an OSGi service. scheduler-remote The remote implementation of the scheduler service as an OSGi service.","title":"Modules"},{"location":"scheduler/#database","text":"The scheduler service stores snapshots using the AssetManager and additionally uses two tables: oc_scheduled_extended_event Manages scheduled event meta data such as start date, end date, capture agent ID, and so on. oc_scheduled_last_modified Manages the last recording modification date of a status change on an event sent by the capture agent.","title":"Database"},{"location":"scheduler/#api","text":"Here is a sample to create a single event with the scheduler Java API. public void createEvent(Event event) { schedulerService.addEvent(event.getStart(), event.getEnd(), event.getAgentId(), event.getUsers(), event.getMediaPackage(), event.getWfProperties(), event.getCaMetadata(), event.getSource(), \"organization-xyz-script\"; }","title":"API"},{"location":"security/","text":"Opencast Security Issue Process This document summarizes how issues are reported, who is responsible for what actions, and how things are handled internally. The instructions here should be considered the minimum , and further/faster responses are certainly possible depending on the severity and type of issue. For Users and Administrators What to do if you find a security issue Report it to security@opencast.org ! Please include a complete description of the issue, including which version(s) it affects, as well as any steps required to reproduce it. This email will be sent privately to the full list of committers for the project. You should receive an email acknowledging the issue from the QA coordinator, or release manager(s) within 3 business days. At this point, depending on the issue, you may be asked for your Attlassian and GitHub logins. It is enouraged to have one, since internal discussion on JIRA will be restricted to committers and the reporter. Likewise, all code reviews will be done on in an internal GitHub repository. What to do once you have reported your security issue Wait, and/or help fix the issue. The committers will work to find a mitigation for the immediate case, and a proper solution for the long term. This may take a while, please be patient. It may also require you to test it if the issue is derived from a complex system that most committers would not necessarily have access to (e.g: your LDAP or LTI servers). What happens once the issue has been fixed? A notice to security-notices@opencast.org will be released once the issue has been resolved and proper patches applied to the codebase. This notice may be accompanied by a release, or instructions on how to patch a live system depending on the issue. For Committers What to do with a security report If no one else has, draft a new security advisory on GitHub and reply on security@opencast.org with a link to the draft. Thus far, most of our security issues have come with patches attached, however this will not always be true. Depending on the issue and severity, the QA coordinator and/or release manager(s) may apply for a Common Vulnerability and Exposures (CVE) number as well. Where do we review security patches? Minor patches can be reviewed on an adhoc basis but larger patches, especially those requring collaboration in private, or extensive comment and review, can use the opencast-security repository under the opencast-community account. Note that this repository is private to committers, and reporters. Add the security repository to your remotes with this command: git remote add security git@bitbucket.org:opencast-community/opencast-security.git Then create your branch locally. When the branch is ready to be pushed to the security repo do something like this: git push security <your branch name> You can then create your branch like you normally would, pushing it to security rather than your own repository. Once a security issue has been resolved Once a security issue has been resolved, the QA coordinator, and/or the release manager(s) affected will work together to ensure that any relevant release(s) are created and available, and then release the security notice. This notice must contain the CVE (if applicable) as well as the affected version(s) of Opencast, a description of the issue, mitigation/upgrade instructions, as well as credit to the reporter(s) of the issue. As an example, this is a good notice: Hello, this is the official security notice regarding a security issue recently discovered in Opencast. Description: The Solr index for the search service (back-end e.g. for player and media module) in some cases returns results that should not be available to the current user. Affects: This issue affects all recent versions of Opencast. Details: Solr in some cases returned results that should not be available to the current user. For example, if `UserX` has the role `ROLE_USER` and a video should only be available for `ROLE_USER_ADMIN`, `UserX` can still access it. This may happen only if the second role starts with the complete first role. If the rules do not overlap, there should be no problem. Patching the system: Patches for this issue are included in Opencast 2.2.4 and 2.3.0. A patch can also be found at https://bitbucket.org/opencast-community/opencast/pull-requests/1236 Credits: Thanks to Matthias Neugebauer from the University of M\u00fcnster for finding, reporting and fixing the issue. Best regards, Lars Kiesow","title":"Security Issues"},{"location":"security/#opencast-security-issue-process","text":"This document summarizes how issues are reported, who is responsible for what actions, and how things are handled internally. The instructions here should be considered the minimum , and further/faster responses are certainly possible depending on the severity and type of issue.","title":"Opencast Security Issue Process"},{"location":"security/#for-users-and-administrators","text":"","title":"For Users and Administrators"},{"location":"security/#what-to-do-if-you-find-a-security-issue","text":"Report it to security@opencast.org ! Please include a complete description of the issue, including which version(s) it affects, as well as any steps required to reproduce it. This email will be sent privately to the full list of committers for the project. You should receive an email acknowledging the issue from the QA coordinator, or release manager(s) within 3 business days. At this point, depending on the issue, you may be asked for your Attlassian and GitHub logins. It is enouraged to have one, since internal discussion on JIRA will be restricted to committers and the reporter. Likewise, all code reviews will be done on in an internal GitHub repository.","title":"What to do if you find a security issue"},{"location":"security/#what-to-do-once-you-have-reported-your-security-issue","text":"Wait, and/or help fix the issue. The committers will work to find a mitigation for the immediate case, and a proper solution for the long term. This may take a while, please be patient. It may also require you to test it if the issue is derived from a complex system that most committers would not necessarily have access to (e.g: your LDAP or LTI servers).","title":"What to do once you have reported your security issue"},{"location":"security/#what-happens-once-the-issue-has-been-fixed","text":"A notice to security-notices@opencast.org will be released once the issue has been resolved and proper patches applied to the codebase. This notice may be accompanied by a release, or instructions on how to patch a live system depending on the issue.","title":"What happens once the issue has been fixed?"},{"location":"security/#for-committers","text":"","title":"For Committers"},{"location":"security/#what-to-do-with-a-security-report","text":"If no one else has, draft a new security advisory on GitHub and reply on security@opencast.org with a link to the draft. Thus far, most of our security issues have come with patches attached, however this will not always be true. Depending on the issue and severity, the QA coordinator and/or release manager(s) may apply for a Common Vulnerability and Exposures (CVE) number as well.","title":"What to do with a security report"},{"location":"security/#where-do-we-review-security-patches","text":"Minor patches can be reviewed on an adhoc basis but larger patches, especially those requring collaboration in private, or extensive comment and review, can use the opencast-security repository under the opencast-community account. Note that this repository is private to committers, and reporters. Add the security repository to your remotes with this command: git remote add security git@bitbucket.org:opencast-community/opencast-security.git Then create your branch locally. When the branch is ready to be pushed to the security repo do something like this: git push security <your branch name> You can then create your branch like you normally would, pushing it to security rather than your own repository.","title":"Where do we review security patches?"},{"location":"security/#once-a-security-issue-has-been-resolved","text":"Once a security issue has been resolved, the QA coordinator, and/or the release manager(s) affected will work together to ensure that any relevant release(s) are created and available, and then release the security notice. This notice must contain the CVE (if applicable) as well as the affected version(s) of Opencast, a description of the issue, mitigation/upgrade instructions, as well as credit to the reporter(s) of the issue. As an example, this is a good notice: Hello, this is the official security notice regarding a security issue recently discovered in Opencast. Description: The Solr index for the search service (back-end e.g. for player and media module) in some cases returns results that should not be available to the current user. Affects: This issue affects all recent versions of Opencast. Details: Solr in some cases returned results that should not be available to the current user. For example, if `UserX` has the role `ROLE_USER` and a video should only be available for `ROLE_USER_ADMIN`, `UserX` can still access it. This may happen only if the second role starts with the complete first role. If the rules do not overlap, there should be no problem. Patching the system: Patches for this issue are included in Opencast 2.2.4 and 2.3.0. A patch can also be found at https://bitbucket.org/opencast-community/opencast/pull-requests/1236 Credits: Thanks to Matthias Neugebauer from the University of M\u00fcnster for finding, reporting and fixing the issue. Best regards, Lars Kiesow","title":"Once a security issue has been resolved"},{"location":"statistics/","text":"Statistics Opencast provides an extensible mechanism to make statistics data available to the Opencast administrative user interface and also to third-party applications using the External API. The overall idea is that sources of statistics data ( StatisticsProvider ) are managed at a central service ( StatisticsService ). The StatisticsService supports a minimal set of common attributes, in particular, the type of the StatisticsProvider which implies a data format and available parametrization. A client can use the StatisticsService to retrieve a list of all available StatisticsProviders . The data format as well as the parameters supported by the StatisticsProviders are implied by the type of the provider. This information is used by the client to decide whether it can visualize the statistics data and which component has to be used for visualization. Modules The StatisticsService consists of the following modules: statistics-service-api An API module defining the core StatisticsService and StatisticsProvider functions. statistics-service-impl The default implementation of the StatisticsService as an OSGi service. statistics-service remote The remote implementation of the StatisticsService. statistics-provider-influx An implementation of the StatisticsProvider for InfluxDB statistics-provider-random An implementation of the StatisticsProvider for testing and demo purposes. Interfaces & Classes The Opencast StatisticsService implements the two interfaces StatisticsCoordinator and StatisticsService. StatisticsCoordinator This interface is used by StatisticsProvider implementations to register and unregister themselves at the statistics service. Method Description addProvider Register a statistics provider at the statistics service removeProvider Unregister a statistics provider from the statistics service StatisticsService This is the interface used by clients of the statistics service to retrieve a list of registered statistics providers Method Description getProviders Retrieve lists of statistics providers StatisticsProvider The StatisticsProvider interface provides access to common attributes of the statistics providers: Method Description getId Returns the unique identifier of the statistics provider getType Returns the type of the statistics provider getResourceType Returns the ResourceType of the statistics provider getName Returns the displayable name of the statistics provider getDescription Returns the displayable description of the statistics provider whereas supported resource types are ResourceType Description EPISODE The statistics data relates to an episode SERIES The statistics data relates to a series ORGANIZATION The statistics data does not relate to a particular object Integration The StatisticsService API is supposed to be an internal Opencast interface. External clients can use the External API to access Opencast statistics and the Opencast Admin UI has access through the Admin UI facade. External API The External API supports Opencast statistics by its Statistics API endpoint . Admin UI The Admin UI supports Opencast statistics at various levels: File Description modules/admin-ui/src/main/java/org/opencastproject/adminui/endpoint/StatisticsEndpoint.java Implementation of the Statistics endpoint for the Admin UI facade modules/admin-ui/src/main/webapp/scripts/shared/resources/statisticsResource.js Abstracts StatisticsEndpoint for use in the web application","title":"Statistics"},{"location":"statistics/#statistics","text":"Opencast provides an extensible mechanism to make statistics data available to the Opencast administrative user interface and also to third-party applications using the External API. The overall idea is that sources of statistics data ( StatisticsProvider ) are managed at a central service ( StatisticsService ). The StatisticsService supports a minimal set of common attributes, in particular, the type of the StatisticsProvider which implies a data format and available parametrization. A client can use the StatisticsService to retrieve a list of all available StatisticsProviders . The data format as well as the parameters supported by the StatisticsProviders are implied by the type of the provider. This information is used by the client to decide whether it can visualize the statistics data and which component has to be used for visualization.","title":"Statistics"},{"location":"statistics/#modules","text":"The StatisticsService consists of the following modules: statistics-service-api An API module defining the core StatisticsService and StatisticsProvider functions. statistics-service-impl The default implementation of the StatisticsService as an OSGi service. statistics-service remote The remote implementation of the StatisticsService. statistics-provider-influx An implementation of the StatisticsProvider for InfluxDB statistics-provider-random An implementation of the StatisticsProvider for testing and demo purposes.","title":"Modules"},{"location":"statistics/#interfaces-classes","text":"The Opencast StatisticsService implements the two interfaces StatisticsCoordinator and StatisticsService. StatisticsCoordinator This interface is used by StatisticsProvider implementations to register and unregister themselves at the statistics service. Method Description addProvider Register a statistics provider at the statistics service removeProvider Unregister a statistics provider from the statistics service StatisticsService This is the interface used by clients of the statistics service to retrieve a list of registered statistics providers Method Description getProviders Retrieve lists of statistics providers StatisticsProvider The StatisticsProvider interface provides access to common attributes of the statistics providers: Method Description getId Returns the unique identifier of the statistics provider getType Returns the type of the statistics provider getResourceType Returns the ResourceType of the statistics provider getName Returns the displayable name of the statistics provider getDescription Returns the displayable description of the statistics provider whereas supported resource types are ResourceType Description EPISODE The statistics data relates to an episode SERIES The statistics data relates to a series ORGANIZATION The statistics data does not relate to a particular object","title":"Interfaces &amp; Classes"},{"location":"statistics/#integration","text":"The StatisticsService API is supposed to be an internal Opencast interface. External clients can use the External API to access Opencast statistics and the Opencast Admin UI has access through the Admin UI facade.","title":"Integration"},{"location":"statistics/#external-api","text":"The External API supports Opencast statistics by its Statistics API endpoint .","title":"External API"},{"location":"statistics/#admin-ui","text":"The Admin UI supports Opencast statistics at various levels: File Description modules/admin-ui/src/main/java/org/opencastproject/adminui/endpoint/StatisticsEndpoint.java Implementation of the Statistics endpoint for the Admin UI facade modules/admin-ui/src/main/webapp/scripts/shared/resources/statisticsResource.js Abstracts StatisticsEndpoint for use in the web application","title":"Admin UI"},{"location":"api/","text":"External API Introduction In order to allow for robust technical integration of applications like learning management systems or mobile applications, Opencast offers the External API to allow those applications to provide access to and management of resources exposed through the API. The External API has been designed and implemented to support large numbers of clients, each with considerable amounts of requests per time interval. In addition, security has been a focus to ensure protection of the managed data and to support use cases promoting differing views on the managed data. Architectural Overview The External API has been implemented as an abstraction layer to multiple internal APIs that the underlying application (Opencast) offers for the manipulation of resources like series, events or users (see Figure 1: Architectural overview ). Authentication and Authorization The External API features a dedicated security layer that is in charge of providing support for a variety of authentication and authorization mechanisms. Additionally, the security layer provides means for delegation of authorization to the client application in cases where the API client needs to manage its own set of assets with implicit access control. These concepts are documented in greater detail in the following Authentication and Authorization chapters. Requests for data The abstraction layer is backed by a dedicated index, which is kept up-to-date using Opencast\u2019s message broker. When a request to an API method is received (1), the data is compiled using the index and returned to the client (2). Since the index is scalable and optimized for performance, a large number of requests can be processed per time interval. The corresponding requests along with the potential responses are defined later on in the API chapter. Processing of updates Whenever a client sends updated information to the External API, it will forward that information to the corresponding Opencast services (3), which in turn will process the data and send messages to the message bus accordingly (4). The messages are consumed by the External API\u2019s data store and can be served to its clients from then on. The corresponding requests along with the data structures and potential responses are defined later on in the API chapter. Figure 1: Architectural overview Requests are authenticated and authorized (1), and corresponding responses are sent back to the client (2). Updates are passed on to the backing application services and the modified data is then received through the application\u2019s message infrastructure (4), (5). Access The External API has been implemented using the Restful State Transfer paradigm to expose resources of the underlying system in the URL space that are then accessible using the HTTP protocol and verbs GET , POST , PUT and DELETE . Since as part of the communication, the External API is used to transfer potentially sensitive data between the client and the server including the username and password as part of the Basic Authentication protocol, the API will usually only be available over a secure HTTPS connection only. Url Space The External API is located at the /api namespace on the Opencast admin node. This results in all requests to the External API starting with https://<hostname>/api , where the hostname is depending on the installation and tenant (see \u201cMulti Tenancy\u201d). Versioning The External API is versioned so that applications developed against one version of the API won\u2019t break with enhancements or replacements of existing versions as long as they stay on the same major version. The set of currently supported versions as well as the current version are exposed through REST methods as part of the meta API. Version scheme The External API is following the semantic versioning standard , which is suggesting the use of versions of the form x.z.y where x is the major version, y is the minor version and z is the patch level. Part Comment Major Changes are potentially backward incompatible and require changing client code. Minor Functionality is added in a backwards-compatible manner. Patch Bugfixes applied in a backwards-compatible manner. Backwards Compatibility As a consequence, the External API is expected to be backwards compatible between minor version upgrades, including the patch level. This means that a client that has been developed against version 1.0.0 of the api will work with version 1.1.3 as well. This however may not be true going from version 1.1.0 to 2.0.0 Multi tenancy With Opencast being a multi tenant application, the External API reflects that characteristics as well. Requests are mapped to individual tenants by matching the requests\u2019s target hostname against the list of tenant hostnames.","title":"Introduction"},{"location":"api/#external-api","text":"","title":"External API"},{"location":"api/#introduction","text":"In order to allow for robust technical integration of applications like learning management systems or mobile applications, Opencast offers the External API to allow those applications to provide access to and management of resources exposed through the API. The External API has been designed and implemented to support large numbers of clients, each with considerable amounts of requests per time interval. In addition, security has been a focus to ensure protection of the managed data and to support use cases promoting differing views on the managed data.","title":"Introduction"},{"location":"api/#architectural-overview","text":"The External API has been implemented as an abstraction layer to multiple internal APIs that the underlying application (Opencast) offers for the manipulation of resources like series, events or users (see Figure 1: Architectural overview ).","title":"Architectural Overview"},{"location":"api/#authentication-and-authorization","text":"The External API features a dedicated security layer that is in charge of providing support for a variety of authentication and authorization mechanisms. Additionally, the security layer provides means for delegation of authorization to the client application in cases where the API client needs to manage its own set of assets with implicit access control. These concepts are documented in greater detail in the following Authentication and Authorization chapters.","title":"Authentication and Authorization"},{"location":"api/#requests-for-data","text":"The abstraction layer is backed by a dedicated index, which is kept up-to-date using Opencast\u2019s message broker. When a request to an API method is received (1), the data is compiled using the index and returned to the client (2). Since the index is scalable and optimized for performance, a large number of requests can be processed per time interval. The corresponding requests along with the potential responses are defined later on in the API chapter.","title":"Requests for data"},{"location":"api/#processing-of-updates","text":"Whenever a client sends updated information to the External API, it will forward that information to the corresponding Opencast services (3), which in turn will process the data and send messages to the message bus accordingly (4). The messages are consumed by the External API\u2019s data store and can be served to its clients from then on. The corresponding requests along with the data structures and potential responses are defined later on in the API chapter. Figure 1: Architectural overview Requests are authenticated and authorized (1), and corresponding responses are sent back to the client (2). Updates are passed on to the backing application services and the modified data is then received through the application\u2019s message infrastructure (4), (5).","title":"Processing of updates"},{"location":"api/#access","text":"The External API has been implemented using the Restful State Transfer paradigm to expose resources of the underlying system in the URL space that are then accessible using the HTTP protocol and verbs GET , POST , PUT and DELETE . Since as part of the communication, the External API is used to transfer potentially sensitive data between the client and the server including the username and password as part of the Basic Authentication protocol, the API will usually only be available over a secure HTTPS connection only.","title":"Access"},{"location":"api/#url-space","text":"The External API is located at the /api namespace on the Opencast admin node. This results in all requests to the External API starting with https://<hostname>/api , where the hostname is depending on the installation and tenant (see \u201cMulti Tenancy\u201d).","title":"Url Space"},{"location":"api/#versioning","text":"The External API is versioned so that applications developed against one version of the API won\u2019t break with enhancements or replacements of existing versions as long as they stay on the same major version. The set of currently supported versions as well as the current version are exposed through REST methods as part of the meta API.","title":"Versioning"},{"location":"api/#version-scheme","text":"The External API is following the semantic versioning standard , which is suggesting the use of versions of the form x.z.y where x is the major version, y is the minor version and z is the patch level. Part Comment Major Changes are potentially backward incompatible and require changing client code. Minor Functionality is added in a backwards-compatible manner. Patch Bugfixes applied in a backwards-compatible manner.","title":"Version scheme"},{"location":"api/#backwards-compatibility","text":"As a consequence, the External API is expected to be backwards compatible between minor version upgrades, including the patch level. This means that a client that has been developed against version 1.0.0 of the api will work with version 1.1.3 as well. This however may not be true going from version 1.1.0 to 2.0.0","title":"Backwards Compatibility"},{"location":"api/#multi-tenancy","text":"With Opencast being a multi tenant application, the External API reflects that characteristics as well. Requests are mapped to individual tenants by matching the requests\u2019s target hostname against the list of tenant hostnames.","title":"Multi tenancy"},{"location":"api/agents-api/","text":"General GET /api/agents GET /api/agents/{agent_id} General The Agents API is available since API version 1.1.0. GET /api/agents Returns a list of capture agents. Query String Parameter Type Description limit integer The maximum number of results to return for a single request (see Pagination ) offset integer The index of the first result to return (see Pagination ) Sample request https://opencast.example.org/api/agents?limit=5&offset=1 Response 200 (OK) : A (potentially empty) list of capture agents is returned as JSON array of JSON objects Field Type Description agent_id string The technical identifier of the capture agent status string The status of the capture agent inputs array[string] The inputs of the capture agent update datetime The last date and time this capture agent contactec the server url string The URL as reported by the capture agent Example [ { \"agent_id\": \"ca24\", \"status\": \"offline\", \"inputs\": [\"default\"], \"update\": \"2018-03-12T18:17:25Z\", \"url\": \"127.0.0.1\" } ] GET /api/agents/{agent_id} Returns a single capture agent. Response 200 (OK) : The capture agent is returned as JSON object Field Type Description agent_id string The technical identifier of the capture agent status string The status of the capture agent inputs array[string] The inputs of the capture agent update datetime The last date and time this capture agent contactec the server url string The URL as reported by the capture agent 404 (NOT FOUND) : The specified capture agent does not exist. Example { \"agent_id\": \"ca24\", \"status\": \"offline\", \"inputs\": [\"default\"], \"update\": \"2018-03-12T18:17:25Z\", \"url\": \"127.0.0.1\" }","title":"Agents API"},{"location":"api/agents-api/#general","text":"The Agents API is available since API version 1.1.0.","title":"General"},{"location":"api/agents-api/#get-apiagents","text":"Returns a list of capture agents. Query String Parameter Type Description limit integer The maximum number of results to return for a single request (see Pagination ) offset integer The index of the first result to return (see Pagination ) Sample request https://opencast.example.org/api/agents?limit=5&offset=1 Response 200 (OK) : A (potentially empty) list of capture agents is returned as JSON array of JSON objects Field Type Description agent_id string The technical identifier of the capture agent status string The status of the capture agent inputs array[string] The inputs of the capture agent update datetime The last date and time this capture agent contactec the server url string The URL as reported by the capture agent Example [ { \"agent_id\": \"ca24\", \"status\": \"offline\", \"inputs\": [\"default\"], \"update\": \"2018-03-12T18:17:25Z\", \"url\": \"127.0.0.1\" } ]","title":"GET /api/agents"},{"location":"api/agents-api/#get-apiagentsagent_id","text":"Returns a single capture agent. Response 200 (OK) : The capture agent is returned as JSON object Field Type Description agent_id string The technical identifier of the capture agent status string The status of the capture agent inputs array[string] The inputs of the capture agent update datetime The last date and time this capture agent contactec the server url string The URL as reported by the capture agent 404 (NOT FOUND) : The specified capture agent does not exist. Example { \"agent_id\": \"ca24\", \"status\": \"offline\", \"inputs\": [\"default\"], \"update\": \"2018-03-12T18:17:25Z\", \"url\": \"127.0.0.1\" }","title":"GET /api/agents/{agent_id}"},{"location":"api/authentication/","text":"Introduction The External API\u2019s security layer is designed to support a multitude of mechanisms for authentication such as API keys, digest authentication and others. While the current implementation only supports basic authentication, further authentication mechanisms may be added in the future. Basic Authentication The External API is protected by basic authentication, requiring a user and a password be sent in the form of the standard HTTP Authorization header. (see Figure 2 ). In the header, the username and password are sent encoded in Base64 format. The incoming requests are matched against an existing user whose password needs to match with the one that is found in the Authorization request header. NOTE: Basic authentication is not activated by default, please activate it in the security settings ( etc/security/mh_default_org.xml ) before using the External API. Figure 2: Authentication and authorization based on Basic Authentication Protection of authentication data Since Base64 is by no means regarded as encryption in the sense of security, it is strongly recommended to only offer access to the External API over HTTPS rather than over HTTP in order to avoid unprotected transmission of the username and password via the Authorization header. Validation criteria When initiating the connection, the External API analyzes the header and extracts the username to match against an existing API user. If that user is found, the connection is authenticated, otherwise it\u2019s rejected.","title":"Authentication"},{"location":"api/authentication/#introduction","text":"The External API\u2019s security layer is designed to support a multitude of mechanisms for authentication such as API keys, digest authentication and others. While the current implementation only supports basic authentication, further authentication mechanisms may be added in the future.","title":"Introduction"},{"location":"api/authentication/#basic-authentication","text":"The External API is protected by basic authentication, requiring a user and a password be sent in the form of the standard HTTP Authorization header. (see Figure 2 ). In the header, the username and password are sent encoded in Base64 format. The incoming requests are matched against an existing user whose password needs to match with the one that is found in the Authorization request header. NOTE: Basic authentication is not activated by default, please activate it in the security settings ( etc/security/mh_default_org.xml ) before using the External API. Figure 2: Authentication and authorization based on Basic Authentication","title":"Basic Authentication"},{"location":"api/authentication/#protection-of-authentication-data","text":"Since Base64 is by no means regarded as encryption in the sense of security, it is strongly recommended to only offer access to the External API over HTTPS rather than over HTTP in order to avoid unprotected transmission of the username and password via the Authorization header.","title":"Protection of authentication data"},{"location":"api/authentication/#validation-criteria","text":"When initiating the connection, the External API analyzes the header and extracts the username to match against an existing API user. If that user is found, the connection is authenticated, otherwise it\u2019s rejected.","title":"Validation criteria"},{"location":"api/authorization/","text":"External API Introduction The External API can be accessed in two different ways: Either using a single dedicated user with access to everything (\u201csuper user\u201d) or by implementing more fine grained access through user and role switching upon every request (\u201cuser switching\u201d or \u201csudo\u201d execution mode), where the request is executed in the name and using the roles of the specified user. The first method is ideal for scenarios where the end users of the external application are not managed in Opencast. The downside of this approach is a potential security risk as well as the inability to audit and track changes made by the external applications back to the actual user who actually triggered the changes. The second method is more cumbersome to implement but leads a much improved control and assessment of security. Delegation of Authorization In situations where the provider of the External API offers a super user who is allowed \u201csudo\u201d requests that are executed on behalf of another user, the External API is actually delegating authorization to the client application. In this cause authorization is performed upon login of the super user, but then the super user can switch to any other user or any set of roles (with a few exceptions for security reasons). Note that in order to allow for user switching, a specific role needs to be assigned to the super user, and that role cannot be obtained by manipulating the role set (see Role switching ). User switching When working with a super user, it is considered a best practice to specify a dedicated execution user upon each request whenever possible and reasonable. This way, creation or modification of resources can later be audited and mapped back to that user if needed. The execution user can be specified by setting the X-RUN-AS-USER request header with the user name as its value, as seen in this sample request: Request Headers GET /api X-RUN-AS-USER: john.doe Switching to another user can potentially fail for various reasons: The user might not exist or may not be allowed to switch to due to potential privilege escalation, or the current user might not be allowed to switch users at all. If the request to switch to another user fails, the following response codes are returned: Response code Comment 403 (FORBIDDEN) The current user is not allowed to switch users 403 (FORBIDDEN) The user cannot be switched to due to potential escalation of privileges 412 Precondition failed The user specified in the X-RUN-AS-USER header does not exist Role switching Rather than specifying an execution user, the client might choose to specify a set of roles that should be used when executing the request. This technique is recommended in cases where the users are not managed by the External API. By specifying a set of roles, the corresponding request will be executed using the API\u2019s anonymous user but equipped with the specified set of roles. The execution user\u2019s roles can be specified by setting the X-RUN-WITH-ROLES request header with the set of roles as its value and with individual roles separated by comma, as seen in this sample request: Request Headers GET /api X-RUN-WITH-ROLES: ROLE_A,ROLE_B Switching to a set of roles can potentially fail for various reasons: The role may not be granted to due to potential privilege escalation, or the current user might not be allowed to switch roles at all. If the request to apply a set of roles fails, the following response codes are returned: Response code Comment 403 (FORBIDDEN) The current user is not allowed to switch roles 403 (FORBIDDEN) The roles cannot be granted to due to potential escalation of privileges Best practice One user per external application As a best practice, the External API provider should create one super user per external application and tenant, so that access through that super user can be controlled, limited and turned off individually for each external application and tenant. Preference for user and role switching Client implementations accessing the External API through a super user are urged to implement and enforce user and role switching as much as possible, since it allows for auditing of user activity on the External API and introduces less risk by running requests with a limited set of privileges. Obviously, if all requests are executed using the super user directly, it is not possible to track which user initiated a given action. Access Control Most events in Opencast come with an access control list (ACL), containing entries that map actions to roles, either allowing or denying that action. Opencast currently only supports the ability to explicitly allow an action and consider everything else to be denied. Roles When a user authenticates against Opencast, it is assigned its set of roles that determine the user's access to Opencast data entities. There are multiple ways to associate roles with a user: Explicit assignment directly to the user Directly through membership in groups (ROLE_GROUP_<group name>) Indirectly through membership in groups (whatever roles have been assigned to the group) In addition, a special role is assigned that uniquely identifies a user (\"user role\"). The user role can be determined by evaluating the userrole attribute in the Base API's call to /info/me .","title":"Authorization"},{"location":"api/authorization/#external-api","text":"","title":"External API"},{"location":"api/authorization/#introduction","text":"The External API can be accessed in two different ways: Either using a single dedicated user with access to everything (\u201csuper user\u201d) or by implementing more fine grained access through user and role switching upon every request (\u201cuser switching\u201d or \u201csudo\u201d execution mode), where the request is executed in the name and using the roles of the specified user. The first method is ideal for scenarios where the end users of the external application are not managed in Opencast. The downside of this approach is a potential security risk as well as the inability to audit and track changes made by the external applications back to the actual user who actually triggered the changes. The second method is more cumbersome to implement but leads a much improved control and assessment of security.","title":"Introduction"},{"location":"api/authorization/#delegation-of-authorization","text":"In situations where the provider of the External API offers a super user who is allowed \u201csudo\u201d requests that are executed on behalf of another user, the External API is actually delegating authorization to the client application. In this cause authorization is performed upon login of the super user, but then the super user can switch to any other user or any set of roles (with a few exceptions for security reasons). Note that in order to allow for user switching, a specific role needs to be assigned to the super user, and that role cannot be obtained by manipulating the role set (see Role switching ).","title":"Delegation of Authorization"},{"location":"api/authorization/#user-switching","text":"When working with a super user, it is considered a best practice to specify a dedicated execution user upon each request whenever possible and reasonable. This way, creation or modification of resources can later be audited and mapped back to that user if needed. The execution user can be specified by setting the X-RUN-AS-USER request header with the user name as its value, as seen in this sample request: Request Headers GET /api X-RUN-AS-USER: john.doe Switching to another user can potentially fail for various reasons: The user might not exist or may not be allowed to switch to due to potential privilege escalation, or the current user might not be allowed to switch users at all. If the request to switch to another user fails, the following response codes are returned: Response code Comment 403 (FORBIDDEN) The current user is not allowed to switch users 403 (FORBIDDEN) The user cannot be switched to due to potential escalation of privileges 412 Precondition failed The user specified in the X-RUN-AS-USER header does not exist","title":"User switching"},{"location":"api/authorization/#role-switching","text":"Rather than specifying an execution user, the client might choose to specify a set of roles that should be used when executing the request. This technique is recommended in cases where the users are not managed by the External API. By specifying a set of roles, the corresponding request will be executed using the API\u2019s anonymous user but equipped with the specified set of roles. The execution user\u2019s roles can be specified by setting the X-RUN-WITH-ROLES request header with the set of roles as its value and with individual roles separated by comma, as seen in this sample request: Request Headers GET /api X-RUN-WITH-ROLES: ROLE_A,ROLE_B Switching to a set of roles can potentially fail for various reasons: The role may not be granted to due to potential privilege escalation, or the current user might not be allowed to switch roles at all. If the request to apply a set of roles fails, the following response codes are returned: Response code Comment 403 (FORBIDDEN) The current user is not allowed to switch roles 403 (FORBIDDEN) The roles cannot be granted to due to potential escalation of privileges","title":"Role switching"},{"location":"api/authorization/#best-practice","text":"","title":"Best practice"},{"location":"api/authorization/#one-user-per-external-application","text":"As a best practice, the External API provider should create one super user per external application and tenant, so that access through that super user can be controlled, limited and turned off individually for each external application and tenant.","title":"One user per external application"},{"location":"api/authorization/#preference-for-user-and-role-switching","text":"Client implementations accessing the External API through a super user are urged to implement and enforce user and role switching as much as possible, since it allows for auditing of user activity on the External API and introduces less risk by running requests with a limited set of privileges. Obviously, if all requests are executed using the super user directly, it is not possible to track which user initiated a given action.","title":"Preference for user and role switching"},{"location":"api/authorization/#access-control","text":"Most events in Opencast come with an access control list (ACL), containing entries that map actions to roles, either allowing or denying that action. Opencast currently only supports the ability to explicitly allow an action and consider everything else to be denied.","title":"Access Control"},{"location":"api/authorization/#roles","text":"When a user authenticates against Opencast, it is assigned its set of roles that determine the user's access to Opencast data entities. There are multiple ways to associate roles with a user: Explicit assignment directly to the user Directly through membership in groups (ROLE_GROUP_<group name>) Indirectly through membership in groups (whatever roles have been assigned to the group) In addition, a special role is assigned that uniquely identifies a user (\"user role\"). The user role can be determined by evaluating the userrole attribute in the Base API's call to /info/me .","title":"Roles"},{"location":"api/base-api/","text":"Information GET /api User and Organization GET /api/info/me GET /api/info/me/roles GET /api/info/organization GET /api/info/organization/properties Versions GET /api/version GET /api/version/default Information In order to assess key characteristics of the External API and to test general connectivity, the External API\u2019s root url is not protected through authentication: GET /api Returns key characteristics of the External API such as the API base URL and the default version. Response 200 (OK) : The api information is returned as a JSON object containing the following fields: Field Type Description version string Default version of the External API url string Base URL clients shall use to communicate with the External API Example { \"url\": \"https:\\/\\/api.opencast.org\\/api\", \"version\": \"v1.0.1\" } User and Organization GET /api/info/me Returns information on the logged in user. Response 200 (OK) : The user information is returned as a JSON object containing the following fields: Field Type Description provider string The Opencast user provider that manages this user name string Displayable name of the user username string The username userrole string The role uniquly identifying the user email string The e-mail address of the user Example { \"provider\": \"opencast\", \"name\": \"Opencast Student\", \"userrole\": \"ROLE_USER_92623987_OPENCAST_ORG\", \"email\": \"nowhere@opencast.org\", \"username\": \"92623987@opencast.org\" } GET /api/info/me/roles Returns current user's roles. Response 200 (OK) : The set of roles is returned as array[string] . Example [ \"ROLE_USER_92623987@opencast.org\", \"ROLE_STUDENT\" ] GET /api/info/organization Returns the current organization. Response 200 (OK) : The organization details are returned as JSON object containing the following fields: Field Type Description adminRole string The role administrator users have anonymousRole string The role unauthenticated users have id string The tenant identifier name string The tenant name Example { \"adminRole\": \"ROLE_ADMIN\", \"anonymousRole\": \"ROLE_ANONYMOUS\", \"id\": \"mh_default_org\", \"name\": \"Opencast\" } GET /api/info/organization/properties Returns the current organization's properties. The set of properties is a key-value set that depends on the configuration of Opencast. Response 200 (OK) : The organization properties are returned as property . Example { \"org.opencastproject.feed.url\": \"http://feeds.opencast.org\", \"org.opencastproject.admin.documentation.url\": \"http://documentation.opencast.org\", \"org.opencastproject.external.api.url\": \"http://api.opencast.org\" } Versions GET /api/version Returns a list of available version as well as the default version. Response 200 (OK) : The version information is returned as JSON object containing the following fields: Field Type Description versions array[string] All External API versions supported by this server default string The default External API version used by this server Example { \"versions\": [ \"v1.0.0\", \"v1.1.0\" ], \"default\": \"v1.1.0\" } GET /api/version/default Returns the default version. Response 200 (OK) : The default version is returned as JSON object containing the following fields: Field Type Description default string The default External API version used by this server Example { \"default\": \"v1.1.0\" }","title":"Base API"},{"location":"api/base-api/#information","text":"In order to assess key characteristics of the External API and to test general connectivity, the External API\u2019s root url is not protected through authentication:","title":"Information"},{"location":"api/base-api/#get-api","text":"Returns key characteristics of the External API such as the API base URL and the default version. Response 200 (OK) : The api information is returned as a JSON object containing the following fields: Field Type Description version string Default version of the External API url string Base URL clients shall use to communicate with the External API Example { \"url\": \"https:\\/\\/api.opencast.org\\/api\", \"version\": \"v1.0.1\" }","title":"GET /api"},{"location":"api/base-api/#user-and-organization","text":"","title":"User and Organization"},{"location":"api/base-api/#get-apiinfome","text":"Returns information on the logged in user. Response 200 (OK) : The user information is returned as a JSON object containing the following fields: Field Type Description provider string The Opencast user provider that manages this user name string Displayable name of the user username string The username userrole string The role uniquly identifying the user email string The e-mail address of the user Example { \"provider\": \"opencast\", \"name\": \"Opencast Student\", \"userrole\": \"ROLE_USER_92623987_OPENCAST_ORG\", \"email\": \"nowhere@opencast.org\", \"username\": \"92623987@opencast.org\" }","title":"GET /api/info/me"},{"location":"api/base-api/#get-apiinfomeroles","text":"Returns current user's roles. Response 200 (OK) : The set of roles is returned as array[string] . Example [ \"ROLE_USER_92623987@opencast.org\", \"ROLE_STUDENT\" ]","title":"GET /api/info/me/roles"},{"location":"api/base-api/#get-apiinfoorganization","text":"Returns the current organization. Response 200 (OK) : The organization details are returned as JSON object containing the following fields: Field Type Description adminRole string The role administrator users have anonymousRole string The role unauthenticated users have id string The tenant identifier name string The tenant name Example { \"adminRole\": \"ROLE_ADMIN\", \"anonymousRole\": \"ROLE_ANONYMOUS\", \"id\": \"mh_default_org\", \"name\": \"Opencast\" }","title":"GET /api/info/organization"},{"location":"api/base-api/#get-apiinfoorganizationproperties","text":"Returns the current organization's properties. The set of properties is a key-value set that depends on the configuration of Opencast. Response 200 (OK) : The organization properties are returned as property . Example { \"org.opencastproject.feed.url\": \"http://feeds.opencast.org\", \"org.opencastproject.admin.documentation.url\": \"http://documentation.opencast.org\", \"org.opencastproject.external.api.url\": \"http://api.opencast.org\" }","title":"GET /api/info/organization/properties"},{"location":"api/base-api/#versions","text":"","title":"Versions"},{"location":"api/base-api/#get-apiversion","text":"Returns a list of available version as well as the default version. Response 200 (OK) : The version information is returned as JSON object containing the following fields: Field Type Description versions array[string] All External API versions supported by this server default string The default External API version used by this server Example { \"versions\": [ \"v1.0.0\", \"v1.1.0\" ], \"default\": \"v1.1.0\" }","title":"GET /api/version"},{"location":"api/base-api/#get-apiversiondefault","text":"Returns the default version. Response 200 (OK) : The default version is returned as JSON object containing the following fields: Field Type Description default string The default External API version used by this server Example { \"default\": \"v1.1.0\" }","title":"GET /api/version/default"},{"location":"api/events-api/","text":"General GET /api/events POST /api/events GET /api/events/{event_id} POST /api/events/{event_id} DELETE /api/events/{event_id} Access Policy GET /api/events/{event_id}/acl PUT /api/events/{event_id}/acl POST /api/events/{event_id}/acl/{action} DELETE /api/events/{event_id}/acl/{action}/{role} Metadata GET /api/events/{event_id}/metadata GET /api/events/{event_id}/metadata PUT /api/events/{event_id}/metadata DELETE /api/events/{event_id}/metadata Publications GET /api/events/{event_id}/publications GET /api/events/{event_id}/publications/{publication_id} Scheduling Information GET /api/events/{event_id}/scheduling PUT /api/events/{event_id}/scheduling General GET /api/events Returns a list of events. The following query string parameters are supported to filter, sort and paginate the returned list: Query String Parameter Type Description filter string A comma-separated list of filters to limit the results with (see Filtering ). See the below table for the list of available filters. Version 1.5.0 and newer support comma separated use of the filter keyword, creating a logical OR. sort string A comma-separated list of sort criteria (see Sorting ). See the below table for the list of available sort criteria limit integer The maximum number of results to return (see Pagination ) offset integer The index of the first result to return (see Pagination ) The following filters are available: Filter Name Description contributors Events where the contributors match. Can occur multiple times location Events based upon the location it is scheduled in series Events based upon which series they are a part of. Use the series identifier here. If using version 1.1.0 or higher, please use is_part_of instead subject Filters events based upon which subject they are a part of textFilter Filters events where any part of the event's metadata fields match this value identifier Filters events whose identifiers match this value. Can occur multiple times (version 1.1.0 and higher) title Filters events whose title match this value (version 1.1.0 and higher) description Filters events whose description match this value (version 1.1.0 and higher) series_name Filters events that belong to series with the given name (version 1.1.0 or higher) language Filters events whose language match this value (version 1.1.0 or higher) created Filters events whose created match this value (version 1.1.0 or higher) license Filters events whose license match this value (version 1.1.0 or higher) rightsholder Filters events whose rights holder matches this value (version 1.1.0 or higher) status Filters events based on their status (version 1.1.0 or higher) is_part_of Events based upon which series they are a part of. Use the series identifier here (version 1.1.0 or higher) source Filter events whose source match this value (version 1.1.0 or higher) agent_id Filter events based on the capture agent id (version 1.1.0 or higher) start Filter events based on start date (version 1.1.0 or higher) technical_start Filter events based on the technical start date (version 1.1.0 or higher) Note: The filters start and technical_start expect the following value: datetime + '/' + datetime The list can be sorted by the following criteria: Sort Criteria Description title By the title of the event presenter By the presenter of the event start_date By the start date of the event end_date By the end date of the event review_status By whether the event has been reviewed and approved or not [DEPRECATED] workflow_state By the current processing state of the event. Is it scheduled to be recorded (INSTANTIATED), currently processing (RUNNING), paused waiting for a resource or user paused (PAUSED), cancelled (STOPPED), currently failing (FAILING), already failed (FAILED), or finally SUCCEEDED scheduling_status By the current scheduling status of the event [DEPRECATED] series_name By the series name of the event location By the location (capture agent) that the event will be or has been recorded on This request additionally supports the following query string parameters to include additional information directly in the response: Query String Parameter Type Description sign boolean Whether public distribution urls should be signed withacl boolean Whether the acl metadata should be included in the response withmetadata boolean Whether the metadata catalogs should be included in the response withpublications boolean Whether the publication ids and urls should be included in the response withscheduling boolean Whether the scheduling information should be included in the response (version 1.1.0 and higher). By setting the optional sign parameter to true , the method will pre-sign distribution urls if URL signing is turned on in Opencast. Remember to consider the maximum validity of signed URLs when caching this response. Sample request https://opencast.example.org/api/events?sort=title:DESC&limit=5&offset=1&filter=location:ca-01 Response 200 (OK) : A (potentially empty) list of events is returned. The list is represented as JSON array where each element is a JSON object with the following fields: Field Type Description identifier string The unique identifier of the event creator string The technical creator of this event presenter * array[string] The presenters of this event created * datetime The date and time this event was created subjects * array[string] The subjects of this event start datetime The technical (version < 1.4.0) or bibliographic (version >= 1.4.0) start date and time of this event description * string The description of this event title * string The title of this event processing_state string The current processing state of this event duration string The bibliographic duration of this event archive_version string The current version of this event contributor * array[string] The contributors of this event has_previews boolean Whether this event can be opened with the video editor location * string The bibliographic location of this event publication_status array[string] The publications available for this event language * string The language of this event (version 1.1.0 and higher) rightsholder * string The rights holder of this event (version 1.1.0 and higher) license * string The license of this event (version 1.1.0 and higher) is_part_of * string The technical identifier of the series this event belongs to (version 1.1.0 and higher) series string The title of the series this event belongs to (version 1.1.0 and higher) source * string The source of this event (version 1.1.0 and higher) status string The technical status of this event (version 1.1.0 and higher) * Metadata fields of metadata catalog dublincore/episode Example [ { \"identifier\": \"776d4970-bc2d-45c4-900a-1f80b7990cb8\", \"creator\": \"Opencast Project Administrator\", \"presenter\": [ \"Prof. X\", \"Dr. Who\" ], \"created\": \"2018-03-19T16:11:48Z\", \"subjects\": [ \"Mathematics\", \"Basics\" ], \"start\": \"2018-03-19T16:11:48Z\", \"description\": \"This lecture is about the very basics\", \"title\": \"Lecture 1 - The Basics\", \"processing_state\": \"SUCCEEDED\", \"duration\": 0, \"archive_version\": 3, \"contributor\": [ \"Prof. X\" ], \"has_previews\": true, \"location\": \"\", \"publication_status\": [ \"internal\", \"engage-player\", \"api\", \"oaipmh-default\" ] }, { \"identifier\": \"2356d450-b42d-94xd-2a0d-3fa04745c0cb8\", \"creator\": \"Opencast Project Administrator\", \"presenter\": [ ], \"created\": \"2018-04-20T09:38:42Z\", \"subjects\": [ \"Physics\" ], \"start\": \"2018-04-23T08:00:00Z\", \"description\": \"The forces explained\", \"title\": \"Lecture 8 - Advanced stuff\", \"processing_state\": \"SUCCEEDED\", \"duration\": 0, \"archive_version\": 2, \"contributor\": [ ], \"has_previews\": true, \"location\": \"\", \"publication_status\": [ \"internal\", \"engage-player\", \"api\", \"oaipmh-default\" ] } ] POST /api/events Creates an event by sending metadata, access control list, processing instructions and files in a multipart request . Multipart Form Parameters Type Description acl acl A collection of roles with their possible action metadata catalogs Event metadata as Form param presenter file Presenter movie track presentation file Presentation movie track audio file Audio track processing string Processing instructions task configuration scheduling string Scheduling information (version 1.1.0 and higher) Scheduling Field Required Type Description agent_id Yes string The technical identifier of the capture agent start Yes datetime The date and time (UTC) when the recording shall start end Yes* datetime The date and time (UTC) when the recording shall end duration Yes* integer The duration of the recording in milliseconds rrule Yes* rrule The recurrence rule used to create multiple scheduled events * When creating a single scheduled event, either end or duration must be specified. When creating multiple scheduled events, the fields end , duration and rrule are required To create a single scheduled event, omit the field rrule . You can specify the start and end of the recording by using either the fields start and end or the fields start and duration . Please note that specifying both end and duration is not valid. To create multiple scheduled events, the field rrule is used. This field contains the recurrence rule used to determine the dates and times the scheduled events shall be created within the time period specified by the fields start and end + duration . Please note that duration is a mandatory field when creating multiple scheduled events. Additional Notes Both start and end must be specified in UTC Sample metadata: [ { \"flavor\": \"dublincore/episode\", \"fields\": [ { \"id\": \"title\", \"value\": \"Captivating title\" }, { \"id\": \"subjects\", \"value\": [\"John Clark\", \"Thiago Melo Costa\"] }, { \"id\": \"description\", \"value\": \"A great description\" }, { \"id\": \"startDate\", \"value\": \"2016-06-22\" }, { \"id\": \"startTime\", \"value\": \"13:30:00Z\" } ] } ] processing: { \"workflow\": \"schedule-and-upload\", \"configuration\": { \"flagForCutting\": \"false\", \"flagForReview\": \"false\", \"publishToEngage\": \"true\", \"publishToHarvesting\": \"true\", \"straightToPublishing\": \"true\" } } acl: [ { \"action\": \"write\", \"role\": \"ROLE_ADMIN\" }, { \"action\": \"read\", \"role\": \"ROLE_USER\" } ] scheduling a single event: { \"agent_id\": \"ca24\", \"start\": \"2018-03-27T16:00:00Z\", \"end\": \"2018-03-27T19:00:00Z\", \"inputs\": [\"default\"] } scheduling multiple events: { \"agent_id\": \"ca24\", \"start\": \"2019-05-20T16:00:00Z\", \"end\": \"2019-06-10T19:00:00Z\", \"inputs\": [\"default\"], \"rrule\":\"FREQ=WEEKLY;BYDAY=MO,TU,WE,TH,FR;BYHOUR=16;BYMINUTE=0\", \"duration\":1080000; } Response 200 (OK) : Multiple new events were created (scheduling with rrule ). 201 (CREATED) : A new event is created and its identifier is returned in the Location header. 400 (BAD REQUEST) : The request is invalid or inconsistent. 409 (CONFLICT) : The event could not be created due to a scheduling conflict. A list of conflicting events is returned. In case of success, when scheduling multiple events (200): [ { \"identifier\": \"e6aeb8df-a852-46cd-8128-b89de696f20e\" }, { \"identifier\": \"90e93bf6-ed95-483b-8e57-3e2de77a786f\" } ] In case of success (201): Location: http://api.opencast.org/api/events/e6aeb8df-a852-46cd-8128-b89de696f20e { \"identifier\": \"e6aeb8df-a852-46cd-8128-b89de696f20e\" } In case of a conflict (409): [ { \"start\":\"2018-03-21T14:00:00Z\", \"end\":\"2018-03-21T16:00:00Z\", \"title\":\"My Event 06\" } ] GET /api/events/{event_id} Returns a single event. By setting the optional sign parameter to true , the method will pre-sign distribution urls if signing is turned on in Opencast. Remember to consider the maximum validity of signed URLs when caching this response. Query String Parameter Type Description sign boolean Whether public distribution urls should be signed. withacl boolean Whether the acl metadata should be included in the response. withmetadata boolean Whether the metadata catalogs should be included in the response. withpublications boolean Whether the publication ids and urls should be included in the response. withscheduling boolean Whether the scheduling information should be included in the response (version 1.1.0 and higher). Response 200 (OK) : The event is returned as JSON object with the following fields: Field Type Description identifier string The unique identifier of the event creator string The technical creator of this event presenter * array[string] The presenters of this event created * datetime The date and time this event was created subjects * array[string] The subjects of this event start datetime The technical (version < 1.4.0) or bibliographic (version >= 1.4.0) start date and time of this event description * string The description of this event title * string The title of this event processing_state string The current processing state of this event duration string The bibliographic duration of this event archive_version string The current version of this event contributor * array[string] The contributors of this event has_previews boolean Whether this event can be opened with the video editor location * string The bibliographic location of this event publication_status array[string] The publications available for this event language * string The language of this event (version 1.1.0 and higher) rightsholder * string The rights holder of this event (version 1.1.0 and higher) license * string The license of this event (version 1.1.0 and higher) is_part_of * string The technical identifier of the series this event belongs to (version 1.1.0 and higher) series string The title of the series this event belongs to (version 1.1.0 and higher) source * string The source of this event (version 1.1.0 and higher) status string The technical status of this event (version 1.1.0 and higher) * Metadata fields of metadata catalog dublincore/episode 404 (NOT FOUND) : The specified event does not exist. { \"identifier\": \"776d4970-bc2d-45c4-900a-1f80b7990cb8\", \"creator\": \"Opencast Project Administrator\", \"presenter\": [ \"Prof. X\", \"Dr. Who\" ], \"created\": \"2018-03-19T16:11:48Z\", \"subjects\": [ \"Mathematics\", \"Basics\" ], \"start\": \"2018-03-19T16:11:48Z\", \"description\": \"This lecture is about the very basics\", \"title\": \"Lecture 1 - The Basics\", \"processing_state\": \"SUCCEEDED\", \"duration\": 0, \"archive_version\": 3, \"contributor\": [ \"Prof. X\" ], \"has_previews\": true, \"location\": \"\", \"publication_status\": [ \"internal\", \"engage-player\", \"api\", \"oaipmh-default\" ] } POST /api/events/{event_id} Updates an event. Multipart Form Parameters Type Description acl acl A collection of roles with their possible action metadata catalogs Event metadata as Form param presenter file Presenter movie track presentation file Presentation movie track audio file Audio track processing string Processing instructions task configuration scheduling string Scheduling information (version 1.1.0 and higher) Sample This sample request will update the Dublin Core metadata section of the event only. metadata: [ { \"flavor\": \"dublincore/episode\", \"fields\": [ { \"id\": \"title\", \"value\": \"Captivating title\" }, { \"id\": \"subjects\", \"value\": [\"Space\", \"Final Frontier\"] }, { \"id\": \"description\", \"value\": \"A great description\" } ] } ] Response 204 (NO CONTENT) : The event has been updated. 404 (NOT FOUND) : The specified event does not exist. 409 (CONFLICT) : The event could not be updated due to a scheduling conflict. A list of conflicting events is returned. In case of a conflict (when updating scheduling ): [ { \"start\":\"2018-03-21T14:00:00Z\", \"end\":\"2018-03-21T16:00:00Z\", \"title\":\"My Event 06\" } ] DELETE /api/events/{event_id} Deletes an event. Response 204 (NO CONTENT) : The event has been deleted. 404 (NOT FOUND) : The specified event does not exist. Access Policy Most events in Opencast come with an access control list (ACL), containing entries that map actions to roles, either allowing or denying that action. The section on roles in the chapter on Authorization will help shed some light on what kind of roles are available and how they are assigned to the current user. For more information about access control lists, please refer to Access Control Lists . GET /api/events/{event_id}/acl Returns an event's access policy. Response 200 (OK) : The access control list for the specified event is returned as acl . 404 (NOT FOUND) : The specified event does not exist. [ { \"allow\": true, \"action\": \"write\", \"role\": \"ROLE_ADMIN\" }, { \"allow\": true, \"action\": \"read\", \"role\": \"ROLE_USER\" } ] PUT /api/events/{event_id}/acl Update an event's access policy. Form Parameters Type Description acl acl Access policy Note that the existing access control list will be overwritten. Sample acl: [ { \"allow\": true, \"action\": \"write\", \"role\": \"ROLE_ADMIN\" }, { \"allow\": true, \"action\": \"read\", \"role\": \"ROLE_USER\" } ] Response 204 (NO CONTENT) : The access control list for the specified event is updated. 404 (NOT FOUND) : The specified event does not exist. POST /api/events/{event_id}/acl/{action} Grants permission to execute action on the specified event to any user with role role . Note that this is a convenience method to avoid having to build and post a complete access control list. Path Parameters Type Description event_id string Event identifier action string The action that is allowed to be executed Form Parameters Type Description role string The role that is granted permission Note that other access control lists entries will not be affected by this request. Sample role: \"ROLE_STUDENT\" Response 204 (NO CONTENT) : The permission has been created in the access control list of the specified event. 404 (NOT FOUND) : The specified event does not exist. DELETE /api/events/{event_id}/acl/{action}/{role} Revokes permission to execute action on the specified event from any user with role role . Path Parameters Type Description event_id string Event identifier action string The action that is no longer allowed to be executed role string The role that is no longer granted permission Note that other access control lists entries will not be affected by this request. Response 204 (NO CONTENT) : The permission has been revoked from the access control list of the specified event. 404 (NOT FOUND) : The specified event does not exist. Metadata This section describes how to use the External API to work with metadata catalogs associated to events. Opencast manages the bibliographic metadata of series using metadata catalogs which are identified by flavors. The default metadata catalog for Opencast events has the flavor dublincore/episode . Opencast additionally supports extended metadata catalogs for series that can be configured. The External API supports both the default event metadata catalog and events extended metadata catalogs. For the default event metadata catalog, the metadata is directly returned in the responses. Since the metadata catalogs can be configured, the External API provides a facility to retrieve the catalog configuration of series metadata catalogs. For more details about this mechanism, please refer to \"Metadata Catalogs\" . GET /api/events/{event_id}/metadata Returns the complete set of metadata. Response 200 (OK) : The metadata collection is returned as catalogs . 404 (OK) : The specified event does not exist. [ { \"label\": \"EVENTS.EVENTS.DETAILS.CATALOG.EPISODE\", \"flavor\": \"dublincore/episode\", \"fields\": [ { \"id\": \"title\", \"readOnly\": false, \"value\": \"Captivating title\", \"label\": \"EVENTS.EVENTS.DETAILS.METADATA.TITLE\", \"type\": \"text\", \"required\": true }, { \"id\": \"description\", \"readOnly\": false, \"value\": \"A great description\", \"label\": \"EVENTS.EVENTS.DETAILS.METADATA.DESCRIPTION\", \"type\": \"text_long\", \"required\": false } ] }, { \"label\": \"EVENTS.EVENTS.DETAILS.CATALOG.LICENSE\", \"flavor\": \"license/episode\", \"fields\": [ { \"id\": \"license\", \"readOnly\": false, \"value\": \"CCND\", \"label\": \"EVENTS.EVENTS.DETAILS.METADATA.LICENSE\", \"collection\": { \"BSD\": \"EVENTS.LICENSE.BSD\", \"GPL3\": \"EVENTS.LICENSE.GPL\", \"CCND\": \"EVENTS.LICENSE.CCND\" }, \"type\": \"text\", \"required\": false } ] } ] GET /api/events/{event_id}/metadata Returns the event's metadata of the specified type. For a metadata catalog there is the flavor such as dublincore/episode and this is the unique type. Query String Parameters Type Description type flavor The type of metadata to get Response 200 (OK) : The metadata collection is returned as fields . 404 (NOT FOUND) : The specified event does not exist. [ { \"id\": \"title\", \"readOnly\": false, \"value\": \"Captivating title\", \"label\": \"EVENTS.EVENTS.DETAILS.METADATA.TITLE\", \"type\": \"text\", \"required\": true }, { \"id\": \"description\", \"readOnly\": false, \"value\": \"A great description\", \"label\": \"EVENTS.EVENTS.DETAILS.METADATA.DESCRIPTION\", \"type\": \"text_long\", \"required\": false } ] PUT /api/events/{event_id}/metadata Update the metadata with the matching type of the specified event. For a metadata catalog there is the flavor such as dublincore/episode and this is the unique type. Query String Parameters Type Description type flavor The type of metadata to update Form Parameters Type Description metadata values Metadata fields and values to be updated Note that metadata fields not included in the form parameter metadata will not be updated. Sample metadata: [ { \"id\": \"title\", \"value\": \"Captivating title\" }, { \"id\": \"subjects\", \"value\": [\"Space\", \"Final Frontier\"] }, { \"id\": \"description\", \"value\": \"A great description\" } ] Response 204 (NO CONTENT) : The metadata of the given namespace has been updated. 400 (BAD REQUEST) : The request is invalid or inconsistent. 404 (NOT FOUND) : The specified event does not exist. DELETE /api/events/{event_id}/metadata Delete the metadata namespace catalog of the specified event. This will remove all fields and values of the catalog. Query String Parameters Type Description type flavor The type of metadata to delete Note that the metadata catalog of type dublincore/episode cannot be deleted. Response 204 (NO CONTENT) : The metadata of the given namespace has been updated. 403 (FORBIDDEN) : The main metadata catalog dublincore/episode cannot be deleted as it has mandatory fields. 404 (NOT FOUND) : The specified event does not exist. Publications GET /api/events/{event_id}/publications Returns an event's list of publications. Query String Parameter Type Description sign boolean Whether public distribution urls should be signed Response 200 (OK) : The list of publications is returned. 404 (NOT FOUND) : The specified event does not exist. [ { \"id\":\"publication-1\", \"channel\":\"engage\", \"mediatype\":\"text/html\", \"url\":\"http://engage.opencast.org/engage/ui/player.html?id=123\" }, { \"id\":\"publication-2\", \"channel\":\"oaipmh\", \"mediatype\":\"text/html\", \"url\":\"http://oaipmh.opencast.org/default/?verb=GetRecord&id=123\" } ] GET /api/events/{event_id}/publications/{publication_id} Returns a single publication. Query String Parameter Type Description sign boolean Whether public distribution urls should be signed Response 200 (OK) : The track details are returned. 404 (NOT FOUND) : The specified event does not exist. 404 (NOT FOUND) : The specified publication does not exist. { \"id\":\"publication-1\", \"channel\":\"engage\", \"mediatype\":\"text/html\", \"url\":\"http://engage.opencast.org/engage/ui/player.html?id=123\", \"media\":[ { \"id\":\"track-1\", \"mediatype\":\"video/mp4\", \"url\":\"http://download.opencast.org/123/presenter.mp4\", \"flavor\":\"presenter/delivery\", \"size\":84938490, \"checksum\":\"58308405383094\", \"tags\":[ ], \"has_audio\":true, \"has_video\":true, \"duration\":3648, \"description\":\"Video: h264 (Constrained Baseline) (avc1 / 0x31637661), yuv420p, 640x360, 447 kb/s, 25 fps, 25\" }, { \"id\":\"track-2\", \"mediatype\":\"audio/aac\", \"url\":\"http://download.opencast.org/123/presenter.m4a\", \"flavor\":\"presenter/audio\", \"size\":9364, \"checksum\":\"839478372\", \"tags\":[ ], \"has_audio\":true, \"has_video\":false, \"duration\":3648, \"description\":\"aac (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 96 kb/s (default)\" } ], \"attachments\":[ { \"id\":\"attachment-1\", \"mediatype\":\"image/png\", \"url\":\"http://download.opencast.org/123/preview.png\", \"flavor\":\"presenter/preview\", \"size\":62728, \"checksum\":\"389475737\", \"tags\":[ ] } ], \"metadata\":[ { \"id\":\"catalog-1\", \"mediatype\":\"text/xml\", \"url\":\"http://download.opencast.org/123/dublincore.xml\", \"flavor\":\"dublincore/episode\", \"size\":364, \"checksum\":\"18498498383\", \"tags\":[ ] } ] } Scheduling Information GET /api/events/{event_id}/scheduling Available since API version 1.1.0. Returns an event's scheduling information. Response 200 (OK) : The scheduling information is returned. 204 (NO CONTENT) : The event is not scheduled. 404 (NOT FOUND) : The specified event does not exist. { \"agent_id\": \"ca24\", \"start\": \"2018-03-27T16:00:00Z\", \"end\": \"2018-03-27T19:00:00Z\", \"inputs\": [\"default\"] } PUT /api/events/{event_id}/scheduling Available since API version 1.1.0. Update the scheduling information of the event with id {event_id} . Form Parameters Type Description Default Version scheduling string The scheduling information. Required 1.1.0 allowConflict boolean Allow conflicts when updating scheduling. false 1.2.0 Sample scheduling: { \"agent_id\": \"ca24\", \"start\": \"2018-03-27T16:00:00Z\", \"end\": \"2018-03-27T19:00:00Z\", \"inputs\": [\"default\"] } Response 204 (NO CONTENT) : The scheduling information of the event has been updated. 400 (BAD REQUEST) : The request is invalid or inconsistent. 404 (NOT FOUND) : The specified event does not exist. 409 (CONFLICT) : The scheduling information could not be updated due to a conflict. A list of conflicting events is returned. In case of a conflict: [ { \"start\":\"2018-03-21T14:00:00Z\", \"end\":\"2018-03-21T16:00:00Z\", \"title\":\"My Event 06\" } ] allowConflict allows the schedule to be updated without checking for conflicts. To allow conflicts ( true ) the call MUST be made with a user that has an Administrative Role . If not handled properly this will likely cause two or more events to be scheduled on a particular capture agent at the same time, which will then cause a capture failure for all but one of the events. The person making this call and allowing conflicts to exist, will bear the responsibility of resolving the conflicts that might result.","title":"Events API"},{"location":"api/events-api/#general","text":"","title":"General"},{"location":"api/events-api/#get-apievents","text":"Returns a list of events. The following query string parameters are supported to filter, sort and paginate the returned list: Query String Parameter Type Description filter string A comma-separated list of filters to limit the results with (see Filtering ). See the below table for the list of available filters. Version 1.5.0 and newer support comma separated use of the filter keyword, creating a logical OR. sort string A comma-separated list of sort criteria (see Sorting ). See the below table for the list of available sort criteria limit integer The maximum number of results to return (see Pagination ) offset integer The index of the first result to return (see Pagination ) The following filters are available: Filter Name Description contributors Events where the contributors match. Can occur multiple times location Events based upon the location it is scheduled in series Events based upon which series they are a part of. Use the series identifier here. If using version 1.1.0 or higher, please use is_part_of instead subject Filters events based upon which subject they are a part of textFilter Filters events where any part of the event's metadata fields match this value identifier Filters events whose identifiers match this value. Can occur multiple times (version 1.1.0 and higher) title Filters events whose title match this value (version 1.1.0 and higher) description Filters events whose description match this value (version 1.1.0 and higher) series_name Filters events that belong to series with the given name (version 1.1.0 or higher) language Filters events whose language match this value (version 1.1.0 or higher) created Filters events whose created match this value (version 1.1.0 or higher) license Filters events whose license match this value (version 1.1.0 or higher) rightsholder Filters events whose rights holder matches this value (version 1.1.0 or higher) status Filters events based on their status (version 1.1.0 or higher) is_part_of Events based upon which series they are a part of. Use the series identifier here (version 1.1.0 or higher) source Filter events whose source match this value (version 1.1.0 or higher) agent_id Filter events based on the capture agent id (version 1.1.0 or higher) start Filter events based on start date (version 1.1.0 or higher) technical_start Filter events based on the technical start date (version 1.1.0 or higher) Note: The filters start and technical_start expect the following value: datetime + '/' + datetime The list can be sorted by the following criteria: Sort Criteria Description title By the title of the event presenter By the presenter of the event start_date By the start date of the event end_date By the end date of the event review_status By whether the event has been reviewed and approved or not [DEPRECATED] workflow_state By the current processing state of the event. Is it scheduled to be recorded (INSTANTIATED), currently processing (RUNNING), paused waiting for a resource or user paused (PAUSED), cancelled (STOPPED), currently failing (FAILING), already failed (FAILED), or finally SUCCEEDED scheduling_status By the current scheduling status of the event [DEPRECATED] series_name By the series name of the event location By the location (capture agent) that the event will be or has been recorded on This request additionally supports the following query string parameters to include additional information directly in the response: Query String Parameter Type Description sign boolean Whether public distribution urls should be signed withacl boolean Whether the acl metadata should be included in the response withmetadata boolean Whether the metadata catalogs should be included in the response withpublications boolean Whether the publication ids and urls should be included in the response withscheduling boolean Whether the scheduling information should be included in the response (version 1.1.0 and higher). By setting the optional sign parameter to true , the method will pre-sign distribution urls if URL signing is turned on in Opencast. Remember to consider the maximum validity of signed URLs when caching this response. Sample request https://opencast.example.org/api/events?sort=title:DESC&limit=5&offset=1&filter=location:ca-01 Response 200 (OK) : A (potentially empty) list of events is returned. The list is represented as JSON array where each element is a JSON object with the following fields: Field Type Description identifier string The unique identifier of the event creator string The technical creator of this event presenter * array[string] The presenters of this event created * datetime The date and time this event was created subjects * array[string] The subjects of this event start datetime The technical (version < 1.4.0) or bibliographic (version >= 1.4.0) start date and time of this event description * string The description of this event title * string The title of this event processing_state string The current processing state of this event duration string The bibliographic duration of this event archive_version string The current version of this event contributor * array[string] The contributors of this event has_previews boolean Whether this event can be opened with the video editor location * string The bibliographic location of this event publication_status array[string] The publications available for this event language * string The language of this event (version 1.1.0 and higher) rightsholder * string The rights holder of this event (version 1.1.0 and higher) license * string The license of this event (version 1.1.0 and higher) is_part_of * string The technical identifier of the series this event belongs to (version 1.1.0 and higher) series string The title of the series this event belongs to (version 1.1.0 and higher) source * string The source of this event (version 1.1.0 and higher) status string The technical status of this event (version 1.1.0 and higher) * Metadata fields of metadata catalog dublincore/episode Example [ { \"identifier\": \"776d4970-bc2d-45c4-900a-1f80b7990cb8\", \"creator\": \"Opencast Project Administrator\", \"presenter\": [ \"Prof. X\", \"Dr. Who\" ], \"created\": \"2018-03-19T16:11:48Z\", \"subjects\": [ \"Mathematics\", \"Basics\" ], \"start\": \"2018-03-19T16:11:48Z\", \"description\": \"This lecture is about the very basics\", \"title\": \"Lecture 1 - The Basics\", \"processing_state\": \"SUCCEEDED\", \"duration\": 0, \"archive_version\": 3, \"contributor\": [ \"Prof. X\" ], \"has_previews\": true, \"location\": \"\", \"publication_status\": [ \"internal\", \"engage-player\", \"api\", \"oaipmh-default\" ] }, { \"identifier\": \"2356d450-b42d-94xd-2a0d-3fa04745c0cb8\", \"creator\": \"Opencast Project Administrator\", \"presenter\": [ ], \"created\": \"2018-04-20T09:38:42Z\", \"subjects\": [ \"Physics\" ], \"start\": \"2018-04-23T08:00:00Z\", \"description\": \"The forces explained\", \"title\": \"Lecture 8 - Advanced stuff\", \"processing_state\": \"SUCCEEDED\", \"duration\": 0, \"archive_version\": 2, \"contributor\": [ ], \"has_previews\": true, \"location\": \"\", \"publication_status\": [ \"internal\", \"engage-player\", \"api\", \"oaipmh-default\" ] } ]","title":"GET /api/events"},{"location":"api/events-api/#post-apievents","text":"Creates an event by sending metadata, access control list, processing instructions and files in a multipart request . Multipart Form Parameters Type Description acl acl A collection of roles with their possible action metadata catalogs Event metadata as Form param presenter file Presenter movie track presentation file Presentation movie track audio file Audio track processing string Processing instructions task configuration scheduling string Scheduling information (version 1.1.0 and higher) Scheduling Field Required Type Description agent_id Yes string The technical identifier of the capture agent start Yes datetime The date and time (UTC) when the recording shall start end Yes* datetime The date and time (UTC) when the recording shall end duration Yes* integer The duration of the recording in milliseconds rrule Yes* rrule The recurrence rule used to create multiple scheduled events * When creating a single scheduled event, either end or duration must be specified. When creating multiple scheduled events, the fields end , duration and rrule are required To create a single scheduled event, omit the field rrule . You can specify the start and end of the recording by using either the fields start and end or the fields start and duration . Please note that specifying both end and duration is not valid. To create multiple scheduled events, the field rrule is used. This field contains the recurrence rule used to determine the dates and times the scheduled events shall be created within the time period specified by the fields start and end + duration . Please note that duration is a mandatory field when creating multiple scheduled events. Additional Notes Both start and end must be specified in UTC Sample metadata: [ { \"flavor\": \"dublincore/episode\", \"fields\": [ { \"id\": \"title\", \"value\": \"Captivating title\" }, { \"id\": \"subjects\", \"value\": [\"John Clark\", \"Thiago Melo Costa\"] }, { \"id\": \"description\", \"value\": \"A great description\" }, { \"id\": \"startDate\", \"value\": \"2016-06-22\" }, { \"id\": \"startTime\", \"value\": \"13:30:00Z\" } ] } ] processing: { \"workflow\": \"schedule-and-upload\", \"configuration\": { \"flagForCutting\": \"false\", \"flagForReview\": \"false\", \"publishToEngage\": \"true\", \"publishToHarvesting\": \"true\", \"straightToPublishing\": \"true\" } } acl: [ { \"action\": \"write\", \"role\": \"ROLE_ADMIN\" }, { \"action\": \"read\", \"role\": \"ROLE_USER\" } ] scheduling a single event: { \"agent_id\": \"ca24\", \"start\": \"2018-03-27T16:00:00Z\", \"end\": \"2018-03-27T19:00:00Z\", \"inputs\": [\"default\"] } scheduling multiple events: { \"agent_id\": \"ca24\", \"start\": \"2019-05-20T16:00:00Z\", \"end\": \"2019-06-10T19:00:00Z\", \"inputs\": [\"default\"], \"rrule\":\"FREQ=WEEKLY;BYDAY=MO,TU,WE,TH,FR;BYHOUR=16;BYMINUTE=0\", \"duration\":1080000; } Response 200 (OK) : Multiple new events were created (scheduling with rrule ). 201 (CREATED) : A new event is created and its identifier is returned in the Location header. 400 (BAD REQUEST) : The request is invalid or inconsistent. 409 (CONFLICT) : The event could not be created due to a scheduling conflict. A list of conflicting events is returned. In case of success, when scheduling multiple events (200): [ { \"identifier\": \"e6aeb8df-a852-46cd-8128-b89de696f20e\" }, { \"identifier\": \"90e93bf6-ed95-483b-8e57-3e2de77a786f\" } ] In case of success (201): Location: http://api.opencast.org/api/events/e6aeb8df-a852-46cd-8128-b89de696f20e { \"identifier\": \"e6aeb8df-a852-46cd-8128-b89de696f20e\" } In case of a conflict (409): [ { \"start\":\"2018-03-21T14:00:00Z\", \"end\":\"2018-03-21T16:00:00Z\", \"title\":\"My Event 06\" } ]","title":"POST /api/events"},{"location":"api/events-api/#get-apieventsevent_id","text":"Returns a single event. By setting the optional sign parameter to true , the method will pre-sign distribution urls if signing is turned on in Opencast. Remember to consider the maximum validity of signed URLs when caching this response. Query String Parameter Type Description sign boolean Whether public distribution urls should be signed. withacl boolean Whether the acl metadata should be included in the response. withmetadata boolean Whether the metadata catalogs should be included in the response. withpublications boolean Whether the publication ids and urls should be included in the response. withscheduling boolean Whether the scheduling information should be included in the response (version 1.1.0 and higher). Response 200 (OK) : The event is returned as JSON object with the following fields: Field Type Description identifier string The unique identifier of the event creator string The technical creator of this event presenter * array[string] The presenters of this event created * datetime The date and time this event was created subjects * array[string] The subjects of this event start datetime The technical (version < 1.4.0) or bibliographic (version >= 1.4.0) start date and time of this event description * string The description of this event title * string The title of this event processing_state string The current processing state of this event duration string The bibliographic duration of this event archive_version string The current version of this event contributor * array[string] The contributors of this event has_previews boolean Whether this event can be opened with the video editor location * string The bibliographic location of this event publication_status array[string] The publications available for this event language * string The language of this event (version 1.1.0 and higher) rightsholder * string The rights holder of this event (version 1.1.0 and higher) license * string The license of this event (version 1.1.0 and higher) is_part_of * string The technical identifier of the series this event belongs to (version 1.1.0 and higher) series string The title of the series this event belongs to (version 1.1.0 and higher) source * string The source of this event (version 1.1.0 and higher) status string The technical status of this event (version 1.1.0 and higher) * Metadata fields of metadata catalog dublincore/episode 404 (NOT FOUND) : The specified event does not exist. { \"identifier\": \"776d4970-bc2d-45c4-900a-1f80b7990cb8\", \"creator\": \"Opencast Project Administrator\", \"presenter\": [ \"Prof. X\", \"Dr. Who\" ], \"created\": \"2018-03-19T16:11:48Z\", \"subjects\": [ \"Mathematics\", \"Basics\" ], \"start\": \"2018-03-19T16:11:48Z\", \"description\": \"This lecture is about the very basics\", \"title\": \"Lecture 1 - The Basics\", \"processing_state\": \"SUCCEEDED\", \"duration\": 0, \"archive_version\": 3, \"contributor\": [ \"Prof. X\" ], \"has_previews\": true, \"location\": \"\", \"publication_status\": [ \"internal\", \"engage-player\", \"api\", \"oaipmh-default\" ] }","title":"GET /api/events/{event_id}"},{"location":"api/events-api/#post-apieventsevent_id","text":"Updates an event. Multipart Form Parameters Type Description acl acl A collection of roles with their possible action metadata catalogs Event metadata as Form param presenter file Presenter movie track presentation file Presentation movie track audio file Audio track processing string Processing instructions task configuration scheduling string Scheduling information (version 1.1.0 and higher) Sample This sample request will update the Dublin Core metadata section of the event only. metadata: [ { \"flavor\": \"dublincore/episode\", \"fields\": [ { \"id\": \"title\", \"value\": \"Captivating title\" }, { \"id\": \"subjects\", \"value\": [\"Space\", \"Final Frontier\"] }, { \"id\": \"description\", \"value\": \"A great description\" } ] } ] Response 204 (NO CONTENT) : The event has been updated. 404 (NOT FOUND) : The specified event does not exist. 409 (CONFLICT) : The event could not be updated due to a scheduling conflict. A list of conflicting events is returned. In case of a conflict (when updating scheduling ): [ { \"start\":\"2018-03-21T14:00:00Z\", \"end\":\"2018-03-21T16:00:00Z\", \"title\":\"My Event 06\" } ]","title":"POST /api/events/{event_id}"},{"location":"api/events-api/#delete-apieventsevent_id","text":"Deletes an event. Response 204 (NO CONTENT) : The event has been deleted. 404 (NOT FOUND) : The specified event does not exist.","title":"DELETE /api/events/{event_id}"},{"location":"api/events-api/#access-policy","text":"Most events in Opencast come with an access control list (ACL), containing entries that map actions to roles, either allowing or denying that action. The section on roles in the chapter on Authorization will help shed some light on what kind of roles are available and how they are assigned to the current user. For more information about access control lists, please refer to Access Control Lists .","title":"Access Policy"},{"location":"api/events-api/#get-apieventsevent_idacl","text":"Returns an event's access policy. Response 200 (OK) : The access control list for the specified event is returned as acl . 404 (NOT FOUND) : The specified event does not exist. [ { \"allow\": true, \"action\": \"write\", \"role\": \"ROLE_ADMIN\" }, { \"allow\": true, \"action\": \"read\", \"role\": \"ROLE_USER\" } ]","title":"GET /api/events/{event_id}/acl"},{"location":"api/events-api/#put-apieventsevent_idacl","text":"Update an event's access policy. Form Parameters Type Description acl acl Access policy Note that the existing access control list will be overwritten. Sample acl: [ { \"allow\": true, \"action\": \"write\", \"role\": \"ROLE_ADMIN\" }, { \"allow\": true, \"action\": \"read\", \"role\": \"ROLE_USER\" } ] Response 204 (NO CONTENT) : The access control list for the specified event is updated. 404 (NOT FOUND) : The specified event does not exist.","title":"PUT /api/events/{event_id}/acl"},{"location":"api/events-api/#post-apieventsevent_idaclaction","text":"Grants permission to execute action on the specified event to any user with role role . Note that this is a convenience method to avoid having to build and post a complete access control list. Path Parameters Type Description event_id string Event identifier action string The action that is allowed to be executed Form Parameters Type Description role string The role that is granted permission Note that other access control lists entries will not be affected by this request. Sample role: \"ROLE_STUDENT\" Response 204 (NO CONTENT) : The permission has been created in the access control list of the specified event. 404 (NOT FOUND) : The specified event does not exist.","title":"POST /api/events/{event_id}/acl/{action}"},{"location":"api/events-api/#delete-apieventsevent_idaclactionrole","text":"Revokes permission to execute action on the specified event from any user with role role . Path Parameters Type Description event_id string Event identifier action string The action that is no longer allowed to be executed role string The role that is no longer granted permission Note that other access control lists entries will not be affected by this request. Response 204 (NO CONTENT) : The permission has been revoked from the access control list of the specified event. 404 (NOT FOUND) : The specified event does not exist.","title":"DELETE /api/events/{event_id}/acl/{action}/{role}"},{"location":"api/events-api/#metadata","text":"This section describes how to use the External API to work with metadata catalogs associated to events. Opencast manages the bibliographic metadata of series using metadata catalogs which are identified by flavors. The default metadata catalog for Opencast events has the flavor dublincore/episode . Opencast additionally supports extended metadata catalogs for series that can be configured. The External API supports both the default event metadata catalog and events extended metadata catalogs. For the default event metadata catalog, the metadata is directly returned in the responses. Since the metadata catalogs can be configured, the External API provides a facility to retrieve the catalog configuration of series metadata catalogs. For more details about this mechanism, please refer to \"Metadata Catalogs\" .","title":"Metadata"},{"location":"api/events-api/#get-apieventsevent_idmetadata","text":"Returns the complete set of metadata. Response 200 (OK) : The metadata collection is returned as catalogs . 404 (OK) : The specified event does not exist. [ { \"label\": \"EVENTS.EVENTS.DETAILS.CATALOG.EPISODE\", \"flavor\": \"dublincore/episode\", \"fields\": [ { \"id\": \"title\", \"readOnly\": false, \"value\": \"Captivating title\", \"label\": \"EVENTS.EVENTS.DETAILS.METADATA.TITLE\", \"type\": \"text\", \"required\": true }, { \"id\": \"description\", \"readOnly\": false, \"value\": \"A great description\", \"label\": \"EVENTS.EVENTS.DETAILS.METADATA.DESCRIPTION\", \"type\": \"text_long\", \"required\": false } ] }, { \"label\": \"EVENTS.EVENTS.DETAILS.CATALOG.LICENSE\", \"flavor\": \"license/episode\", \"fields\": [ { \"id\": \"license\", \"readOnly\": false, \"value\": \"CCND\", \"label\": \"EVENTS.EVENTS.DETAILS.METADATA.LICENSE\", \"collection\": { \"BSD\": \"EVENTS.LICENSE.BSD\", \"GPL3\": \"EVENTS.LICENSE.GPL\", \"CCND\": \"EVENTS.LICENSE.CCND\" }, \"type\": \"text\", \"required\": false } ] } ]","title":"GET /api/events/{event_id}/metadata"},{"location":"api/events-api/#get-apieventsevent_idmetadata_1","text":"Returns the event's metadata of the specified type. For a metadata catalog there is the flavor such as dublincore/episode and this is the unique type. Query String Parameters Type Description type flavor The type of metadata to get Response 200 (OK) : The metadata collection is returned as fields . 404 (NOT FOUND) : The specified event does not exist. [ { \"id\": \"title\", \"readOnly\": false, \"value\": \"Captivating title\", \"label\": \"EVENTS.EVENTS.DETAILS.METADATA.TITLE\", \"type\": \"text\", \"required\": true }, { \"id\": \"description\", \"readOnly\": false, \"value\": \"A great description\", \"label\": \"EVENTS.EVENTS.DETAILS.METADATA.DESCRIPTION\", \"type\": \"text_long\", \"required\": false } ]","title":"GET /api/events/{event_id}/metadata"},{"location":"api/events-api/#put-apieventsevent_idmetadata","text":"Update the metadata with the matching type of the specified event. For a metadata catalog there is the flavor such as dublincore/episode and this is the unique type. Query String Parameters Type Description type flavor The type of metadata to update Form Parameters Type Description metadata values Metadata fields and values to be updated Note that metadata fields not included in the form parameter metadata will not be updated. Sample metadata: [ { \"id\": \"title\", \"value\": \"Captivating title\" }, { \"id\": \"subjects\", \"value\": [\"Space\", \"Final Frontier\"] }, { \"id\": \"description\", \"value\": \"A great description\" } ] Response 204 (NO CONTENT) : The metadata of the given namespace has been updated. 400 (BAD REQUEST) : The request is invalid or inconsistent. 404 (NOT FOUND) : The specified event does not exist.","title":"PUT /api/events/{event_id}/metadata"},{"location":"api/events-api/#delete-apieventsevent_idmetadata","text":"Delete the metadata namespace catalog of the specified event. This will remove all fields and values of the catalog. Query String Parameters Type Description type flavor The type of metadata to delete Note that the metadata catalog of type dublincore/episode cannot be deleted. Response 204 (NO CONTENT) : The metadata of the given namespace has been updated. 403 (FORBIDDEN) : The main metadata catalog dublincore/episode cannot be deleted as it has mandatory fields. 404 (NOT FOUND) : The specified event does not exist.","title":"DELETE /api/events/{event_id}/metadata"},{"location":"api/events-api/#publications","text":"","title":"Publications"},{"location":"api/events-api/#get-apieventsevent_idpublications","text":"Returns an event's list of publications. Query String Parameter Type Description sign boolean Whether public distribution urls should be signed Response 200 (OK) : The list of publications is returned. 404 (NOT FOUND) : The specified event does not exist. [ { \"id\":\"publication-1\", \"channel\":\"engage\", \"mediatype\":\"text/html\", \"url\":\"http://engage.opencast.org/engage/ui/player.html?id=123\" }, { \"id\":\"publication-2\", \"channel\":\"oaipmh\", \"mediatype\":\"text/html\", \"url\":\"http://oaipmh.opencast.org/default/?verb=GetRecord&id=123\" } ]","title":"GET /api/events/{event_id}/publications"},{"location":"api/events-api/#get-apieventsevent_idpublicationspublication_id","text":"Returns a single publication. Query String Parameter Type Description sign boolean Whether public distribution urls should be signed Response 200 (OK) : The track details are returned. 404 (NOT FOUND) : The specified event does not exist. 404 (NOT FOUND) : The specified publication does not exist. { \"id\":\"publication-1\", \"channel\":\"engage\", \"mediatype\":\"text/html\", \"url\":\"http://engage.opencast.org/engage/ui/player.html?id=123\", \"media\":[ { \"id\":\"track-1\", \"mediatype\":\"video/mp4\", \"url\":\"http://download.opencast.org/123/presenter.mp4\", \"flavor\":\"presenter/delivery\", \"size\":84938490, \"checksum\":\"58308405383094\", \"tags\":[ ], \"has_audio\":true, \"has_video\":true, \"duration\":3648, \"description\":\"Video: h264 (Constrained Baseline) (avc1 / 0x31637661), yuv420p, 640x360, 447 kb/s, 25 fps, 25\" }, { \"id\":\"track-2\", \"mediatype\":\"audio/aac\", \"url\":\"http://download.opencast.org/123/presenter.m4a\", \"flavor\":\"presenter/audio\", \"size\":9364, \"checksum\":\"839478372\", \"tags\":[ ], \"has_audio\":true, \"has_video\":false, \"duration\":3648, \"description\":\"aac (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 96 kb/s (default)\" } ], \"attachments\":[ { \"id\":\"attachment-1\", \"mediatype\":\"image/png\", \"url\":\"http://download.opencast.org/123/preview.png\", \"flavor\":\"presenter/preview\", \"size\":62728, \"checksum\":\"389475737\", \"tags\":[ ] } ], \"metadata\":[ { \"id\":\"catalog-1\", \"mediatype\":\"text/xml\", \"url\":\"http://download.opencast.org/123/dublincore.xml\", \"flavor\":\"dublincore/episode\", \"size\":364, \"checksum\":\"18498498383\", \"tags\":[ ] } ] }","title":"GET /api/events/{event_id}/publications/{publication_id}"},{"location":"api/events-api/#scheduling-information","text":"","title":"Scheduling Information"},{"location":"api/events-api/#get-apieventsevent_idscheduling","text":"Available since API version 1.1.0. Returns an event's scheduling information. Response 200 (OK) : The scheduling information is returned. 204 (NO CONTENT) : The event is not scheduled. 404 (NOT FOUND) : The specified event does not exist. { \"agent_id\": \"ca24\", \"start\": \"2018-03-27T16:00:00Z\", \"end\": \"2018-03-27T19:00:00Z\", \"inputs\": [\"default\"] }","title":"GET /api/events/{event_id}/scheduling"},{"location":"api/events-api/#put-apieventsevent_idscheduling","text":"Available since API version 1.1.0. Update the scheduling information of the event with id {event_id} . Form Parameters Type Description Default Version scheduling string The scheduling information. Required 1.1.0 allowConflict boolean Allow conflicts when updating scheduling. false 1.2.0 Sample scheduling: { \"agent_id\": \"ca24\", \"start\": \"2018-03-27T16:00:00Z\", \"end\": \"2018-03-27T19:00:00Z\", \"inputs\": [\"default\"] } Response 204 (NO CONTENT) : The scheduling information of the event has been updated. 400 (BAD REQUEST) : The request is invalid or inconsistent. 404 (NOT FOUND) : The specified event does not exist. 409 (CONFLICT) : The scheduling information could not be updated due to a conflict. A list of conflicting events is returned. In case of a conflict: [ { \"start\":\"2018-03-21T14:00:00Z\", \"end\":\"2018-03-21T16:00:00Z\", \"title\":\"My Event 06\" } ] allowConflict allows the schedule to be updated without checking for conflicts. To allow conflicts ( true ) the call MUST be made with a user that has an Administrative Role . If not handled properly this will likely cause two or more events to be scheduled on a particular capture agent at the same time, which will then cause a capture failure for all but one of the events. The person making this call and allowing conflicts to exist, will bear the responsibility of resolving the conflicts that might result.","title":"PUT /api/events/{event_id}/scheduling"},{"location":"api/glossary/","text":"Glossary General Client A system that is using the External API, making requests and consuming responses. Data Event A recording that is either going to take place, has been recorded using Opencast scheduling or has been uploaded either using the External API or the Opencast administrative user interface. A collection of events may be grouped using a series. Series A collection of events. Users and Groups User A person accessing data provided by the External API. Producer A user that is managing individual recordings or groups of recordings. The producer uses the External API to create, curate, publish, retract recordings. Spectator A spectator accesses a subset of recordings that has been published by a producer.","title":"Glossary"},{"location":"api/glossary/#glossary","text":"","title":"Glossary"},{"location":"api/glossary/#general","text":"","title":"General"},{"location":"api/glossary/#client","text":"A system that is using the External API, making requests and consuming responses.","title":"Client"},{"location":"api/glossary/#data","text":"","title":"Data"},{"location":"api/glossary/#event","text":"A recording that is either going to take place, has been recorded using Opencast scheduling or has been uploaded either using the External API or the Opencast administrative user interface. A collection of events may be grouped using a series.","title":"Event"},{"location":"api/glossary/#series","text":"A collection of events.","title":"Series"},{"location":"api/glossary/#users-and-groups","text":"","title":"Users and Groups"},{"location":"api/glossary/#user","text":"A person accessing data provided by the External API.","title":"User"},{"location":"api/glossary/#producer","text":"A user that is managing individual recordings or groups of recordings. The producer uses the External API to create, curate, publish, retract recordings.","title":"Producer"},{"location":"api/glossary/#spectator","text":"A spectator accesses a subset of recordings that has been published by a producer.","title":"Spectator"},{"location":"api/groups-api/","text":"General GET /api/groups GET /api/groups/{group_id} POST /api/groups PUT /api/groups/{group_id} DELETE /api/groups/{group_id} Members POST /api/groups/{group_id}/members DELETE /api/groups/{group_id}/members/{member_id} General GET /api/groups Returns a list of groups. The following query string parameters can be used to filter, sort and paginate the returned list: Query String Parameter Type Description filter string A comma-separated list of filters to limit the results with (see Filtering ). See the below table for the list of available filters sort string A comma-separated list of sort criteria (see Sorting ). See the below table for the list of available sort criteria limit integer The maximum number of results to return (see Pagination ) offset integer The index of the first result to return (see Pagination ) The following filters are available: Filter Name Description name Groups where the name specified in the metadata field match The list can be sorted by the following criteria: Sort Criteria Description name By the group name description By the group description role By the group role members By the group members roles By the group roles Sample request https://opencast.domain.com/api/groups?sort=name:ASC&limit=2&offset=1 Response 200 (OK) : A (potentially empty) list of groups as JSON array of groups wheres the JSON objects representing groups have the following fields: Field Type Description identifier string The identifier of the group role string The role of the group organization string The tenant identifier roles string The roles assigned to the group (comma-separated list) members string The list of users that belong to this group (comma-separated list of usernames) name string The name of the group description string The description of the group Example [ { \"identifier\": \"MH_DEFAULT_ORG_SYSTEM_ADMINS\", \"role\": \"ROLE_GROUP_MH_DEFAULT_ORG_SYSTEM_ADMINS\", \"organization\": \"mh_default_org\", \"roles\": \"ROLE_OAUTH_USER,ROLE_SUDO,ROLE_ADMIN,ROLE_ANONYMOUS\", \"members\": \"admin,admin2\", \"name\": \"Opencast Project System Administrators\", \"description\": \"System administrators of 'Opencast Project'\" }, { \"identifier\": \"MH_DEFAULT_ORG_EXTERNAL_APPLICATIONS\", \"role\": \"ROLE_GROUP_MH_DEFAULT_ORG_EXTERNAL_APPLICATIONS\", \"organization\": \"mh_default_org\", \"roles\": \"ROLE_EXAMPLE1,ROLE_EXAMPLE2,ROLE_EXAMPLE3\", \"members\": \"apiuser\", \"name\": \"Opencast Project External Applications\", \"description\": \"External application users of 'Opencast Project'\" } ] GET /api/groups/{group_id} Returns a single group. Response 200 (OK) : The group is returned as JSON object with the following fields: Field Type Description identifier string The identifier of the group role string The role of the group organization string The tenant identifier roles string The roles assigned to the group (comma-separated list) members string The list of users that belong to this group (comma-separated list of usernames) name string The name of the group description string The description of the group 404 (NOT FOUND) : The specified group does not exist. Example { \"identifier\": \"MH_DEFAULT_ORG_SYSTEM_ADMINS\", \"role\": \"ROLE_GROUP_MH_DEFAULT_ORG_SYSTEM_ADMINS\", \"organization\": \"mh_default_org\", \"roles\": \"ROLE_OAUTH_USER,ROLE_SUDO,ROLE_ADMIN,ROLE_ANONYMOUS\", \"members\": \"admin,admin2\", \"name\": \"Opencast Project System Administrators\", \"description\": \"System administrators of 'Opencast Project'\" } POST /api/groups Creates a group. Form Parameters Required Type Description name yes string Group Name description no string Group Description roles no string Comma-separated list of roles members no string Comma-separated list of members Response This request does not return data. 201 (CREATED) : A new group is created. 400 (BAD REQUEST) : The request is invalid or inconsistent. 409 (CONFLICT) : The group could not be created because a group with the same name already exists. PUT /api/groups/{group_id} Updates a group. Form Parameters Required Type Description name yes string Group Name description no string Group Description roles no string Comma-separated list of roles members no string Comma-separated list of members If any of form parameters are ommited, the respective fields of the group will not be changed. Response This request does not return data. 200 (OK) : The group has been updated. 404 (NOT FOUND) : The specified group does not exist. DELETE /api/groups/{group_id} Deletes a group. Response This request does not return data. 204 (NO CONTENT) : The group has been deleted. 404 (NOT FOUND) : The specified group does not exist. Members POST /api/groups/{group_id}/members Adds a member to a group. Form Parameters Required Type Description member yes string The username of the member to be added Response 200 (OK) : The member has been added or was already member of the group. If the member has been added, the request does not return data. In case that the member already was in the group, the following message is returned as string: Member is already member of group 404 (NOT FOUND) : The specified group does not exist. DELETE /api/groups/{group_id}/members/{member_id} Removes a member from a group Response 200 (NO CONTENT) : The member has been removed. 404 (NOT FOUND) : The specified group or member does not exist.","title":"Groups API"},{"location":"api/groups-api/#general","text":"","title":"General"},{"location":"api/groups-api/#get-apigroups","text":"Returns a list of groups. The following query string parameters can be used to filter, sort and paginate the returned list: Query String Parameter Type Description filter string A comma-separated list of filters to limit the results with (see Filtering ). See the below table for the list of available filters sort string A comma-separated list of sort criteria (see Sorting ). See the below table for the list of available sort criteria limit integer The maximum number of results to return (see Pagination ) offset integer The index of the first result to return (see Pagination ) The following filters are available: Filter Name Description name Groups where the name specified in the metadata field match The list can be sorted by the following criteria: Sort Criteria Description name By the group name description By the group description role By the group role members By the group members roles By the group roles Sample request https://opencast.domain.com/api/groups?sort=name:ASC&limit=2&offset=1 Response 200 (OK) : A (potentially empty) list of groups as JSON array of groups wheres the JSON objects representing groups have the following fields: Field Type Description identifier string The identifier of the group role string The role of the group organization string The tenant identifier roles string The roles assigned to the group (comma-separated list) members string The list of users that belong to this group (comma-separated list of usernames) name string The name of the group description string The description of the group Example [ { \"identifier\": \"MH_DEFAULT_ORG_SYSTEM_ADMINS\", \"role\": \"ROLE_GROUP_MH_DEFAULT_ORG_SYSTEM_ADMINS\", \"organization\": \"mh_default_org\", \"roles\": \"ROLE_OAUTH_USER,ROLE_SUDO,ROLE_ADMIN,ROLE_ANONYMOUS\", \"members\": \"admin,admin2\", \"name\": \"Opencast Project System Administrators\", \"description\": \"System administrators of 'Opencast Project'\" }, { \"identifier\": \"MH_DEFAULT_ORG_EXTERNAL_APPLICATIONS\", \"role\": \"ROLE_GROUP_MH_DEFAULT_ORG_EXTERNAL_APPLICATIONS\", \"organization\": \"mh_default_org\", \"roles\": \"ROLE_EXAMPLE1,ROLE_EXAMPLE2,ROLE_EXAMPLE3\", \"members\": \"apiuser\", \"name\": \"Opencast Project External Applications\", \"description\": \"External application users of 'Opencast Project'\" } ]","title":"GET /api/groups"},{"location":"api/groups-api/#get-apigroupsgroup_id","text":"Returns a single group. Response 200 (OK) : The group is returned as JSON object with the following fields: Field Type Description identifier string The identifier of the group role string The role of the group organization string The tenant identifier roles string The roles assigned to the group (comma-separated list) members string The list of users that belong to this group (comma-separated list of usernames) name string The name of the group description string The description of the group 404 (NOT FOUND) : The specified group does not exist. Example { \"identifier\": \"MH_DEFAULT_ORG_SYSTEM_ADMINS\", \"role\": \"ROLE_GROUP_MH_DEFAULT_ORG_SYSTEM_ADMINS\", \"organization\": \"mh_default_org\", \"roles\": \"ROLE_OAUTH_USER,ROLE_SUDO,ROLE_ADMIN,ROLE_ANONYMOUS\", \"members\": \"admin,admin2\", \"name\": \"Opencast Project System Administrators\", \"description\": \"System administrators of 'Opencast Project'\" }","title":"GET /api/groups/{group_id}"},{"location":"api/groups-api/#post-apigroups","text":"Creates a group. Form Parameters Required Type Description name yes string Group Name description no string Group Description roles no string Comma-separated list of roles members no string Comma-separated list of members Response This request does not return data. 201 (CREATED) : A new group is created. 400 (BAD REQUEST) : The request is invalid or inconsistent. 409 (CONFLICT) : The group could not be created because a group with the same name already exists.","title":"POST /api/groups"},{"location":"api/groups-api/#put-apigroupsgroup_id","text":"Updates a group. Form Parameters Required Type Description name yes string Group Name description no string Group Description roles no string Comma-separated list of roles members no string Comma-separated list of members If any of form parameters are ommited, the respective fields of the group will not be changed. Response This request does not return data. 200 (OK) : The group has been updated. 404 (NOT FOUND) : The specified group does not exist.","title":"PUT /api/groups/{group_id}"},{"location":"api/groups-api/#delete-apigroupsgroup_id","text":"Deletes a group. Response This request does not return data. 204 (NO CONTENT) : The group has been deleted. 404 (NOT FOUND) : The specified group does not exist.","title":"DELETE /api/groups/{group_id}"},{"location":"api/groups-api/#members","text":"","title":"Members"},{"location":"api/groups-api/#post-apigroupsgroup_idmembers","text":"Adds a member to a group. Form Parameters Required Type Description member yes string The username of the member to be added Response 200 (OK) : The member has been added or was already member of the group. If the member has been added, the request does not return data. In case that the member already was in the group, the following message is returned as string: Member is already member of group 404 (NOT FOUND) : The specified group does not exist.","title":"POST /api/groups/{group_id}/members"},{"location":"api/groups-api/#delete-apigroupsgroup_idmembersmember_id","text":"Removes a member from a group Response 200 (NO CONTENT) : The member has been removed. 404 (NOT FOUND) : The specified group or member does not exist.","title":"DELETE /api/groups/{group_id}/members/{member_id}"},{"location":"api/security-api/","text":"Security API Introduction Opencast is distributing encoded media to download and streaming servers to make that media available to end users. At the same time, that media needs to be protected such that - once provided a link to the download and/or streaming representations - only authorized users are able to consume it. This is achieved by handing signed URLs to end users which are validated by the distribution servers and become invalid after a given period of time (usually 1 hour, depending on the server configuration). As a consequence, users of the External API who are presenting URLs to distributed media for playback will need to make sure that those urls are signed, otherwise the distribution servers will refuse to deliver the content and respond with a 401 NOT AUTHORIZED status. Best practices The use of signed URLs requires a set of best practices to be followed when clients interact with the External API, most notably in the area of performance and caching. Performance When consuming URLs that need to be signed before handing them to the user, client implementors may be inclinded to use the sign=true parameter for the events queries to request all URLs to be already signed. On one hand, this saves the client implementation from having to explicitly sign those URLs that users are visiting for playback. On the other hand, signing URLs introduces an overhead to performance for the pre-signing of all urls that are sent to the client, so in these cases it will be important to make sure not to transfer large lists and require presigning. Caching One obvious caveat when using pre-signed URLs is the use of cached responses. As described above, signed URLs have a maxmimum life time and therefore need to be refreshed on a regular basis so that a user's request to play back a recording won't be rejected by the distribution servers. Secure access by source IP The signing facility of the security API provides the ability to sign URLs and restrict that URL to a given IP address. Even though this greatly increases security in sense that signed URLs can only be accessed from that device, it is important to note that in many network setups, source IP addresses of network packets will undergo network address translation (NAT) with NAT replacing the original source address from private networks with a single public address, thereby diminishing the security impact of adding the source IP address immensely. URL Signing POST /api/security/sign Returns a signed URL that can be played back for the indicated period of time, while access is optionally restricted to the specified IP address. Form Parameters Required Type Description url yes string The URL to be signed valid-until no datetime The date and time until when the signed URL is valid valid-source no string The IP address from which the url can be accessed Response 200 (OK) : A JSON object containing the signed URL or an error message is returned: Field Type Description url string The signed URL valid-until datetime The date and time until when the signed URL is valid In case of an error: Field Type Description error string An error message describing the error 401 (NOT AUTHORIZED) : The caller is not authorized to have the link signed. Example { \"valid-until\": \"2018-03-19T13:08:39Z\", \"url\":\"http://localhost?policy=eyJTdGF0ZW1lbnQiOnsiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6MTUyMTQ2NDkxOTI4NH0sIlJlc291cmNlIjoiaHR0cDpcL1wvbG9jYWxob3N0In19&keyId=demoKeyOne&signature=717dd8f958a15c1cdb7e88a61417a07bb6a1e6238d9293805cc0893f798a07e8\" } Error example: { \"error\": \"Given URL cannot be signed\" }","title":"Security API"},{"location":"api/security-api/#security-api","text":"","title":"Security API"},{"location":"api/security-api/#introduction","text":"Opencast is distributing encoded media to download and streaming servers to make that media available to end users. At the same time, that media needs to be protected such that - once provided a link to the download and/or streaming representations - only authorized users are able to consume it. This is achieved by handing signed URLs to end users which are validated by the distribution servers and become invalid after a given period of time (usually 1 hour, depending on the server configuration). As a consequence, users of the External API who are presenting URLs to distributed media for playback will need to make sure that those urls are signed, otherwise the distribution servers will refuse to deliver the content and respond with a 401 NOT AUTHORIZED status.","title":"Introduction"},{"location":"api/security-api/#best-practices","text":"The use of signed URLs requires a set of best practices to be followed when clients interact with the External API, most notably in the area of performance and caching.","title":"Best practices"},{"location":"api/security-api/#performance","text":"When consuming URLs that need to be signed before handing them to the user, client implementors may be inclinded to use the sign=true parameter for the events queries to request all URLs to be already signed. On one hand, this saves the client implementation from having to explicitly sign those URLs that users are visiting for playback. On the other hand, signing URLs introduces an overhead to performance for the pre-signing of all urls that are sent to the client, so in these cases it will be important to make sure not to transfer large lists and require presigning.","title":"Performance"},{"location":"api/security-api/#caching","text":"One obvious caveat when using pre-signed URLs is the use of cached responses. As described above, signed URLs have a maxmimum life time and therefore need to be refreshed on a regular basis so that a user's request to play back a recording won't be rejected by the distribution servers.","title":"Caching"},{"location":"api/security-api/#secure-access-by-source-ip","text":"The signing facility of the security API provides the ability to sign URLs and restrict that URL to a given IP address. Even though this greatly increases security in sense that signed URLs can only be accessed from that device, it is important to note that in many network setups, source IP addresses of network packets will undergo network address translation (NAT) with NAT replacing the original source address from private networks with a single public address, thereby diminishing the security impact of adding the source IP address immensely.","title":"Secure access by source IP"},{"location":"api/security-api/#url-signing","text":"","title":"URL Signing"},{"location":"api/security-api/#post-apisecuritysign","text":"Returns a signed URL that can be played back for the indicated period of time, while access is optionally restricted to the specified IP address. Form Parameters Required Type Description url yes string The URL to be signed valid-until no datetime The date and time until when the signed URL is valid valid-source no string The IP address from which the url can be accessed Response 200 (OK) : A JSON object containing the signed URL or an error message is returned: Field Type Description url string The signed URL valid-until datetime The date and time until when the signed URL is valid In case of an error: Field Type Description error string An error message describing the error 401 (NOT AUTHORIZED) : The caller is not authorized to have the link signed. Example { \"valid-until\": \"2018-03-19T13:08:39Z\", \"url\":\"http://localhost?policy=eyJTdGF0ZW1lbnQiOnsiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6MTUyMTQ2NDkxOTI4NH0sIlJlc291cmNlIjoiaHR0cDpcL1wvbG9jYWxob3N0In19&keyId=demoKeyOne&signature=717dd8f958a15c1cdb7e88a61417a07bb6a1e6238d9293805cc0893f798a07e8\" } Error example: { \"error\": \"Given URL cannot be signed\" }","title":"POST /api/security/sign"},{"location":"api/series-api/","text":"General GET /api/series GET /api/series/{series_id} POST /api/series DELETE /api/series/{series_id} Metadata GET /api/series/{series_id}/metadata GET /api/series/{series_id}/metadata PUT /api/series/{series_id}/metadata DELETE /api/series/{series_id}/metadata Access Policy GET /api/series/{series_id}/acl PUT /api/series/{series_id}/acl Properties GET /api/series/{series_id}/properties PUT /api/series/{series_id}/properties General GET /api/series Returns a list of series. The following query string parameters are supported to filter, sort and paginate the returned list: Query String Parameter Type Description filter string A comma-separated list of filters to limit the results with (see Filtering ). See the below table for the list of available filters sort string A comma-separated list of sort criteria (see Sorting ). See the below table for the list of available sort criteria limit integer The maximum number of results to return (see Pagination ) offset integer The index of the first result to return (see Pagination ) The following filters are available: Filter Name Description contributors Series where the contributors specified in the metadata field match. Can occur multiple times Creator Series where the creator specified in the metadata field match (please use creator for version 1.1.0 and higher instead) creationDate Series that were created between two dates. The two dates are in UTC format to the second i.e. yyyy-MM-ddTHH:mm:ssZ e.g. 2014-09-27T16:25Z. They are seperated by a forward slash (url encoded or not) so an example of the full filter would be CreationDate:2015-05-08T00:00:00.000Z/2015-05-10T00:00:00.000Z language Series based upon the language specified license Series based upon the license specified organizers Series where the organizers specified in the metadata field match. Can occur multiple times managedAcl Series who have the same managed acl name subject By the subject they are a part of. Can occur multiple times textFilter Filters series where any part of the series' metadata fields match this value title By the title of the series identifier By the technical identifiers of the series. Can occur multiple times (version 1.1.0 and higher) description By the description of the series (version 1.1.0 and higher) creator Series where the creator specified in the metadata field match (version 1.1.0 and higher) publishers Series where the publishers specified in the metadata field match. Can occur multiple times (version 1.1.0 and higher) rightsholder By the rights holder of the series (version 1.1.0 and higher) The list can be sorted by the following criteria: Sort Criteria Description contributors By the series contributors created By when the series was created creator By who created the series title By the title of the series This request additionally supports the following query string parameters to include additional information directly in the response: Query String Parameter Type Description withacl boolean Whether the acl should be included in the response (version 1.5.0 and higher) Sample request https://opencast.domain.com/api/series?filter=creator:Default Administrator&sort=title:ASC&limit=2&offset=1 Response 200 (OK) : A (potentially empty) list of series is returned as JSON array contained JSON objects describing the series: Field Type Description identifier string The unique identifier of the series created * datetime The data when the series was created creator string The name of the user that has created the series title * string The title of the series contributors * array[string] The contributors of the series publishers * array[string] The publishers of the series subjects * array[string] The subjects of the series organizers * array[string] The organizers of the series description * string The description of the series (version 1.1.0 and higher) language * string The language of the series (version 1.1.0 and higher) license * string The license of the series (version 1.1.0 and higher) rightsholder * string The rights holder of the series (version 1.1.0 and higher) * Metadata fields from the default metadata catalog dublincore/series Example [ { \"identifier\": \"dc11ab0a-fd5b-462d-a939-0a4703df38cf\", \"creator\": \"Opencast Project Administrator\", \"created\": \"2018-03-19T15:40:21Z\", \"subjects\": [ \"Mathematics\" ], \"organizers\": [ \"John Doe\", \"Prof. X\" ], \"publishers\": [ \"University of Prof. X\" ], \"contributors\": [ \"Hans Muster\", \"Maria M\u00fcller\" ], \"title\": \"Advanced Mathematics\" }, { \"identifier\": \"6a4462ca-3783-432a-81c3-962ca756dc6f\", \"creator\": \"Opencast Project Administrator\", \"created\": \"2018-03-19T15:41:20Z\", \"subjects\": [ \"Physics\", \"Mathematics\" ], \"organizers\": [ \"Dr. Who\" ], \"publishers\": [ \"University of Prof. X\", \"Doctor Who\" ], \"contributors\": [ \"Dr. Who\" ], \"title\": \"Basics of Physics\" } ] GET /api/series/{series_id} Returns a single series. Query String Parameter Type Description withacl boolean Whether the acl should be included in the response (version 1.5.0 and higher) Response 200 (OK) : The series is returned as a JSON object containing the following fields: Field Type Description identifier string The unique identifier of the series created * datetime The data when the series was created creator string The name of the user that has created the series title * string The title of the series contributors * array[string] The contributors of the series publishers * array[string] The publishers of the series subjects * array[string] The subjects of the series organizers * array[string] The organizers of the series organization * string The identifier of the tenant this series belongs to opt_out string Field is not used language * string The language of the series (version 1.1.0 and higher) license * string The license of the series (version 1.1.0 and higher) rightsholder * string The rights holder of the series (version 1.1.0 and higher) * Fields from the default metadata catalog dublincore/series 404 (NOT FOUND) : The specified series does not exist. Example { \"identifier\": \"dc11ab0a-fd5b-462d-a939-0a4703df38cf\", \"creator\": \"Opencast Project Administrator\", \"opt_out\": false, \"created\": \"2018-03-19T15:40:21Z\", \"subjects\": [ \"Mathematics\" ], \"organization\": \"mh_default_org\", \"organizers\": [ \"John Doe\", \"Prof. X\" ], \"description\": \"This is a advanced mathematics course\", \"publishers\": [ \"University of Prof. X\" ], \"contributors\": [ \"Hans Muster\", \"Maria M\u00fcller\" ], \"title\": \"Advanced Mathematics\" } POST /api/series Creates a series. Form Parameters Required Type Description metadata yes catalogs Series metadata acl no acl A collection of roles with their possible action theme no string The theme ID to be applied to the series Sample metadata: [ { \"label\": \"Opencast Series DublinCore\", \"flavor\": \"dublincore/series\", \"fields\": [ { \"id\": \"title\", \"value\": \"Captivating title\" }, { \"id\": \"subjects\", \"value\": [\"John Clark\", \"Thiago Melo Costa\"] }, { \"id\": \"description\", \"value\": \"A great description\" } ] } ] acl: [ { \"allow\": true, \"action\": \"write\", \"role\": \"ROLE_ADMIN\" }, { \"allow\": true, \"action\": \"read\", \"role\": \"ROLE_USER\" } ] theme: \"1234\" Response 201 (CREATED) : A new series is created and its identifier is returned in the Location header. 400 (BAD REQUEST) : The request is invalid or inconsistent. 401 (UNAUTHORIZED) : The user doesn't have the rights to create the series. Location: http://api.opencast.org/api/series/4fd0ef66-aea5-4b7a-a62a-a4ada0eafd6f { \"identifier\": \"4fd0ef66-aea5-4b7a-a62a-a4ada0eafd6f\" } DELETE /api/series/{series_id} Deletes a series Response 204 (NO CONTENT) : The series has been deleted. 404 (NOT FOUND) : The specified series does not exist. Metadata This section describes how to use the External API to work with metadata catalogs associated to series. Opencast manages the bibliographic metadata of series using metadata catalogs which are identified by flavors. The default metadata catalog for Opencast series has the flavor dublincore/series . Opencast additionally supports extended metadata catalogs for series that can be configured. The External API supports both the default series metadata catalog and series extended metadata catalogs. For the default series metadata catalog, the metadata is directly returned in the responses. Since the metadata catalogs can be configured, the External API provides a facility to retrieve the catalog configuration of series metadata catalogs. For more details about this mechanism, please refer to \"Metadata Catalogs\" . GET /api/series/{series_id}/metadata Returns a series' metadata of all types. Response 200 (OK) : The series' metadata are returned as catalogs 404 (NOT FOUND) : The specified series does not exist. Example [ { \"label\": \"EVENTS.EVENTS.DETAILS.CATALOG.EPISODE\", \"flavor\": \"dublincore/series\", \"fields\": [ { \"id\": \"title\", \"readOnly\": false, \"value\": \"Captivating title\", \"label\": \"EVENTS.EVENTS.DETAILS.METADATA.TITLE\", \"type\": \"text\", \"required\": true }, { \"id\": \"description\", \"readOnly\": false, \"value\": \"A great description\", \"label\": \"EVENTS.EVENTS.DETAILS.METADATA.DESCRIPTION\", \"type\": \"text_long\", \"required\": false } ] }, { \"label\": \"EVENTS.EVENTS.DETAILS.CATALOG.LICENSE\", \"flavor\": \"license/series\", \"fields\": [ { \"id\": \"license\", \"readOnly\": false, \"value\": \"CCND\", \"label\": \"EVENTS.EVENTS.DETAILS.METADATA.LICENSE\", \"collection\": { \"BSD\": \"EVENTS.LICENSE.BSD\", \"GPL3\": \"EVENTS.LICENSE.GPL\", \"CCND\": \"EVENTS.LICENSE.CCND\" }, \"type\": \"text\", \"required\": false } ] } ] GET /api/series/{series_id}/metadata Returns a series' metadata collection of the given type when the query string parameter type is specified. For each metadata catalog there is a unique property called the flavor such as dublincore/series so the type in this example would be \"dublincore/series\". Query String Parameters Type Description type flavor The type of metadata to return Response 200 (OK) : The series' metadata are returned as fields above. 404 (NOT FOUND) : The specified series does not exist. Example [ { \"id\": \"title\", \"readOnly\": false, \"value\": \"Captivating title\", \"label\": \"EVENTS.EVENTS.DETAILS.METADATA.TITLE\", \"type\": \"text\", \"required\": true }, { \"id\": \"description\", \"readOnly\": false, \"value\": \"A great description\", \"label\": \"EVENTS.EVENTS.DETAILS.METADATA.DESCRIPTION\", \"type\": \"text_long\", \"required\": false } ] PUT /api/series/{series_id}/metadata Update a series' metadata of the given type. Query String Parameters Required Type Description type yes flavor The type of metadata to update Form Parameters Required Type Description metadata yes values Series metadata as Form param Note that metadata fields not contained in the argument won't be updated. Example metadata: [ { \"id\": \"title\", \"value\": \"Captivating title - edited\" }, { \"id\": \"creator\", \"value\": [\"John Clark\", \"Thiago Melo Costa\"] }, { \"id\": \"description\", \"value\": \"A great description - edited\" } ] Response 200 (OK) : The series' metadata have been updated. 400 (BAD REQUEST) : The request is invalid or inconsistent. 404 (NOT FOUND) : The specified series does not exist. Returns: The full metadata catalog of the series DELETE /api/series/{series_id}/metadata Deletes a series' metadata catalog of the given type. All fields and values of that catalog will be deleted. Query String Parameters Required Type Description type yes flavor The type of metadata to delete Note that the default metadata catalog (type dublincore/series) cannot be deleted. Response 204 (NO CONTENT) : The metadata have been deleted. 403 (FORBIDDEN) : The main metadata catalog dublincore/series cannot be deleted as it has mandatory fields. 404 (NOT FOUND) : The specified series does not exist. Access Policy GET /api/series/{series_id}/acl Returns a series' access policy. Response 200 (OK) : The series' access policy of type acl is returned. 404 (NOT FOUND) : The specified series does not exist. Example [ { \"allow\": true, \"action\": \"write\", \"role\": \"ROLE_ADMIN\" }, { \"allow\": true, \"action\": \"read\", \"role\": \"ROLE_USER\" } ] PUT /api/series/{series_id}/acl Updates a series' access policy. Parameters Required Type Description Default Version acl yes acl Access policy to be applied override no boolean Whether the episode ACL of all events of this series should be removed false 1.2.0 Note that the existing access policy of the series will be overwritten. Response 200 (OK) : The updated access control list of type acl is returned. 404 (NOT FOUND) : The specified series does not exist. Example acl: [ { \"allow\": true, \"action\": \"write\", \"role\": \"ROLE_ADMIN\" }, { \"allow\": true, \"action\": \"read\", \"role\": \"ROLE_USER\" } ] returns [ { \"allow\": true, \"action\": \"write\", \"role\": \"ROLE_ADMIN\" }, { \"allow\": true, \"action\": \"read\", \"role\": \"ROLE_USER\" } ] Properties Properties can be assigned to series in means of key-value pairs. Both the property name (key) and property value are of type string . GET /api/series/{series_id}/properties Returns a series' properties. Response 200 (OK) : The series' properties are returned as property 404 (NOT FOUND) : The specified series does not exist. Example { \"ondemand\": \"true\", \"live\": \"false\" } PUT /api/series/{series_id}/properties Add or update properties of a series. Form parameters Required Type Description properties yes property List of properties to be assigned to the series The request can be used to add new properties and/or update existing properties. Properties not included in the request are not affected. Response 200 (OK) : The added/updated series' properties are returned as JSON object. 404 (NOT FOUND) : The specified series does not exist. Example Assume that the series already has the properties theme = 1000 and live = true . To add the property ondemand and update the value of the existing property live the following form parameter is used: properties: { \"ondemand\": \"true\", \"live\": \"false\" } The response of the request will contain the properties added/updated by this request: { \"ondemand\": \"true\", \"live\": \"false\" } After this, the properties of the series are: { \"theme\": \"1000\", \"ondemand\": \"true\", \"live\": \"false\" }","title":"Series API"},{"location":"api/series-api/#general","text":"","title":"General"},{"location":"api/series-api/#get-apiseries","text":"Returns a list of series. The following query string parameters are supported to filter, sort and paginate the returned list: Query String Parameter Type Description filter string A comma-separated list of filters to limit the results with (see Filtering ). See the below table for the list of available filters sort string A comma-separated list of sort criteria (see Sorting ). See the below table for the list of available sort criteria limit integer The maximum number of results to return (see Pagination ) offset integer The index of the first result to return (see Pagination ) The following filters are available: Filter Name Description contributors Series where the contributors specified in the metadata field match. Can occur multiple times Creator Series where the creator specified in the metadata field match (please use creator for version 1.1.0 and higher instead) creationDate Series that were created between two dates. The two dates are in UTC format to the second i.e. yyyy-MM-ddTHH:mm:ssZ e.g. 2014-09-27T16:25Z. They are seperated by a forward slash (url encoded or not) so an example of the full filter would be CreationDate:2015-05-08T00:00:00.000Z/2015-05-10T00:00:00.000Z language Series based upon the language specified license Series based upon the license specified organizers Series where the organizers specified in the metadata field match. Can occur multiple times managedAcl Series who have the same managed acl name subject By the subject they are a part of. Can occur multiple times textFilter Filters series where any part of the series' metadata fields match this value title By the title of the series identifier By the technical identifiers of the series. Can occur multiple times (version 1.1.0 and higher) description By the description of the series (version 1.1.0 and higher) creator Series where the creator specified in the metadata field match (version 1.1.0 and higher) publishers Series where the publishers specified in the metadata field match. Can occur multiple times (version 1.1.0 and higher) rightsholder By the rights holder of the series (version 1.1.0 and higher) The list can be sorted by the following criteria: Sort Criteria Description contributors By the series contributors created By when the series was created creator By who created the series title By the title of the series This request additionally supports the following query string parameters to include additional information directly in the response: Query String Parameter Type Description withacl boolean Whether the acl should be included in the response (version 1.5.0 and higher) Sample request https://opencast.domain.com/api/series?filter=creator:Default Administrator&sort=title:ASC&limit=2&offset=1 Response 200 (OK) : A (potentially empty) list of series is returned as JSON array contained JSON objects describing the series: Field Type Description identifier string The unique identifier of the series created * datetime The data when the series was created creator string The name of the user that has created the series title * string The title of the series contributors * array[string] The contributors of the series publishers * array[string] The publishers of the series subjects * array[string] The subjects of the series organizers * array[string] The organizers of the series description * string The description of the series (version 1.1.0 and higher) language * string The language of the series (version 1.1.0 and higher) license * string The license of the series (version 1.1.0 and higher) rightsholder * string The rights holder of the series (version 1.1.0 and higher) * Metadata fields from the default metadata catalog dublincore/series Example [ { \"identifier\": \"dc11ab0a-fd5b-462d-a939-0a4703df38cf\", \"creator\": \"Opencast Project Administrator\", \"created\": \"2018-03-19T15:40:21Z\", \"subjects\": [ \"Mathematics\" ], \"organizers\": [ \"John Doe\", \"Prof. X\" ], \"publishers\": [ \"University of Prof. X\" ], \"contributors\": [ \"Hans Muster\", \"Maria M\u00fcller\" ], \"title\": \"Advanced Mathematics\" }, { \"identifier\": \"6a4462ca-3783-432a-81c3-962ca756dc6f\", \"creator\": \"Opencast Project Administrator\", \"created\": \"2018-03-19T15:41:20Z\", \"subjects\": [ \"Physics\", \"Mathematics\" ], \"organizers\": [ \"Dr. Who\" ], \"publishers\": [ \"University of Prof. X\", \"Doctor Who\" ], \"contributors\": [ \"Dr. Who\" ], \"title\": \"Basics of Physics\" } ]","title":"GET /api/series"},{"location":"api/series-api/#get-apiseriesseries_id","text":"Returns a single series. Query String Parameter Type Description withacl boolean Whether the acl should be included in the response (version 1.5.0 and higher) Response 200 (OK) : The series is returned as a JSON object containing the following fields: Field Type Description identifier string The unique identifier of the series created * datetime The data when the series was created creator string The name of the user that has created the series title * string The title of the series contributors * array[string] The contributors of the series publishers * array[string] The publishers of the series subjects * array[string] The subjects of the series organizers * array[string] The organizers of the series organization * string The identifier of the tenant this series belongs to opt_out string Field is not used language * string The language of the series (version 1.1.0 and higher) license * string The license of the series (version 1.1.0 and higher) rightsholder * string The rights holder of the series (version 1.1.0 and higher) * Fields from the default metadata catalog dublincore/series 404 (NOT FOUND) : The specified series does not exist. Example { \"identifier\": \"dc11ab0a-fd5b-462d-a939-0a4703df38cf\", \"creator\": \"Opencast Project Administrator\", \"opt_out\": false, \"created\": \"2018-03-19T15:40:21Z\", \"subjects\": [ \"Mathematics\" ], \"organization\": \"mh_default_org\", \"organizers\": [ \"John Doe\", \"Prof. X\" ], \"description\": \"This is a advanced mathematics course\", \"publishers\": [ \"University of Prof. X\" ], \"contributors\": [ \"Hans Muster\", \"Maria M\u00fcller\" ], \"title\": \"Advanced Mathematics\" }","title":"GET /api/series/{series_id}"},{"location":"api/series-api/#post-apiseries","text":"Creates a series. Form Parameters Required Type Description metadata yes catalogs Series metadata acl no acl A collection of roles with their possible action theme no string The theme ID to be applied to the series Sample metadata: [ { \"label\": \"Opencast Series DublinCore\", \"flavor\": \"dublincore/series\", \"fields\": [ { \"id\": \"title\", \"value\": \"Captivating title\" }, { \"id\": \"subjects\", \"value\": [\"John Clark\", \"Thiago Melo Costa\"] }, { \"id\": \"description\", \"value\": \"A great description\" } ] } ] acl: [ { \"allow\": true, \"action\": \"write\", \"role\": \"ROLE_ADMIN\" }, { \"allow\": true, \"action\": \"read\", \"role\": \"ROLE_USER\" } ] theme: \"1234\" Response 201 (CREATED) : A new series is created and its identifier is returned in the Location header. 400 (BAD REQUEST) : The request is invalid or inconsistent. 401 (UNAUTHORIZED) : The user doesn't have the rights to create the series. Location: http://api.opencast.org/api/series/4fd0ef66-aea5-4b7a-a62a-a4ada0eafd6f { \"identifier\": \"4fd0ef66-aea5-4b7a-a62a-a4ada0eafd6f\" }","title":"POST /api/series"},{"location":"api/series-api/#delete-apiseriesseries_id","text":"Deletes a series Response 204 (NO CONTENT) : The series has been deleted. 404 (NOT FOUND) : The specified series does not exist.","title":"DELETE /api/series/{series_id}"},{"location":"api/series-api/#metadata","text":"This section describes how to use the External API to work with metadata catalogs associated to series. Opencast manages the bibliographic metadata of series using metadata catalogs which are identified by flavors. The default metadata catalog for Opencast series has the flavor dublincore/series . Opencast additionally supports extended metadata catalogs for series that can be configured. The External API supports both the default series metadata catalog and series extended metadata catalogs. For the default series metadata catalog, the metadata is directly returned in the responses. Since the metadata catalogs can be configured, the External API provides a facility to retrieve the catalog configuration of series metadata catalogs. For more details about this mechanism, please refer to \"Metadata Catalogs\" .","title":"Metadata"},{"location":"api/series-api/#get-apiseriesseries_idmetadata","text":"Returns a series' metadata of all types. Response 200 (OK) : The series' metadata are returned as catalogs 404 (NOT FOUND) : The specified series does not exist. Example [ { \"label\": \"EVENTS.EVENTS.DETAILS.CATALOG.EPISODE\", \"flavor\": \"dublincore/series\", \"fields\": [ { \"id\": \"title\", \"readOnly\": false, \"value\": \"Captivating title\", \"label\": \"EVENTS.EVENTS.DETAILS.METADATA.TITLE\", \"type\": \"text\", \"required\": true }, { \"id\": \"description\", \"readOnly\": false, \"value\": \"A great description\", \"label\": \"EVENTS.EVENTS.DETAILS.METADATA.DESCRIPTION\", \"type\": \"text_long\", \"required\": false } ] }, { \"label\": \"EVENTS.EVENTS.DETAILS.CATALOG.LICENSE\", \"flavor\": \"license/series\", \"fields\": [ { \"id\": \"license\", \"readOnly\": false, \"value\": \"CCND\", \"label\": \"EVENTS.EVENTS.DETAILS.METADATA.LICENSE\", \"collection\": { \"BSD\": \"EVENTS.LICENSE.BSD\", \"GPL3\": \"EVENTS.LICENSE.GPL\", \"CCND\": \"EVENTS.LICENSE.CCND\" }, \"type\": \"text\", \"required\": false } ] } ]","title":"GET /api/series/{series_id}/metadata"},{"location":"api/series-api/#get-apiseriesseries_idmetadata_1","text":"Returns a series' metadata collection of the given type when the query string parameter type is specified. For each metadata catalog there is a unique property called the flavor such as dublincore/series so the type in this example would be \"dublincore/series\". Query String Parameters Type Description type flavor The type of metadata to return Response 200 (OK) : The series' metadata are returned as fields above. 404 (NOT FOUND) : The specified series does not exist. Example [ { \"id\": \"title\", \"readOnly\": false, \"value\": \"Captivating title\", \"label\": \"EVENTS.EVENTS.DETAILS.METADATA.TITLE\", \"type\": \"text\", \"required\": true }, { \"id\": \"description\", \"readOnly\": false, \"value\": \"A great description\", \"label\": \"EVENTS.EVENTS.DETAILS.METADATA.DESCRIPTION\", \"type\": \"text_long\", \"required\": false } ]","title":"GET /api/series/{series_id}/metadata"},{"location":"api/series-api/#put-apiseriesseries_idmetadata","text":"Update a series' metadata of the given type. Query String Parameters Required Type Description type yes flavor The type of metadata to update Form Parameters Required Type Description metadata yes values Series metadata as Form param Note that metadata fields not contained in the argument won't be updated. Example metadata: [ { \"id\": \"title\", \"value\": \"Captivating title - edited\" }, { \"id\": \"creator\", \"value\": [\"John Clark\", \"Thiago Melo Costa\"] }, { \"id\": \"description\", \"value\": \"A great description - edited\" } ] Response 200 (OK) : The series' metadata have been updated. 400 (BAD REQUEST) : The request is invalid or inconsistent. 404 (NOT FOUND) : The specified series does not exist. Returns: The full metadata catalog of the series","title":"PUT /api/series/{series_id}/metadata"},{"location":"api/series-api/#delete-apiseriesseries_idmetadata","text":"Deletes a series' metadata catalog of the given type. All fields and values of that catalog will be deleted. Query String Parameters Required Type Description type yes flavor The type of metadata to delete Note that the default metadata catalog (type dublincore/series) cannot be deleted. Response 204 (NO CONTENT) : The metadata have been deleted. 403 (FORBIDDEN) : The main metadata catalog dublincore/series cannot be deleted as it has mandatory fields. 404 (NOT FOUND) : The specified series does not exist.","title":"DELETE /api/series/{series_id}/metadata"},{"location":"api/series-api/#access-policy","text":"","title":"Access Policy"},{"location":"api/series-api/#get-apiseriesseries_idacl","text":"Returns a series' access policy. Response 200 (OK) : The series' access policy of type acl is returned. 404 (NOT FOUND) : The specified series does not exist. Example [ { \"allow\": true, \"action\": \"write\", \"role\": \"ROLE_ADMIN\" }, { \"allow\": true, \"action\": \"read\", \"role\": \"ROLE_USER\" } ]","title":"GET /api/series/{series_id}/acl"},{"location":"api/series-api/#put-apiseriesseries_idacl","text":"Updates a series' access policy. Parameters Required Type Description Default Version acl yes acl Access policy to be applied override no boolean Whether the episode ACL of all events of this series should be removed false 1.2.0 Note that the existing access policy of the series will be overwritten. Response 200 (OK) : The updated access control list of type acl is returned. 404 (NOT FOUND) : The specified series does not exist. Example acl: [ { \"allow\": true, \"action\": \"write\", \"role\": \"ROLE_ADMIN\" }, { \"allow\": true, \"action\": \"read\", \"role\": \"ROLE_USER\" } ] returns [ { \"allow\": true, \"action\": \"write\", \"role\": \"ROLE_ADMIN\" }, { \"allow\": true, \"action\": \"read\", \"role\": \"ROLE_USER\" } ]","title":"PUT /api/series/{series_id}/acl"},{"location":"api/series-api/#properties","text":"Properties can be assigned to series in means of key-value pairs. Both the property name (key) and property value are of type string .","title":"Properties"},{"location":"api/series-api/#get-apiseriesseries_idproperties","text":"Returns a series' properties. Response 200 (OK) : The series' properties are returned as property 404 (NOT FOUND) : The specified series does not exist. Example { \"ondemand\": \"true\", \"live\": \"false\" }","title":"GET /api/series/{series_id}/properties"},{"location":"api/series-api/#put-apiseriesseries_idproperties","text":"Add or update properties of a series. Form parameters Required Type Description properties yes property List of properties to be assigned to the series The request can be used to add new properties and/or update existing properties. Properties not included in the request are not affected. Response 200 (OK) : The added/updated series' properties are returned as JSON object. 404 (NOT FOUND) : The specified series does not exist. Example Assume that the series already has the properties theme = 1000 and live = true . To add the property ondemand and update the value of the existing property live the following form parameter is used: properties: { \"ondemand\": \"true\", \"live\": \"false\" } The response of the request will contain the properties added/updated by this request: { \"ondemand\": \"true\", \"live\": \"false\" } After this, the properties of the series are: { \"theme\": \"1000\", \"ondemand\": \"true\", \"live\": \"false\" }","title":"PUT /api/series/{series_id}/properties"},{"location":"api/statistics-api/","text":"General GET /api/statistics/providers GET /api/statistics/providers/{providerId} POST /api/statistics/data/query Time Series Statistics Provider POST /api/statistics/data/export.csv General The Statistics API is available since API version 1.3.0. GET /api/statistics/providers Returns a list of statistics providers. The following query string parameters are supported to filter the returned list: Query String Parameter Type Description filter string A comma-separated list of filters to limit the results with (see Filtering ). See the below table for the list of available filters Filter Name Description resourceType Filter statistics provider by resource type (either episode , series or organization ) This request additionally supports the following query string parameters to include additional information directly in the response: Query String Parameter Type Description withparameters boolean Whether support parameters should be included in the response Sample request https://opencast.domain.com/api/statistics/providers?filter=resourceType:episode Response 200 (OK) : A (potentially empty) list of providers is returned as a JSON array containing JSON objects describing the series: Field Type Description identifier string The unique identifier of the provider title string The title of the provider description string The description of the provider type * string The type of the provider resourceType string The resource type of the provider parameters array[parameter] Supported query parameters (optional) * Currently, only the timeseries type is supported Example [ { \"identifier\": \"influxdb-episode-views-provider\", \"title\": \"Episode Views\", \"description\": \"Episode Views, Lorem Ipsum\", \"type\": \"timeSeries\", \"resourceType\": \"episode\" } ] GET /api/statistics/providers/{providerId} Returns a statistics provider. This request additionally supports the following query string parameters to include additional information directly in the response: Query String Parameter Type Description withparameters boolean Whether support parameters should be included in the response Sample request https://opencast.domain.com/api/statistics/providers/a-timeseries-provider?withparameters=true Response 200 (OK) : A (potentially empty) list of providers is returned as a JSON array containing JSON objects describing the series: Field Type Description identifier string The unique identifier of the provider title string The title of the provider description string The description of the provider type * string The type of the provider resourceType string The resource type of the provider parameters array[parameter] Supported query parameters (optional) * Currently, only the timeSeries type is supported Example { \"identifier\": \"a-timeseries-provider\", \"title\": \"Episode Views\", \"description\": \"Episode Views, Lorem Ipsum\", \"type\": \"timeseries\", \"resourceType\": \"episode\", \"parameters\": [ { \"name\": \"resourceId\", \"optional\": false, \"type\": \"string\" }, { \"name\": \"from\", \"optional\": false, \"type\": \"datetime\" }, { \"name\": \"to\", \"optional\": false, \"type\": \"datetime\" }, { \"values\": [ \"daily\", \"weekly\", \"monthly\", \"yearly\" ], \"name\": \"dataResolution\", \"optional\": false, \"type\": \"enumeration\" } ] } POST /api/statistics/data/query Retrieves statistical data from one or more providers Form Parameters Required Type Description data yes array[object] A JSON array describing the statistics queries to request (see below) The JSON array consists of query JSON objects. A query JSON object contains information about a statistics query to be executed: Field Required Type Description provider yes property A JSON object with information about the statistics provider to be queried parameters yes property A JSON object containing the parameters The JSON object provider has the following fields: Field Required Type Description identifier yes string A JSON object with information about the statistics provider to be queried The format of the JSON object parameters depends on the provider type that is queried, and is described separately for each provider in the next section. Example [ { \"provider\": { \"identifier\": \"a-statistics-provider\" }, \"parameters\": { \"resourceId\": \"93213324-5d29-428d-bbfd-369a2bae6700\" } }, { \"provider\": { \"identifier\": \"a-timeseries-provider\" }, \"parameters\": { \"resourceId\": \"23413432-5a15-328e-aafe-562a2bae6800\", \"from\": \"2019-04-10T13:45:32Z\", \"to\": \"2019-04-12T00:00:00Z\", \"dataResolution\": \"daily\" } } ] Response 200 (OK) : A (potentially empty) list of query results is returned as a JSON array containing JSON objects 400 (BAD REQUEST) : The request was not valid Field name Type Description provider property A JSON object describing the statistics provider as described below parameters property A JSON object describing the query parameters data property A JSON object containing the query result Here, a statistics provider JSON object has the following fields: Field Type Description identifier string The unique identifier of the provider type * string The type of the provider resourceType string The resource type of the provider Note that the format of data is implied by the type of the statistics provider. Example [ { \"provider\": { \"identifier\": \"a-statistics-provider\", \"type\": \"someType\", \"resourceType\": \"episode\", }, \"parameters\": { \"resourceId\": \"93213324-5d29-428d-bbfd-369a2bae6700\" }, \"data\": { \"value\": \"1\" }, { \"provider\": { \"identifier\": \"a-timeseries-provider\", \"type\": \"timeseries\", \"resourceType\": \"episode\", }, \"parameters\": { \"resourceId\": \"23413432-5a15-328e-aafe-562a2bae6800\", \"from\": \"2019-04-10T13:45:32Z\", \"to\": \"2019-04-12T00:00:00Z\", \"dataResolution\": \"daily\" }, \"data\": { \"labels\": [\"2019-04-10T13:45:32Z\", \"2019-04-11T00:00:00Z\", \"2019-04-12T00:00:00Z\"], \"values\": [20, 100, 300], \"total\": 420 } ] Time Series Statistics Provider Time Series Statistics Providers (type = timeseries) support some well-defined parameters and deliver a well-defined data format. Parameters: Field name Type Description resourceId string The technical identifier of the resource the data relates to from datetime Start of time period this query relates to to datetime End of time period this query relates to dataResolution string hourly , daily , monthly or yearly (as described by provider) Query Result Data Field: Field name Type Description labels array[datetime] The dates of the measurement points values array[integer] The values of the measurement points total integer The sum of all values POST /api/statistics/data/export.csv Retrieves statistical data in csv format. Form Parameters Required Type Description data yes array[object] A JSON object describing the statistics query to request (see below) filter no string A comma-separated list of filters to limit the results with (see Filtering ). All standard dublin core meta data fields are filterable. limit no integer The maximum number of resources to return (see Pagination ) offset no integer The index of the first resource to return (see Pagination ) Note that limit and offset relate to the resource here, not CSV lines. There can be multiple lines in a CSV for a resource, e.g. an event. However, you cannot limit by lines, but only by e.g. events. A query JSON object contains information about a statistics query to be executed: Field Required Type Description provider yes property A JSON object with information about the statistics provider to be queried parameters yes property A JSON object containing the parameters Here, a statistics provider JSON object has the following fields: Field Type Description identifier string The unique identifier of the provider resourceType string The resource type of the provider There parameters are the same as described above , but with one additional field: Field name Type Description detailLevel string EPISODE , SERIES , or ORGANIZATION (only available for CSV exports) Example data: { \"parameters\": { \"resourceId\": \"mh_default_org\", \"detailLevel\": \"EPISODE\", \"from\": \"2018-12-31T23:00:00.000Z\", \"to\": \"2019-12-31T22:59:59.999Z\", \"dataResolution\": \"YEARLY\" }, \"provider\": { \"identifier\": \"organization.views.sum.influx\", \"resourceType\": \"organization\" } } filter: presenters:Hans Dampf Response 200 (OK) : A (potentially empty) CSV file containing the resource statistics with all available meta data 400 (BAD REQUEST) : The request was not valid","title":"Statistics API"},{"location":"api/statistics-api/#general","text":"The Statistics API is available since API version 1.3.0.","title":"General"},{"location":"api/statistics-api/#get-apistatisticsproviders","text":"Returns a list of statistics providers. The following query string parameters are supported to filter the returned list: Query String Parameter Type Description filter string A comma-separated list of filters to limit the results with (see Filtering ). See the below table for the list of available filters Filter Name Description resourceType Filter statistics provider by resource type (either episode , series or organization ) This request additionally supports the following query string parameters to include additional information directly in the response: Query String Parameter Type Description withparameters boolean Whether support parameters should be included in the response Sample request https://opencast.domain.com/api/statistics/providers?filter=resourceType:episode Response 200 (OK) : A (potentially empty) list of providers is returned as a JSON array containing JSON objects describing the series: Field Type Description identifier string The unique identifier of the provider title string The title of the provider description string The description of the provider type * string The type of the provider resourceType string The resource type of the provider parameters array[parameter] Supported query parameters (optional) * Currently, only the timeseries type is supported Example [ { \"identifier\": \"influxdb-episode-views-provider\", \"title\": \"Episode Views\", \"description\": \"Episode Views, Lorem Ipsum\", \"type\": \"timeSeries\", \"resourceType\": \"episode\" } ]","title":"GET /api/statistics/providers"},{"location":"api/statistics-api/#get-apistatisticsprovidersproviderid","text":"Returns a statistics provider. This request additionally supports the following query string parameters to include additional information directly in the response: Query String Parameter Type Description withparameters boolean Whether support parameters should be included in the response Sample request https://opencast.domain.com/api/statistics/providers/a-timeseries-provider?withparameters=true Response 200 (OK) : A (potentially empty) list of providers is returned as a JSON array containing JSON objects describing the series: Field Type Description identifier string The unique identifier of the provider title string The title of the provider description string The description of the provider type * string The type of the provider resourceType string The resource type of the provider parameters array[parameter] Supported query parameters (optional) * Currently, only the timeSeries type is supported Example { \"identifier\": \"a-timeseries-provider\", \"title\": \"Episode Views\", \"description\": \"Episode Views, Lorem Ipsum\", \"type\": \"timeseries\", \"resourceType\": \"episode\", \"parameters\": [ { \"name\": \"resourceId\", \"optional\": false, \"type\": \"string\" }, { \"name\": \"from\", \"optional\": false, \"type\": \"datetime\" }, { \"name\": \"to\", \"optional\": false, \"type\": \"datetime\" }, { \"values\": [ \"daily\", \"weekly\", \"monthly\", \"yearly\" ], \"name\": \"dataResolution\", \"optional\": false, \"type\": \"enumeration\" } ] }","title":"GET /api/statistics/providers/{providerId}"},{"location":"api/statistics-api/#post-apistatisticsdataquery","text":"Retrieves statistical data from one or more providers Form Parameters Required Type Description data yes array[object] A JSON array describing the statistics queries to request (see below) The JSON array consists of query JSON objects. A query JSON object contains information about a statistics query to be executed: Field Required Type Description provider yes property A JSON object with information about the statistics provider to be queried parameters yes property A JSON object containing the parameters The JSON object provider has the following fields: Field Required Type Description identifier yes string A JSON object with information about the statistics provider to be queried The format of the JSON object parameters depends on the provider type that is queried, and is described separately for each provider in the next section. Example [ { \"provider\": { \"identifier\": \"a-statistics-provider\" }, \"parameters\": { \"resourceId\": \"93213324-5d29-428d-bbfd-369a2bae6700\" } }, { \"provider\": { \"identifier\": \"a-timeseries-provider\" }, \"parameters\": { \"resourceId\": \"23413432-5a15-328e-aafe-562a2bae6800\", \"from\": \"2019-04-10T13:45:32Z\", \"to\": \"2019-04-12T00:00:00Z\", \"dataResolution\": \"daily\" } } ] Response 200 (OK) : A (potentially empty) list of query results is returned as a JSON array containing JSON objects 400 (BAD REQUEST) : The request was not valid Field name Type Description provider property A JSON object describing the statistics provider as described below parameters property A JSON object describing the query parameters data property A JSON object containing the query result Here, a statistics provider JSON object has the following fields: Field Type Description identifier string The unique identifier of the provider type * string The type of the provider resourceType string The resource type of the provider Note that the format of data is implied by the type of the statistics provider. Example [ { \"provider\": { \"identifier\": \"a-statistics-provider\", \"type\": \"someType\", \"resourceType\": \"episode\", }, \"parameters\": { \"resourceId\": \"93213324-5d29-428d-bbfd-369a2bae6700\" }, \"data\": { \"value\": \"1\" }, { \"provider\": { \"identifier\": \"a-timeseries-provider\", \"type\": \"timeseries\", \"resourceType\": \"episode\", }, \"parameters\": { \"resourceId\": \"23413432-5a15-328e-aafe-562a2bae6800\", \"from\": \"2019-04-10T13:45:32Z\", \"to\": \"2019-04-12T00:00:00Z\", \"dataResolution\": \"daily\" }, \"data\": { \"labels\": [\"2019-04-10T13:45:32Z\", \"2019-04-11T00:00:00Z\", \"2019-04-12T00:00:00Z\"], \"values\": [20, 100, 300], \"total\": 420 } ]","title":"POST /api/statistics/data/query"},{"location":"api/statistics-api/#time-series-statistics-provider","text":"Time Series Statistics Providers (type = timeseries) support some well-defined parameters and deliver a well-defined data format. Parameters: Field name Type Description resourceId string The technical identifier of the resource the data relates to from datetime Start of time period this query relates to to datetime End of time period this query relates to dataResolution string hourly , daily , monthly or yearly (as described by provider) Query Result Data Field: Field name Type Description labels array[datetime] The dates of the measurement points values array[integer] The values of the measurement points total integer The sum of all values","title":"Time Series Statistics Provider"},{"location":"api/statistics-api/#post-apistatisticsdataexportcsv","text":"Retrieves statistical data in csv format. Form Parameters Required Type Description data yes array[object] A JSON object describing the statistics query to request (see below) filter no string A comma-separated list of filters to limit the results with (see Filtering ). All standard dublin core meta data fields are filterable. limit no integer The maximum number of resources to return (see Pagination ) offset no integer The index of the first resource to return (see Pagination ) Note that limit and offset relate to the resource here, not CSV lines. There can be multiple lines in a CSV for a resource, e.g. an event. However, you cannot limit by lines, but only by e.g. events. A query JSON object contains information about a statistics query to be executed: Field Required Type Description provider yes property A JSON object with information about the statistics provider to be queried parameters yes property A JSON object containing the parameters Here, a statistics provider JSON object has the following fields: Field Type Description identifier string The unique identifier of the provider resourceType string The resource type of the provider There parameters are the same as described above , but with one additional field: Field name Type Description detailLevel string EPISODE , SERIES , or ORGANIZATION (only available for CSV exports) Example data: { \"parameters\": { \"resourceId\": \"mh_default_org\", \"detailLevel\": \"EPISODE\", \"from\": \"2018-12-31T23:00:00.000Z\", \"to\": \"2019-12-31T22:59:59.999Z\", \"dataResolution\": \"YEARLY\" }, \"provider\": { \"identifier\": \"organization.views.sum.influx\", \"resourceType\": \"organization\" } } filter: presenters:Hans Dampf Response 200 (OK) : A (potentially empty) CSV file containing the resource statistics with all available meta data 400 (BAD REQUEST) : The request was not valid","title":"POST /api/statistics/data/export.csv"},{"location":"api/types/","text":"Data Types This page defines data types that are commonly used by many External API requests. The page is divided in several sections that start with general information about how the types are supposed to work. Data Types Basic Extended array object file flavor property Date and Time Recurrence Rule Metadata Catalogs fields values catalog catalogs Access Control Lists ace acl Workflow workflow_retry_strategy workflow_state workflow_operation_state operation_definition operation_instance Statistics parameters Basic This section defines basic data types used in the External API specification. Type Description string An UTF-8 encoded string boolean true or false integer An integer number, i.e. [-][0-9]+[0-9]* Extended This section describes extended data types used in the External API specification. array The External API makes use of JSON arrays often. We indicate the element type in the brackets, e.g. array[string] . The empty array [] is allowed. Examples [\"String1\", \"String2\"] [] object The External API makes use of JSON objects. The empty object {} might be allowed. Examples { \"myObject1\": { \"key\": \"value\"; }, \"myObject2\": { \"array\": [1, 2, 3] }, \"valid\": false } {} file This data type indicates that a file is needed as parameter. flavor Opencast uses flavors to locate tracks, attachments and catalogs associated to events. Flavors have a type and a subtype and are written in the form type + \"/\" + subtype . flavor ::= type + \"/\" + subtype whereas both type and subtype consist of ([a-z][A-Z][1-9])+([a-z][A-Z][1-9][+-])* Example dublincore/episode property Opencast often uses sets of key-value pairs to associate properties to objects. The External API uses JSON objects to represent those properties. Both the name of the JSON object field and its value are of type string . Example { \"key\": \"value\", \"live\": \"true\" } Date and Time Type Encoding Description date ISO 8601 Date datetime ISO 8601 Date and Time Examples date : 2018-03-11 datetime : 2018-03-11T13:23:51Z Recurrence Rule To define a set of recurring events within a given time period, Opencast uses recurrence rules. For more details about reccurrence rules, please refer to the Internet Calendaring and Scheduling Core Object Specification (iCalendar) . Example \"rrule\":\"FREQ=WEEKLY;BYDAY=MO,TU,WE,TH,FR;BYHOUR=16;BYMINUTE=0\" Please note that BYHOUR is specified in UTC. Metadata Catalogs The External API is designed to take full advantage of the powerful metadata facilities of Opencast. Opencast distinguishes between bibliographic metadata and technical metadata: Bibliographic metadata is supposed to describe the associated objects and to be used to present those objects to endusers (e.g. in a video portal). Technical metadata is used by Opencast to manage objects (e.g. permissions, processing, scheduling) For events and series, Opencast manages bibliographic metadata in metadata catalogs. There are two kind of metadata catalogs: The default metadata catalogs for events ( dublincore/episode ) and series ( dublincore/series ) An arbitrary number of configurable extended metadata catalogs can be configured for both events and series While the extended metadata can be fully configured, the default metadata catalogs are supposed to hold a minimum set of defined metadata fields. As the metadata catalogs are configurable, the External API provides means of gathering the metadata catalog configuration. This is done by \"/api/events/{event_id}/metadata\" and \"/api/series/{series_id}/metdata\" . Those requests return self-describing metadata catalogs that do not just contain the values of all metadata fields but also a list of available metadata fields and their configuration. Note that the Opencast configuration defines which metadata catalogs are available for events and series. The following sections define data types that are used to manage metadata catalogs. fields Each metadata catalogs has a list of metadata fields that is described as array of JSON objects with the following fields: Field Optional Type Description id no string Technical identifier of the metadata field value no string Current value of the metadata field label no string Displayable name of the metadata field (i18n) type no string Type of the metadata field readOnly no boolean Whether this metadata field is read-only required no boolean Whether this metadata field is mandatory collection yes string Pre-defined list of values as JSON object translatable yes boolean Whether the field value supports i18n Example [ { \"readOnly\": false, \"id\": \"title\", \"label\": \"EVENTS.EVENTS.DETAILS.METADATA.TITLE\", \"type\": \"text\", \"value\": \"Test Event Title\", \"required\": true }, { \"translatable\": true, \"readOnly\": false, \"id\": \"language\", \"label\": \"EVENTS.EVENTS.DETAILS.METADATA.LANGUAGE\", \"type\": \"text\", \"value\": \"\", \"required\": false }, { \"translatable\": true, \"readOnly\": false, \"id\": \"license\", \"label\": \"EVENTS.EVENTS.DETAILS.METADATA.LICENSE\", \"type\": \"text\", \"value\": \"\", \"required\": false }, [...] ] values To modifiy values of metadata catalogs, a JSON array with JSON objects contained the following fields is used: Field Required Description id yes The technical identifier of the metadata field value yes The value of the metadata field Notes: Fields which are not included in catalog_values will not be updated Attempting to write readonly fields will result in error Attempting to write empty values to a required field will result in error [ { \"id\": \"title\", \"value\": \"Captivating title - edited\" }, { \"id\": \"creator\", \"value\": [\"John Clark\", \"Thiago Melo Costa\"] }, { \"id\": \"description\", \"value\": \"A great description - edited\" } ] catalog Besides the metadata configuration, the full metadata catalog configuration includes some additional fields describing the catalog itself: Field Type Description label string Displayable name of the metadata catalog flavor string The flavor of the metadata catalog fields fields An array of JSON objects describing the metadata field configurations of the metadata catalogs Example { \"flavor\": \"dublincore/episode\", \"title\": \"EVENTS.EVENTS.DETAILS.CATALOG.EPISODE\", \"fields\": [ { \"readOnly\": false, \"id\": \"title\", \"label\": \"EVENTS.EVENTS.DETAILS.METADATA.TITLE\", \"type\": \"text\", \"value\": \"Test 1\", \"required\": true }, [...] ] } ] catalogs The metadata configuration including all metadata catalogs of a given objects is returned as JSON array whereas its elements are of type catalog . Example [ { \"flavor\": \"dublincore/episode\", \"title\": \"EVENTS.EVENTS.DETAILS.CATALOG.EPISODE\", \"fields\": [ { \"readOnly\": false, \"id\": \"title\", \"label\": \"EVENTS.EVENTS.DETAILS.METADATA.TITLE\", \"type\": \"text\", \"value\": \"Test 1\", \"required\": true }, { \"readOnly\": false, \"id\": \"subjects\", \"label\": \"EVENTS.EVENTS.DETAILS.METADATA.SUBJECT\", \"type\": \"text\", \"value\": [], \"required\": false }, { \"readOnly\": false, \"id\": \"description\", \"label\": \"EVENTS.EVENTS.DETAILS.METADATA.DESCRIPTION\", \"type\": \"text_long\", \"value\": \"\", \"required\": false }, [...] ] } ] Access Control Lists Opencast uses access control lists (ACL) to manage permissions of objects. Each access control list is associated to exactly one object and consists of a list of access control entries (ACE). The access control entries are a list of triples < role , action , allow > which read like \"Users with role role are allowed to perform action action on the associate object if allow is true\". Opencast defines the following ACL actions: Action Description read Read access write Write access Depending on the configuration of Opencast, there can be additional ACL actions. ace The access control entries are represented as JSON objects with the following fields: ACE field Required Type Description role yes string The role this ACE affects action yes string The actions this ACE affects allow yes boolean Whether role is allowed to perform the action Example { \"allow\": true, \"action\": \"write\", \"role\": \"ROLE_ADMIN\" } acl The access control lists are represented as JSON arrays with element type ace . Example [ { \"allow\": true, \"action\": \"write\", \"role\": \"ROLE_ADMIN\" }, { \"allow\": true, \"action\": \"read\", \"role\": \"ROLE_USER\" } ] Workflow workflow_retry_strategy The retry strategy of a workflow operation definition in case of an error. The following values are possible: Value Description none no retry retry restart the operation hold keep the operation in hold state workflow_state The state the workflow instance is currently in. The following values are possible: Value Description instantiated The workflow instance was instantiated, but is not yet running running The workflow instance is running stopped The workflow instance was stopped paused The workflow instance was paused succeeded The workflow instance has succeeded failed The workflow instance has failed failing The workflow instance has failed and is currently running the error handling workflow workflow_operation_state The state the workflow operation instance is currently in. The following values are possible: Value Description instantiated The workflow operation instance was instantiated, but is not yet running running The workflow operation instance is running paused The workflow operation instance was paused succeeded The workflow operation instance has succeeded failed The workflow operation instance has failed skipped The workflow operation instance was skipped retry The workflow operation instance has failed and a retry is currently running operation_definition The workflow operation_definition entries are represented as JSON objects with the following fields: Field Type Description operation string The operation of this workflow operation definition description string The description of this workflow operation definition configuration property The configuration for this workflow operation definition if string The condition of this workflow operation definition that is required to be true for this operation to be executed unless string The condition of this workflow operation definition that is required to be false for this operation to be executed fail_workflow_on_error boolean Whether an error during this workflow operation definition fails the workflow error_handler_workflow string The identifier of the exception handler workflow of this workflow operation definition retry_strategy workflow_retry_strategy The retry strategy of this workflow operation definition in case of an error max_attempts integer The number of attempts made to execute this workflow operation definition operation_instance The workflow operation_instance entries are represented as JSON objects with the following fields: Field Type Description identifier integer The unique identifier of this workflow operation instance (equals the job identifier) operation string The operation of this workflow operation instance description string The description of this workflow operation instance configuration property The configuration for this workflow operation instance state workflow_operation_state The state of this workflow operation instance start datetime The start date and time of this workflow operation instance completion datetime The completion date and time of this workflow operation instance time_in_queue integer The number of milliseconds this workflow operation instance waited in a service queue host string The host that executed this workflow operation instance if string The condition of this workflow operation instance that is required to be true for this operation to be executed unless string The condition of this workflow operation instance that is required to be false for this operation to be executed fail_workflow_on_error boolean Whether an error during this workflow operation instance fails the workflow error_handler_workflow string The identifier of the exception handler workflow of this workflow operation instance retry_strategy workflow_retry_strategy The retry strategy of this workflow operation instance in case of an error max_attempts integer The number of attempts made to execute this workflow operation instance failed_attempts integer The number of failed attempts to execute this workflow operation instance Statistics parameters The Statistics endpoint can list the available statistics providers. Optionally, the endpoint provides information about supported data query parameters using this JSON object. Field Type Description name string The name of the parameter to be used in requests type string The type of this parameter to be used in requests optional boolean Whether the parameter must be specified in queries values array Values to be used for enumeration types (only for type enumeration ) The following types are available: Type Description string UTF-8 encoded string date ISO 8601 encoded date datetime ISO 8601 encoded date and time integer An integer number, i.e. [-][0-9]+[0-9]* boolean true or false enumeration One of the values provided by the field values","title":"Data Types"},{"location":"api/types/#data-types","text":"This page defines data types that are commonly used by many External API requests. The page is divided in several sections that start with general information about how the types are supposed to work. Data Types Basic Extended array object file flavor property Date and Time Recurrence Rule Metadata Catalogs fields values catalog catalogs Access Control Lists ace acl Workflow workflow_retry_strategy workflow_state workflow_operation_state operation_definition operation_instance Statistics parameters","title":"Data Types"},{"location":"api/types/#basic","text":"This section defines basic data types used in the External API specification. Type Description string An UTF-8 encoded string boolean true or false integer An integer number, i.e. [-][0-9]+[0-9]*","title":"Basic"},{"location":"api/types/#extended","text":"This section describes extended data types used in the External API specification.","title":"Extended"},{"location":"api/types/#array","text":"The External API makes use of JSON arrays often. We indicate the element type in the brackets, e.g. array[string] . The empty array [] is allowed. Examples [\"String1\", \"String2\"] []","title":"array"},{"location":"api/types/#object","text":"The External API makes use of JSON objects. The empty object {} might be allowed. Examples { \"myObject1\": { \"key\": \"value\"; }, \"myObject2\": { \"array\": [1, 2, 3] }, \"valid\": false } {}","title":"object"},{"location":"api/types/#file","text":"This data type indicates that a file is needed as parameter.","title":"file"},{"location":"api/types/#flavor","text":"Opencast uses flavors to locate tracks, attachments and catalogs associated to events. Flavors have a type and a subtype and are written in the form type + \"/\" + subtype . flavor ::= type + \"/\" + subtype whereas both type and subtype consist of ([a-z][A-Z][1-9])+([a-z][A-Z][1-9][+-])* Example dublincore/episode","title":"flavor"},{"location":"api/types/#property","text":"Opencast often uses sets of key-value pairs to associate properties to objects. The External API uses JSON objects to represent those properties. Both the name of the JSON object field and its value are of type string . Example { \"key\": \"value\", \"live\": \"true\" }","title":"property"},{"location":"api/types/#date-and-time","text":"Type Encoding Description date ISO 8601 Date datetime ISO 8601 Date and Time Examples date : 2018-03-11 datetime : 2018-03-11T13:23:51Z","title":"Date and Time"},{"location":"api/types/#recurrence-rule","text":"To define a set of recurring events within a given time period, Opencast uses recurrence rules. For more details about reccurrence rules, please refer to the Internet Calendaring and Scheduling Core Object Specification (iCalendar) . Example \"rrule\":\"FREQ=WEEKLY;BYDAY=MO,TU,WE,TH,FR;BYHOUR=16;BYMINUTE=0\" Please note that BYHOUR is specified in UTC.","title":"Recurrence Rule"},{"location":"api/types/#metadata-catalogs","text":"The External API is designed to take full advantage of the powerful metadata facilities of Opencast. Opencast distinguishes between bibliographic metadata and technical metadata: Bibliographic metadata is supposed to describe the associated objects and to be used to present those objects to endusers (e.g. in a video portal). Technical metadata is used by Opencast to manage objects (e.g. permissions, processing, scheduling) For events and series, Opencast manages bibliographic metadata in metadata catalogs. There are two kind of metadata catalogs: The default metadata catalogs for events ( dublincore/episode ) and series ( dublincore/series ) An arbitrary number of configurable extended metadata catalogs can be configured for both events and series While the extended metadata can be fully configured, the default metadata catalogs are supposed to hold a minimum set of defined metadata fields. As the metadata catalogs are configurable, the External API provides means of gathering the metadata catalog configuration. This is done by \"/api/events/{event_id}/metadata\" and \"/api/series/{series_id}/metdata\" . Those requests return self-describing metadata catalogs that do not just contain the values of all metadata fields but also a list of available metadata fields and their configuration. Note that the Opencast configuration defines which metadata catalogs are available for events and series. The following sections define data types that are used to manage metadata catalogs.","title":"Metadata Catalogs"},{"location":"api/types/#fields","text":"Each metadata catalogs has a list of metadata fields that is described as array of JSON objects with the following fields: Field Optional Type Description id no string Technical identifier of the metadata field value no string Current value of the metadata field label no string Displayable name of the metadata field (i18n) type no string Type of the metadata field readOnly no boolean Whether this metadata field is read-only required no boolean Whether this metadata field is mandatory collection yes string Pre-defined list of values as JSON object translatable yes boolean Whether the field value supports i18n Example [ { \"readOnly\": false, \"id\": \"title\", \"label\": \"EVENTS.EVENTS.DETAILS.METADATA.TITLE\", \"type\": \"text\", \"value\": \"Test Event Title\", \"required\": true }, { \"translatable\": true, \"readOnly\": false, \"id\": \"language\", \"label\": \"EVENTS.EVENTS.DETAILS.METADATA.LANGUAGE\", \"type\": \"text\", \"value\": \"\", \"required\": false }, { \"translatable\": true, \"readOnly\": false, \"id\": \"license\", \"label\": \"EVENTS.EVENTS.DETAILS.METADATA.LICENSE\", \"type\": \"text\", \"value\": \"\", \"required\": false }, [...] ]","title":"fields"},{"location":"api/types/#values","text":"To modifiy values of metadata catalogs, a JSON array with JSON objects contained the following fields is used: Field Required Description id yes The technical identifier of the metadata field value yes The value of the metadata field Notes: Fields which are not included in catalog_values will not be updated Attempting to write readonly fields will result in error Attempting to write empty values to a required field will result in error [ { \"id\": \"title\", \"value\": \"Captivating title - edited\" }, { \"id\": \"creator\", \"value\": [\"John Clark\", \"Thiago Melo Costa\"] }, { \"id\": \"description\", \"value\": \"A great description - edited\" } ]","title":"values"},{"location":"api/types/#catalog","text":"Besides the metadata configuration, the full metadata catalog configuration includes some additional fields describing the catalog itself: Field Type Description label string Displayable name of the metadata catalog flavor string The flavor of the metadata catalog fields fields An array of JSON objects describing the metadata field configurations of the metadata catalogs Example { \"flavor\": \"dublincore/episode\", \"title\": \"EVENTS.EVENTS.DETAILS.CATALOG.EPISODE\", \"fields\": [ { \"readOnly\": false, \"id\": \"title\", \"label\": \"EVENTS.EVENTS.DETAILS.METADATA.TITLE\", \"type\": \"text\", \"value\": \"Test 1\", \"required\": true }, [...] ] } ]","title":"catalog"},{"location":"api/types/#catalogs","text":"The metadata configuration including all metadata catalogs of a given objects is returned as JSON array whereas its elements are of type catalog . Example [ { \"flavor\": \"dublincore/episode\", \"title\": \"EVENTS.EVENTS.DETAILS.CATALOG.EPISODE\", \"fields\": [ { \"readOnly\": false, \"id\": \"title\", \"label\": \"EVENTS.EVENTS.DETAILS.METADATA.TITLE\", \"type\": \"text\", \"value\": \"Test 1\", \"required\": true }, { \"readOnly\": false, \"id\": \"subjects\", \"label\": \"EVENTS.EVENTS.DETAILS.METADATA.SUBJECT\", \"type\": \"text\", \"value\": [], \"required\": false }, { \"readOnly\": false, \"id\": \"description\", \"label\": \"EVENTS.EVENTS.DETAILS.METADATA.DESCRIPTION\", \"type\": \"text_long\", \"value\": \"\", \"required\": false }, [...] ] } ]","title":"catalogs"},{"location":"api/types/#access-control-lists","text":"Opencast uses access control lists (ACL) to manage permissions of objects. Each access control list is associated to exactly one object and consists of a list of access control entries (ACE). The access control entries are a list of triples < role , action , allow > which read like \"Users with role role are allowed to perform action action on the associate object if allow is true\". Opencast defines the following ACL actions: Action Description read Read access write Write access Depending on the configuration of Opencast, there can be additional ACL actions.","title":"Access Control Lists"},{"location":"api/types/#ace","text":"The access control entries are represented as JSON objects with the following fields: ACE field Required Type Description role yes string The role this ACE affects action yes string The actions this ACE affects allow yes boolean Whether role is allowed to perform the action Example { \"allow\": true, \"action\": \"write\", \"role\": \"ROLE_ADMIN\" }","title":"ace"},{"location":"api/types/#acl","text":"The access control lists are represented as JSON arrays with element type ace . Example [ { \"allow\": true, \"action\": \"write\", \"role\": \"ROLE_ADMIN\" }, { \"allow\": true, \"action\": \"read\", \"role\": \"ROLE_USER\" } ]","title":"acl"},{"location":"api/types/#workflow","text":"","title":"Workflow"},{"location":"api/types/#workflow_retry_strategy","text":"The retry strategy of a workflow operation definition in case of an error. The following values are possible: Value Description none no retry retry restart the operation hold keep the operation in hold state","title":"workflow_retry_strategy"},{"location":"api/types/#workflow_state","text":"The state the workflow instance is currently in. The following values are possible: Value Description instantiated The workflow instance was instantiated, but is not yet running running The workflow instance is running stopped The workflow instance was stopped paused The workflow instance was paused succeeded The workflow instance has succeeded failed The workflow instance has failed failing The workflow instance has failed and is currently running the error handling workflow","title":"workflow_state"},{"location":"api/types/#workflow_operation_state","text":"The state the workflow operation instance is currently in. The following values are possible: Value Description instantiated The workflow operation instance was instantiated, but is not yet running running The workflow operation instance is running paused The workflow operation instance was paused succeeded The workflow operation instance has succeeded failed The workflow operation instance has failed skipped The workflow operation instance was skipped retry The workflow operation instance has failed and a retry is currently running","title":"workflow_operation_state"},{"location":"api/types/#operation_definition","text":"The workflow operation_definition entries are represented as JSON objects with the following fields: Field Type Description operation string The operation of this workflow operation definition description string The description of this workflow operation definition configuration property The configuration for this workflow operation definition if string The condition of this workflow operation definition that is required to be true for this operation to be executed unless string The condition of this workflow operation definition that is required to be false for this operation to be executed fail_workflow_on_error boolean Whether an error during this workflow operation definition fails the workflow error_handler_workflow string The identifier of the exception handler workflow of this workflow operation definition retry_strategy workflow_retry_strategy The retry strategy of this workflow operation definition in case of an error max_attempts integer The number of attempts made to execute this workflow operation definition","title":"operation_definition"},{"location":"api/types/#operation_instance","text":"The workflow operation_instance entries are represented as JSON objects with the following fields: Field Type Description identifier integer The unique identifier of this workflow operation instance (equals the job identifier) operation string The operation of this workflow operation instance description string The description of this workflow operation instance configuration property The configuration for this workflow operation instance state workflow_operation_state The state of this workflow operation instance start datetime The start date and time of this workflow operation instance completion datetime The completion date and time of this workflow operation instance time_in_queue integer The number of milliseconds this workflow operation instance waited in a service queue host string The host that executed this workflow operation instance if string The condition of this workflow operation instance that is required to be true for this operation to be executed unless string The condition of this workflow operation instance that is required to be false for this operation to be executed fail_workflow_on_error boolean Whether an error during this workflow operation instance fails the workflow error_handler_workflow string The identifier of the exception handler workflow of this workflow operation instance retry_strategy workflow_retry_strategy The retry strategy of this workflow operation instance in case of an error max_attempts integer The number of attempts made to execute this workflow operation instance failed_attempts integer The number of failed attempts to execute this workflow operation instance","title":"operation_instance"},{"location":"api/types/#statistics","text":"","title":"Statistics"},{"location":"api/types/#parameters","text":"The Statistics endpoint can list the available statistics providers. Optionally, the endpoint provides information about supported data query parameters using this JSON object. Field Type Description name string The name of the parameter to be used in requests type string The type of this parameter to be used in requests optional boolean Whether the parameter must be specified in queries values array Values to be used for enumeration types (only for type enumeration ) The following types are available: Type Description string UTF-8 encoded string date ISO 8601 encoded date datetime ISO 8601 encoded date and time integer An integer number, i.e. [-][0-9]+[0-9]* boolean true or false enumeration One of the values provided by the field values","title":"parameters"},{"location":"api/usage/","text":"Version Since the External API is versioned, it supports specification of a version identifier as part of the standard Accept HTTP request header: Header Type Description Accept string application/<version>+<format> is used to specify the API version and format Notes: The External API currently only supports the format JSON Example Accept: application/v1.0.0+json If that header is not specified, or no version information can be extracted from the header, the assumption is that the request should be executed against the most recent version. If the version specified is not available, 406 (NOT ACCEPTABLE) is returned as the HTTP response code. With every response, the API version is specified as part of the standard HTTP Content-Type header, as in application/v1.0.0+json . Versions should be specified as obtained from the Base API call to /versions . Authentication The External API is using basic authentication. In order to make calls to the API, the following standard request headers need to be sent with every request: Header Type Description Authorization string Sends username and password as defined in Basic Authentication Authorization There are multiple ways to authorize a request - see the authorization section for more details. In short, the External API either supports specifying the execution user, the execution user\u2019s roles or a combination of the two in which case the execution roles will be added to the execution user\u2019s existing roles. If no user is specified, Opencast\u2019s anonymous user is used to execute the request, potentially enriched by the roles provided using the X-ROLES request. Header Type Description X-API-AS-USER string Id of the user in whose name the request should be executed X-API-WITH-ROLES string Set of roles, separated by whitespace, that the execution user should be assigned in addition to existing roles. Formats and Encoding Content Type The External API currently supports JSON format only. Header Type Description Accept string The expected response format is application/json If that header is not specified, the Content-Type will be application/<version>+json . Note that the same header should be used to specify the version of the API that is expected to return the response. In this case, the header looks like this: application/v1+json . See the versioning chapter of the general section for more details. Encoding of single objects JSON notation Single objects are enclosed in curly braces \"{}\" and are not explicitly named. Example { \"firstname\": \"John\", \"lastname\": \"Doe\" } Encoding of collections of objects JSON notation Collections of objects are enclosed in braces \"[ ... ]\" and are not explicitly named. Example [ { \"firstname\": \"Jane\", \"lastname\": \"Doe\" }, { \"firstname\": \"John\", \"lastname\": \"Doe\" } ] Encoding of empty fields Instead of dropping fields that do not contain a value for a specific data object from the JSON response structure, the respective identity element should be used: Type Encoding Description Literals \"\" Strings and numbers Objects {} Non-existing objects Arrays [] Non-existing list of literals or objects Sorting Sorting of result sets is supported by a set of well-defined fields per request, one at a time. Each API request explicitly defines the fields that support sorting. Multiple sort criteria can be specified as a comma-separated list of pairs such as: Sort Name : ASC or Sort Name : DESC . Adding the suffix ASC or DESC sets the order as ascending or descending order and is mandatory. Parameter Description sort Comma-separated list of sort critera Example Ordering the list of events by title: GET /api/events?sort=title:ASC,start_date:DESC Filtering Filtering of result sets is supported by a set of well-defined fields per request. Multiple filter criteria can be defined by specifying a comma-separated list of filters. In this case, the criteria are applied using logical and . A filter is the filter's name followed by a colon \":\" and then the value to filter with so it is the form Filter Name : Value to Filter With . Each API request explicitly defines the fields that support filtering. Parameter Description filter A comma seperated list of filters to limit the results with Note that filter conditions must be URL encoded. Example Filter the list of events by status and by series. GET /api/events?filter=status%3dpublished,series%3dmath Pagination When loading large result sets, being able to address and access the data in well-defined chunks using a limit and offset is essential. Paging is enabled for all requests that return lists of items. Paramter Description limit The maximum number of results to return for a single request offset The index of the first record to return (counting starts on zero) Example From the list of events, return items 50-74. GET /api/events?limit=25&offset=50","title":"Usage"},{"location":"api/usage/#version","text":"Since the External API is versioned, it supports specification of a version identifier as part of the standard Accept HTTP request header: Header Type Description Accept string application/<version>+<format> is used to specify the API version and format Notes: The External API currently only supports the format JSON Example Accept: application/v1.0.0+json If that header is not specified, or no version information can be extracted from the header, the assumption is that the request should be executed against the most recent version. If the version specified is not available, 406 (NOT ACCEPTABLE) is returned as the HTTP response code. With every response, the API version is specified as part of the standard HTTP Content-Type header, as in application/v1.0.0+json . Versions should be specified as obtained from the Base API call to /versions .","title":"Version"},{"location":"api/usage/#authentication","text":"The External API is using basic authentication. In order to make calls to the API, the following standard request headers need to be sent with every request: Header Type Description Authorization string Sends username and password as defined in Basic Authentication","title":"Authentication"},{"location":"api/usage/#authorization","text":"There are multiple ways to authorize a request - see the authorization section for more details. In short, the External API either supports specifying the execution user, the execution user\u2019s roles or a combination of the two in which case the execution roles will be added to the execution user\u2019s existing roles. If no user is specified, Opencast\u2019s anonymous user is used to execute the request, potentially enriched by the roles provided using the X-ROLES request. Header Type Description X-API-AS-USER string Id of the user in whose name the request should be executed X-API-WITH-ROLES string Set of roles, separated by whitespace, that the execution user should be assigned in addition to existing roles.","title":"Authorization"},{"location":"api/usage/#formats-and-encoding","text":"","title":"Formats and Encoding"},{"location":"api/usage/#content-type","text":"The External API currently supports JSON format only. Header Type Description Accept string The expected response format is application/json If that header is not specified, the Content-Type will be application/<version>+json . Note that the same header should be used to specify the version of the API that is expected to return the response. In this case, the header looks like this: application/v1+json . See the versioning chapter of the general section for more details.","title":"Content Type"},{"location":"api/usage/#encoding-of-single-objects","text":"","title":"Encoding of single objects"},{"location":"api/usage/#json-notation","text":"Single objects are enclosed in curly braces \"{}\" and are not explicitly named. Example { \"firstname\": \"John\", \"lastname\": \"Doe\" }","title":"JSON notation"},{"location":"api/usage/#encoding-of-collections-of-objects","text":"","title":"Encoding of collections of objects"},{"location":"api/usage/#json-notation_1","text":"Collections of objects are enclosed in braces \"[ ... ]\" and are not explicitly named. Example [ { \"firstname\": \"Jane\", \"lastname\": \"Doe\" }, { \"firstname\": \"John\", \"lastname\": \"Doe\" } ]","title":"JSON notation"},{"location":"api/usage/#encoding-of-empty-fields","text":"Instead of dropping fields that do not contain a value for a specific data object from the JSON response structure, the respective identity element should be used: Type Encoding Description Literals \"\" Strings and numbers Objects {} Non-existing objects Arrays [] Non-existing list of literals or objects","title":"Encoding of empty fields"},{"location":"api/usage/#sorting","text":"Sorting of result sets is supported by a set of well-defined fields per request, one at a time. Each API request explicitly defines the fields that support sorting. Multiple sort criteria can be specified as a comma-separated list of pairs such as: Sort Name : ASC or Sort Name : DESC . Adding the suffix ASC or DESC sets the order as ascending or descending order and is mandatory. Parameter Description sort Comma-separated list of sort critera Example Ordering the list of events by title: GET /api/events?sort=title:ASC,start_date:DESC","title":"Sorting"},{"location":"api/usage/#filtering","text":"Filtering of result sets is supported by a set of well-defined fields per request. Multiple filter criteria can be defined by specifying a comma-separated list of filters. In this case, the criteria are applied using logical and . A filter is the filter's name followed by a colon \":\" and then the value to filter with so it is the form Filter Name : Value to Filter With . Each API request explicitly defines the fields that support filtering. Parameter Description filter A comma seperated list of filters to limit the results with Note that filter conditions must be URL encoded. Example Filter the list of events by status and by series. GET /api/events?filter=status%3dpublished,series%3dmath","title":"Filtering"},{"location":"api/usage/#pagination","text":"When loading large result sets, being able to address and access the data in well-defined chunks using a limit and offset is essential. Paging is enabled for all requests that return lists of items. Paramter Description limit The maximum number of results to return for a single request offset The index of the first record to return (counting starts on zero) Example From the list of events, return items 50-74. GET /api/events?limit=25&offset=50","title":"Pagination"},{"location":"api/workflow-api/","text":"General Workflow instances GET /api/workflows POST /api/workflows GET /api/workflows/{workflow_instance_id} PUT /api/workflows/{workflow_instance_id} DELETE /api/workflows/{workflow_instance_id} Workflow definitions GET /api/workflow-definitions GET /api/workflow-definitions/{workflow_definition_id} General The Workflow API is available since API version 1.1.0. Workflow instances GET /api/workflows Returns a list of workflow instances. The following query string parameters are supported to filter, sort and pagingate the returned list: Query String Parameter Type Description filter string A comma-separated list of filters to limit the results with (see Filtering ). See the below table for the list of available filters sort string A comma-separated list of sort criteria (see Sorting ). See the below table for the list of available sort criteria limit integer The maximum number of results to return (see Pagination ) offset integer The index of the first result to return (see Pagination ) The following filters are available: Filter Name Description state Workflow instances that are in this state. Can be included multiple times (inclusive or). state_not Workflow instances that are not in this state. Can be included multiple times (inclusive or). current_operation Workflow instances that are currently executing this operation. Can be included multiple times (inclusive or). current_operation_not Workflow instances that are currently not executing this operation. Can be included multiple times (inclusive or). workflow_definition_identifier Workflow instances that use this workflow definition event_identifier Workflow instances where the identifier of the processed event matches event_title Workflow instances where the title of the processed event matches event_created Workflow instances where the processed event was created between two dates event_creator Workflow instances where the creator of the processed event matches event_contributor Workflow instances where the contributor of the processed event matches event_language Workflow instances where the language of the processed event matches event_license Workflow instances where the license of the processed event matches event_subject Workflow instances where the subject of the processed event matches series_identifier Workflow instances where the identifier of series of the processed event matches series_title Workflow instances where the title of series of the processed event matches textFilter Workflow instances where any part of the Workflow definition's metadata fields match this value Note: The filters event_created expect the following value: datetime + '/' + datetime The list can be sorted by the following criteria: Sort Criteria Description event_identifier By the event identifier of the workflow instance event_title By the event title of the workflow instance event_created By the event creation date of the workflow instance event_creator By the event creator of the workflow instance event_contributor By the event contributor of the workflow instance event_language By the event language of the workflow instance event_license By the event license of the workflow instance event_subject By the event subject of the workflow instance series_identifier By the series identifier of the workflow instance series_title By the series title of the workflow instance workflow_definition_identifier By the workflow definition identifier of the workflow instance This request additionally supports the following query string parameters to include additional information directly in the response: Query String Parameter Type Description withoperations boolean Whether the workflow operations should be included in the response withconfiguration boolean Whether the workflow configuration should be included in the response Sample request https://opencast.example.org/api/workflow-definitions?sort=event_created:DESC&limit=5&offset=1&filter=workflow_definition_identifier:fast Response 200 (OK) : A (potentially empty) list of workflow instances is returned. The list is represented as JSON array where each element is a JSON object with the following fields: Field Type Description identifier integer The unique identifier of this workflow instance title string The title of this workflow instance description string The description of this workflow instance state workflow_state The state of this workflow instance operations array[operation_instance] The list of operations of this workflow instance configuration property The configuration for this workflow instance workflow_definition_identifier string The template of this workflow instance (i.e. the unique identifier of the workflow definition) event_identifier string The id of the event this workflow instance belongs to creator string The name of the creator of this workflow instance 400 (BAD REQUEST) : The request is invalid or inconsistent. Example [ { \"workflow_definition_identifier\": \"fast\", \"identifier\": 1603, \"creator\": \"Opencast Project Administrator\", \"operations\": [ { \"identifier\": 1604, \"completion\": \"2018-08-08T08:46:57.3Z\", \"configuration\": { \"publishLive\": \"false\", \"uploadedSearchPreview\": \"false\", \"publishToOaiPmh\": \"true\", \"comment\": \"false\", \"publishToMediaModule\": \"true\" }, \"time_in_queue\": 0, \"failed_attempts\": 0, \"start\": \"2018-08-08T08:46:57.201Z\", \"description\": \"Applying default configuration values\", \"fail_workflow_on_error\": true, \"unless\": \"\", \"max_attempts\": 1, \"host\": \"http:\\/\\/localhost:8080\", \"state\": \"succeeded\", \"operation\": \"defaults\", \"if\": \"\", \"retry_strategy\": \"none\", \"error_handler_workflow\": \"\" }, { \"identifier\": 1605, \"completion\": \"\", \"configuration\": { \"apply-acl\": \"true\" }, \"time_in_queue\": 0, \"failed_attempts\": 0, \"start\": \"2018-08-08T08:47:02.209Z\", \"description\": \"Applying access control entries from series\", \"fail_workflow_on_error\": true, \"unless\": \"\", \"max_attempts\": 1, \"host\": \"http:\\/\\/localhost:8080\", \"state\": \"skipped\", \"operation\": \"series\", \"if\": \"\", \"retry_strategy\": \"none\", \"error_handler_workflow\": \"partial-error\" } ... ], \"configuration\": { \"publishLive\": \"false\", \"workflowDefinitionId\": \"fast\", \"uploadedSearchPreview\": \"false\", \"publishToOaiPmh\": \"true\", \"comment\": \"false\", \"publishToMediaModule\": \"true\" }, \"description\": \"\\n A minimal workflow that transcodes the media into distribution formats, then\\n sends the resulting distribution files, along with their associated metadata,\\n to the distribution channels.\\n \", \"state\": \"succeeded\", \"title\": \"Fast Testing Workflow\", \"event_identifier\": \"f41e4417-b841-4d20-a466-f98ddfbe4c2a\" } ] POST /api/workflows Creates a workflow instance. Form Parameters Required Type Description event_identifier yes string The event identifier this workflow should run against workflow_definition_identifier yes string The identifier of the workflow definition to use configuration no property The optional configuration for this workflow This request additionally supports the following query string parameters to include additional information directly in the response: Query String Parameter Type Description withoperations boolean Whether the workflow operations should be included in the response withconfiguration boolean Whether the workflow configuration should be included in the response Example event_identifier: f41e4417-b841-4d20-a466-f98ddfbe4c2a workflow_definition_identifier: fast configuration: { \"publishLive\": \"false\", \"uploadedSearchPreview\": \"false\", \"publishToOaiPmh\": \"true\", \"comment\": \"false\", \"publishToMediaModule\": \"true\" } Response 201 (CREATED) : A new workflow is created and its identifier is returned in the Location header. Location: http://api.opencast.org/api/workflows/3170 The workflow instance is returned as JSON object with the following fields: Field Type Description identifier string The unique identifier of this workflow instance title string The title of this workflow instance description string The description of this workflow instance tags array[string] The (potentially empty) list of workflow tags of this workflow instance configuration_panel string The configuration panel of this workflow instance operations array[operation_instance] The list of operations of this workflow instance 400 (BAD REQUEST) : The request is invalid or inconsistent. 404 (NOT FOUND) : The specified workflow instance does not exist. GET /api/workflows/{workflow_instance_id} Returns a single workflow instance. This request supports the following query string parameters to include additional information directly in the response: Query String Parameter Type Description withoperations boolean Whether the workflow operations should be included in the response withconfiguration boolean Whether the workflow configuration should be included in the response Response 200 (OK) : The workflow instance is returned as JSON object with the following fields: Field Type Description identifier string The unique identifier of this workflow instance title string The title of this workflow instance description string The description of this workflow instance tags array[string] The (potentially empty) list of workflow tags of this workflow instance configuration_panel string The configuration panel of this workflow instance operations array[operation_instance] The list of operations of this workflow instance 403 (FORBIDDEN) : The user doesn't have the rights to make this request. 404 (NOT FOUND) : The event or workflow definition could not be found. Example { \"workflow_definition_identifier\": \"fast\", \"identifier\": 1603, \"creator\": \"Opencast Project Administrator\", \"operations\": [ { \"identifier\": 1604, \"completion\": \"2018-08-08T08:46:57.3Z\", \"configuration\": { \"publishLive\": \"false\", \"uploadedSearchPreview\": \"false\", \"publishToOaiPmh\": \"true\", \"comment\": \"false\", \"publishToMediaModule\": \"true\" }, \"time_in_queue\": 0, \"failed_attempts\": 0, \"start\": \"2018-08-08T08:46:57.201Z\", \"description\": \"Applying default configuration values\", \"fail_workflow_on_error\": true, \"unless\": \"\", \"max_attempts\": 1, \"host\": \"http:\\/\\/localhost:8080\", \"state\": \"succeeded\", \"operation\": \"defaults\", \"if\": \"\", \"retry_strategy\": \"none\", \"error_handler_workflow\": \"\" }, { \"identifier\": 1605, \"completion\": \"\", \"configuration\": { \"apply-acl\": \"true\" }, \"time_in_queue\": 0, \"failed_attempts\": 0, \"start\": \"2018-08-08T08:47:02.209Z\", \"description\": \"Applying access control entries from series\", \"fail_workflow_on_error\": true, \"unless\": \"\", \"max_attempts\": 1, \"host\": \"http:\\/\\/localhost:8080\", \"state\": \"skipped\", \"operation\": \"series\", \"if\": \"\", \"retry_strategy\": \"none\", \"error_handler_workflow\": \"partial-error\" } ... ], \"configuration\": { \"publishLive\": \"false\", \"workflowDefinitionId\": \"fast\", \"uploadedSearchPreview\": \"false\", \"publishToOaiPmh\": \"true\", \"comment\": \"false\", \"publishToMediaModule\": \"true\" }, \"description\": \"\\n A minimal workflow that transcodes the media into distribution formats, then\\n sends the resulting distribution files, along with their associated metadata,\\n to the distribution channels.\\n \", \"state\": \"running\", \"title\": \"Fast Testing Workflow\", \"event_identifier\": \"f41e4417-b841-4d20-a466-f98ddfbe4c2a\" } PUT /api/workflows/{workflow_instance_id} Updates a workflow instance. Form Parameters Required Type Description state no workflow_state The optional state transition for this workflow configuration no property The optional configuration for this workflow This request additionally supports the following query string parameters to include additional information directly in the response: Query String Parameter Type Description withoperations boolean Whether the workflow operations should be included in the response withconfiguration boolean Whether the workflow configuration should be included in the response Example state: paused configuration: { \"publishLive\": \"false\", \"uploadedSearchPreview\": \"false\", \"publishToOaiPmh\": \"true\", \"comment\": \"false\", \"publishToMediaModule\": \"true\" } Allowed workflow state transitions The following workflow state transitions are allowed: Current state Allowed new state instantiated paused , stopped , running running paused , stopped failing paused , stopped paused paused , stopped , running succeeded paused , stopped stopped paused , stopped failed paused , stopped Response 200 (OK) : The workflow instance is updated and returned as JSON object with the following fields: Field Type Description identifier string The unique identifier of this workflow instance title string The title of this workflow instance description string The description of this workflow instance tags array[string] The (potentially empty) list of workflow tags of this workflow instance configuration_panel string The configuration panel of this workflow instance operations array[operation_instance] The list of operations of this workflow instance 400 (BAD REQUEST) : The request is invalid or inconsistent. 403 (FORBIDDEN) : The user doesn't have the rights to make this request. 404 (NOT FOUND) : The workflow instance could not be found. 409 (CONFLICT) : The workflow instance cannot transition to this state. DELETE /api/workflows/{workflow_instance_id} Deletes a workflow instance. Response 204 (NO CONTENT) : The workflow instance has been deleted. 403 (FORBIDDEN) : The user doesn't have the rights to make this request. 404 (NOT FOUND) : The specified workflow instance does not exist. 409 (CONFLICT) : The workflow instance cannot be deleted in this state. Workflow definitions GET /api/workflow-definitions Returns a list of workflow definitions. The following query string parameters are supported to filter, sort and pagingate the returned list: Query String Parameter Type Description filter string A comma-separated list of filters to limit the results with (see Filtering ). See the below table for the list of available filters sort string A comma-separated list of sort criteria (see Sorting ). See the below table for the list of available sort criteria limit integer The maximum number of results to return (see Pagination ) offset integer The index of the first result to return (see Pagination ) The following filters are available: Filter Name Description tag Workflow definitions where the tag is included The list can be sorted by the following criteria: Sort Criteria Description identifier By the identifier of the workflow definition title By the title of the workflow definition displayorder By the display order of the workflow definition This request additionally supports the following query string parameters to include additional information directly in the response: Query String Parameter Type Description withoperations boolean Whether the workflow operations should be included in the response withconfigurationpanel boolean Whether the workflow configuration panel should be included in the response Sample request https://opencast.example.org/api/workflow-definitions?sort=title:DESC&limit=5&offset=1&filter=tag:archive Response 200 (OK) : A (potentially empty) list of workflow definitions is returned. The list is represented as JSON array where each element is a JSON object with the following fields: Field Type Description identifier string The unique identifier of this workflow definition title string The title of this workflow definition description string The description of this workflow definition tags array[string] The (potentially empty) list of workflow tags of this workflow definition configuration_panel string The configuration panel of this workflow definition operations array[operation_definition] The list of operations of this workflow definition 400 (BAD REQUEST) : The request is invalid or inconsistent. Example [ { \"id\": \"fast\", \"title\": \"Fast Testing Workflow\", \"description\": \"\\n A minimal workflow that transcodes the media into distribution formats, then\\n sends the resulting distribution files, along with their associated metadata,\\n to the distribution channels.\\n \", \"tags\": [ \"schedule\", \"upload\" ], \"configuration_panel\": \"\\n \\n <div id=\\\"workflow-configuration\\\">\\n <fieldset>\\n <legend>Add a comment that the recording needs:</legend>\\n <ul>\\n <li>\\n <input id=\\\"comment\\\" name=\\\"comment\\\" type=\\\"checkbox\\\" class=\\\"configField\\\" value=\\\"true\\\" />\\n <label for=\\\"comment\\\">Review / Cutting</label>\\n </li>\\n </ul>\\n </fieldset>\\n <fieldset>\\n <legend>Immediately distribute the recording to:</legend>\\n <ul>\\n <li>\\n <input id=\\\"publishToMediaModule\\\" name=\\\"publishToMediaModule\\\" type=\\\"checkbox\\\" class=\\\"configField\\\" value=\\\"true\\\" checked=checked />\\n <label for=\\\"publishToMediaModule\\\">Opencast Media Module</label>\\n </li>\\n <li>\\n <input id=\\\"publishToOaiPmh\\\" name=\\\"publishToOaiPmh\\\" type=\\\"checkbox\\\" class=\\\"configField\\\" value=\\\"true\\\" checked=checked />\\n <label for=\\\"publishToOaiPmh\\\">Default OAI-PMH Repository</label>\\n </li>\\n </ul>\\n </fieldset>\\n <fieldset>\\n <legend>Publish live stream:</legend>\\n <ul>\\n <li>\\n <input id=\\\"publishLive\\\" name=\\\"publishLive\\\" type=\\\"checkbox\\\" class=\\\"configField\\\" value=\\\"false\\\" />\\n <label for=\\\"publishLive\\\">Add live event to Opencast Media Module</label>\\n </li>\\n </ul>\\n </fieldset>\\n </div>\\n \\n \", \"operations\": [ { \"id\": \"defaults\", \"description\": \"Applying default configuration values\", \"configuration\": { \"publishLive\": \"false\", \"publishToOaiPmh\": \"true\", \"comment\": \"false\", \"publishToMediaModule\": \"true\", \"uploadedSearchPreview\": \"false\" }, \"unless\": \"\", \"if\": \"\", \"fail_workflow_on_error\": \"true\", \"error_handler_workflow\": \"\", \"retry_strategy\": \"none\", \"max_attempts\": \"1\" }, { \"id\": \"series\", \"description\": \"Applying access control entries from series\", \"configuration\": { \"apply-acl\": \"true\" }, \"unless\": \"\", \"if\": \"\", \"fail_workflow_on_error\": \"true\", \"error_handler_workflow\": \"partial_error\", \"retry_strategy\": \"none\", \"max_attempts\": \"1\" } ... ] } ] GET /api/workflow-definitions/{workflow_definition_id} Returns a single workflow definition. This request supports the following query string parameters to include additional information directly in the response: Query String Parameter Type Description withoperations boolean Whether the workflow operations should be included in the response withconfigurationpanel boolean Whether the workflow configuration panel should be included in the response Response 200 (OK) : The workflow definition is returned as JSON object with the following fields: Field Type Description identifier string The unique identifier of this workflow definition title string The title of this workflow definition description string The description of this workflow definition tags array[string] The (potentially empty) list of workflow tags of this workflow definition configuration_panel string The configuration panel of this workflow definition operations array[operation_definition] The list of operations of this workflow definition 404 (NOT FOUND) : The specified workflow definition does not exist. Example { \"id\": \"fast\", \"title\": \"Fast Testing Workflow\", \"description\": \"\\n A minimal workflow that transcodes the media into distribution formats, then\\n sends the resulting distribution files, along with their associated metadata,\\n to the distribution channels.\\n \", \"tags\": [ \"schedule\", \"upload\" ], \"configuration_panel\": \"\\n \\n <div id=\\\"workflow-configuration\\\">\\n <fieldset>\\n <legend>Add a comment that the recording needs:</legend>\\n <ul>\\n <li>\\n <input id=\\\"comment\\\" name=\\\"comment\\\" type=\\\"checkbox\\\" class=\\\"configField\\\" value=\\\"true\\\" />\\n <label for=\\\"comment\\\">Review / Cutting</label>\\n </li>\\n </ul>\\n </fieldset>\\n <fieldset>\\n <legend>Immediately distribute the recording to:</legend>\\n <ul>\\n <li>\\n <input id=\\\"publishToMediaModule\\\" name=\\\"publishToMediaModule\\\" type=\\\"checkbox\\\" class=\\\"configField\\\" value=\\\"true\\\" checked=checked />\\n <label for=\\\"publishToMediaModule\\\">Opencast Media Module</label>\\n </li>\\n <li>\\n <input id=\\\"publishToOaiPmh\\\" name=\\\"publishToOaiPmh\\\" type=\\\"checkbox\\\" class=\\\"configField\\\" value=\\\"true\\\" checked=checked />\\n <label for=\\\"publishToOaiPmh\\\">Default OAI-PMH Repository</label>\\n </li>\\n </ul>\\n </fieldset>\\n <fieldset>\\n <legend>Publish live stream:</legend>\\n <ul>\\n <li>\\n <input id=\\\"publishLive\\\" name=\\\"publishLive\\\" type=\\\"checkbox\\\" class=\\\"configField\\\" value=\\\"false\\\" />\\n <label for=\\\"publishLive\\\">Add live event to Opencast Media Module</label>\\n </li>\\n </ul>\\n </fieldset>\\n </div>\\n \\n \", \"operations\": [ { \"id\": \"defaults\", \"description\": \"Applying default configuration values\", \"configuration\": { \"publishLive\": \"false\", \"publishToOaiPmh\": \"true\", \"comment\": \"false\", \"publishToMediaModule\": \"true\", \"uploadedSearchPreview\": \"false\" }, \"unless\": \"\", \"if\": \"\", \"fail_workflow_on_error\": \"true\", \"error_handler_workflow\": \"\", \"retry_strategy\": \"none\", \"max_attempts\": \"1\" }, { \"id\": \"series\", \"description\": \"Applying access control entries from series\", \"configuration\": { \"apply-acl\": \"true\" }, \"unless\": \"\", \"if\": \"\", \"fail_workflow_on_error\": \"true\", \"error_handler_workflow\": \"partial_error\", \"retry_strategy\": \"none\", \"max_attempts\": \"1\" } ... ] }","title":"Workflow API"},{"location":"api/workflow-api/#general","text":"The Workflow API is available since API version 1.1.0.","title":"General"},{"location":"api/workflow-api/#workflow-instances","text":"","title":"Workflow instances"},{"location":"api/workflow-api/#get-apiworkflows","text":"Returns a list of workflow instances. The following query string parameters are supported to filter, sort and pagingate the returned list: Query String Parameter Type Description filter string A comma-separated list of filters to limit the results with (see Filtering ). See the below table for the list of available filters sort string A comma-separated list of sort criteria (see Sorting ). See the below table for the list of available sort criteria limit integer The maximum number of results to return (see Pagination ) offset integer The index of the first result to return (see Pagination ) The following filters are available: Filter Name Description state Workflow instances that are in this state. Can be included multiple times (inclusive or). state_not Workflow instances that are not in this state. Can be included multiple times (inclusive or). current_operation Workflow instances that are currently executing this operation. Can be included multiple times (inclusive or). current_operation_not Workflow instances that are currently not executing this operation. Can be included multiple times (inclusive or). workflow_definition_identifier Workflow instances that use this workflow definition event_identifier Workflow instances where the identifier of the processed event matches event_title Workflow instances where the title of the processed event matches event_created Workflow instances where the processed event was created between two dates event_creator Workflow instances where the creator of the processed event matches event_contributor Workflow instances where the contributor of the processed event matches event_language Workflow instances where the language of the processed event matches event_license Workflow instances where the license of the processed event matches event_subject Workflow instances where the subject of the processed event matches series_identifier Workflow instances where the identifier of series of the processed event matches series_title Workflow instances where the title of series of the processed event matches textFilter Workflow instances where any part of the Workflow definition's metadata fields match this value Note: The filters event_created expect the following value: datetime + '/' + datetime The list can be sorted by the following criteria: Sort Criteria Description event_identifier By the event identifier of the workflow instance event_title By the event title of the workflow instance event_created By the event creation date of the workflow instance event_creator By the event creator of the workflow instance event_contributor By the event contributor of the workflow instance event_language By the event language of the workflow instance event_license By the event license of the workflow instance event_subject By the event subject of the workflow instance series_identifier By the series identifier of the workflow instance series_title By the series title of the workflow instance workflow_definition_identifier By the workflow definition identifier of the workflow instance This request additionally supports the following query string parameters to include additional information directly in the response: Query String Parameter Type Description withoperations boolean Whether the workflow operations should be included in the response withconfiguration boolean Whether the workflow configuration should be included in the response Sample request https://opencast.example.org/api/workflow-definitions?sort=event_created:DESC&limit=5&offset=1&filter=workflow_definition_identifier:fast Response 200 (OK) : A (potentially empty) list of workflow instances is returned. The list is represented as JSON array where each element is a JSON object with the following fields: Field Type Description identifier integer The unique identifier of this workflow instance title string The title of this workflow instance description string The description of this workflow instance state workflow_state The state of this workflow instance operations array[operation_instance] The list of operations of this workflow instance configuration property The configuration for this workflow instance workflow_definition_identifier string The template of this workflow instance (i.e. the unique identifier of the workflow definition) event_identifier string The id of the event this workflow instance belongs to creator string The name of the creator of this workflow instance 400 (BAD REQUEST) : The request is invalid or inconsistent. Example [ { \"workflow_definition_identifier\": \"fast\", \"identifier\": 1603, \"creator\": \"Opencast Project Administrator\", \"operations\": [ { \"identifier\": 1604, \"completion\": \"2018-08-08T08:46:57.3Z\", \"configuration\": { \"publishLive\": \"false\", \"uploadedSearchPreview\": \"false\", \"publishToOaiPmh\": \"true\", \"comment\": \"false\", \"publishToMediaModule\": \"true\" }, \"time_in_queue\": 0, \"failed_attempts\": 0, \"start\": \"2018-08-08T08:46:57.201Z\", \"description\": \"Applying default configuration values\", \"fail_workflow_on_error\": true, \"unless\": \"\", \"max_attempts\": 1, \"host\": \"http:\\/\\/localhost:8080\", \"state\": \"succeeded\", \"operation\": \"defaults\", \"if\": \"\", \"retry_strategy\": \"none\", \"error_handler_workflow\": \"\" }, { \"identifier\": 1605, \"completion\": \"\", \"configuration\": { \"apply-acl\": \"true\" }, \"time_in_queue\": 0, \"failed_attempts\": 0, \"start\": \"2018-08-08T08:47:02.209Z\", \"description\": \"Applying access control entries from series\", \"fail_workflow_on_error\": true, \"unless\": \"\", \"max_attempts\": 1, \"host\": \"http:\\/\\/localhost:8080\", \"state\": \"skipped\", \"operation\": \"series\", \"if\": \"\", \"retry_strategy\": \"none\", \"error_handler_workflow\": \"partial-error\" } ... ], \"configuration\": { \"publishLive\": \"false\", \"workflowDefinitionId\": \"fast\", \"uploadedSearchPreview\": \"false\", \"publishToOaiPmh\": \"true\", \"comment\": \"false\", \"publishToMediaModule\": \"true\" }, \"description\": \"\\n A minimal workflow that transcodes the media into distribution formats, then\\n sends the resulting distribution files, along with their associated metadata,\\n to the distribution channels.\\n \", \"state\": \"succeeded\", \"title\": \"Fast Testing Workflow\", \"event_identifier\": \"f41e4417-b841-4d20-a466-f98ddfbe4c2a\" } ]","title":"GET /api/workflows"},{"location":"api/workflow-api/#post-apiworkflows","text":"Creates a workflow instance. Form Parameters Required Type Description event_identifier yes string The event identifier this workflow should run against workflow_definition_identifier yes string The identifier of the workflow definition to use configuration no property The optional configuration for this workflow This request additionally supports the following query string parameters to include additional information directly in the response: Query String Parameter Type Description withoperations boolean Whether the workflow operations should be included in the response withconfiguration boolean Whether the workflow configuration should be included in the response Example event_identifier: f41e4417-b841-4d20-a466-f98ddfbe4c2a workflow_definition_identifier: fast configuration: { \"publishLive\": \"false\", \"uploadedSearchPreview\": \"false\", \"publishToOaiPmh\": \"true\", \"comment\": \"false\", \"publishToMediaModule\": \"true\" } Response 201 (CREATED) : A new workflow is created and its identifier is returned in the Location header. Location: http://api.opencast.org/api/workflows/3170 The workflow instance is returned as JSON object with the following fields: Field Type Description identifier string The unique identifier of this workflow instance title string The title of this workflow instance description string The description of this workflow instance tags array[string] The (potentially empty) list of workflow tags of this workflow instance configuration_panel string The configuration panel of this workflow instance operations array[operation_instance] The list of operations of this workflow instance 400 (BAD REQUEST) : The request is invalid or inconsistent. 404 (NOT FOUND) : The specified workflow instance does not exist.","title":"POST /api/workflows"},{"location":"api/workflow-api/#get-apiworkflowsworkflow_instance_id","text":"Returns a single workflow instance. This request supports the following query string parameters to include additional information directly in the response: Query String Parameter Type Description withoperations boolean Whether the workflow operations should be included in the response withconfiguration boolean Whether the workflow configuration should be included in the response Response 200 (OK) : The workflow instance is returned as JSON object with the following fields: Field Type Description identifier string The unique identifier of this workflow instance title string The title of this workflow instance description string The description of this workflow instance tags array[string] The (potentially empty) list of workflow tags of this workflow instance configuration_panel string The configuration panel of this workflow instance operations array[operation_instance] The list of operations of this workflow instance 403 (FORBIDDEN) : The user doesn't have the rights to make this request. 404 (NOT FOUND) : The event or workflow definition could not be found. Example { \"workflow_definition_identifier\": \"fast\", \"identifier\": 1603, \"creator\": \"Opencast Project Administrator\", \"operations\": [ { \"identifier\": 1604, \"completion\": \"2018-08-08T08:46:57.3Z\", \"configuration\": { \"publishLive\": \"false\", \"uploadedSearchPreview\": \"false\", \"publishToOaiPmh\": \"true\", \"comment\": \"false\", \"publishToMediaModule\": \"true\" }, \"time_in_queue\": 0, \"failed_attempts\": 0, \"start\": \"2018-08-08T08:46:57.201Z\", \"description\": \"Applying default configuration values\", \"fail_workflow_on_error\": true, \"unless\": \"\", \"max_attempts\": 1, \"host\": \"http:\\/\\/localhost:8080\", \"state\": \"succeeded\", \"operation\": \"defaults\", \"if\": \"\", \"retry_strategy\": \"none\", \"error_handler_workflow\": \"\" }, { \"identifier\": 1605, \"completion\": \"\", \"configuration\": { \"apply-acl\": \"true\" }, \"time_in_queue\": 0, \"failed_attempts\": 0, \"start\": \"2018-08-08T08:47:02.209Z\", \"description\": \"Applying access control entries from series\", \"fail_workflow_on_error\": true, \"unless\": \"\", \"max_attempts\": 1, \"host\": \"http:\\/\\/localhost:8080\", \"state\": \"skipped\", \"operation\": \"series\", \"if\": \"\", \"retry_strategy\": \"none\", \"error_handler_workflow\": \"partial-error\" } ... ], \"configuration\": { \"publishLive\": \"false\", \"workflowDefinitionId\": \"fast\", \"uploadedSearchPreview\": \"false\", \"publishToOaiPmh\": \"true\", \"comment\": \"false\", \"publishToMediaModule\": \"true\" }, \"description\": \"\\n A minimal workflow that transcodes the media into distribution formats, then\\n sends the resulting distribution files, along with their associated metadata,\\n to the distribution channels.\\n \", \"state\": \"running\", \"title\": \"Fast Testing Workflow\", \"event_identifier\": \"f41e4417-b841-4d20-a466-f98ddfbe4c2a\" }","title":"GET /api/workflows/{workflow_instance_id}"},{"location":"api/workflow-api/#put-apiworkflowsworkflow_instance_id","text":"Updates a workflow instance. Form Parameters Required Type Description state no workflow_state The optional state transition for this workflow configuration no property The optional configuration for this workflow This request additionally supports the following query string parameters to include additional information directly in the response: Query String Parameter Type Description withoperations boolean Whether the workflow operations should be included in the response withconfiguration boolean Whether the workflow configuration should be included in the response Example state: paused configuration: { \"publishLive\": \"false\", \"uploadedSearchPreview\": \"false\", \"publishToOaiPmh\": \"true\", \"comment\": \"false\", \"publishToMediaModule\": \"true\" } Allowed workflow state transitions The following workflow state transitions are allowed: Current state Allowed new state instantiated paused , stopped , running running paused , stopped failing paused , stopped paused paused , stopped , running succeeded paused , stopped stopped paused , stopped failed paused , stopped Response 200 (OK) : The workflow instance is updated and returned as JSON object with the following fields: Field Type Description identifier string The unique identifier of this workflow instance title string The title of this workflow instance description string The description of this workflow instance tags array[string] The (potentially empty) list of workflow tags of this workflow instance configuration_panel string The configuration panel of this workflow instance operations array[operation_instance] The list of operations of this workflow instance 400 (BAD REQUEST) : The request is invalid or inconsistent. 403 (FORBIDDEN) : The user doesn't have the rights to make this request. 404 (NOT FOUND) : The workflow instance could not be found. 409 (CONFLICT) : The workflow instance cannot transition to this state.","title":"PUT /api/workflows/{workflow_instance_id}"},{"location":"api/workflow-api/#delete-apiworkflowsworkflow_instance_id","text":"Deletes a workflow instance. Response 204 (NO CONTENT) : The workflow instance has been deleted. 403 (FORBIDDEN) : The user doesn't have the rights to make this request. 404 (NOT FOUND) : The specified workflow instance does not exist. 409 (CONFLICT) : The workflow instance cannot be deleted in this state.","title":"DELETE /api/workflows/{workflow_instance_id}"},{"location":"api/workflow-api/#workflow-definitions","text":"","title":"Workflow definitions"},{"location":"api/workflow-api/#get-apiworkflow-definitions","text":"Returns a list of workflow definitions. The following query string parameters are supported to filter, sort and pagingate the returned list: Query String Parameter Type Description filter string A comma-separated list of filters to limit the results with (see Filtering ). See the below table for the list of available filters sort string A comma-separated list of sort criteria (see Sorting ). See the below table for the list of available sort criteria limit integer The maximum number of results to return (see Pagination ) offset integer The index of the first result to return (see Pagination ) The following filters are available: Filter Name Description tag Workflow definitions where the tag is included The list can be sorted by the following criteria: Sort Criteria Description identifier By the identifier of the workflow definition title By the title of the workflow definition displayorder By the display order of the workflow definition This request additionally supports the following query string parameters to include additional information directly in the response: Query String Parameter Type Description withoperations boolean Whether the workflow operations should be included in the response withconfigurationpanel boolean Whether the workflow configuration panel should be included in the response Sample request https://opencast.example.org/api/workflow-definitions?sort=title:DESC&limit=5&offset=1&filter=tag:archive Response 200 (OK) : A (potentially empty) list of workflow definitions is returned. The list is represented as JSON array where each element is a JSON object with the following fields: Field Type Description identifier string The unique identifier of this workflow definition title string The title of this workflow definition description string The description of this workflow definition tags array[string] The (potentially empty) list of workflow tags of this workflow definition configuration_panel string The configuration panel of this workflow definition operations array[operation_definition] The list of operations of this workflow definition 400 (BAD REQUEST) : The request is invalid or inconsistent. Example [ { \"id\": \"fast\", \"title\": \"Fast Testing Workflow\", \"description\": \"\\n A minimal workflow that transcodes the media into distribution formats, then\\n sends the resulting distribution files, along with their associated metadata,\\n to the distribution channels.\\n \", \"tags\": [ \"schedule\", \"upload\" ], \"configuration_panel\": \"\\n \\n <div id=\\\"workflow-configuration\\\">\\n <fieldset>\\n <legend>Add a comment that the recording needs:</legend>\\n <ul>\\n <li>\\n <input id=\\\"comment\\\" name=\\\"comment\\\" type=\\\"checkbox\\\" class=\\\"configField\\\" value=\\\"true\\\" />\\n <label for=\\\"comment\\\">Review / Cutting</label>\\n </li>\\n </ul>\\n </fieldset>\\n <fieldset>\\n <legend>Immediately distribute the recording to:</legend>\\n <ul>\\n <li>\\n <input id=\\\"publishToMediaModule\\\" name=\\\"publishToMediaModule\\\" type=\\\"checkbox\\\" class=\\\"configField\\\" value=\\\"true\\\" checked=checked />\\n <label for=\\\"publishToMediaModule\\\">Opencast Media Module</label>\\n </li>\\n <li>\\n <input id=\\\"publishToOaiPmh\\\" name=\\\"publishToOaiPmh\\\" type=\\\"checkbox\\\" class=\\\"configField\\\" value=\\\"true\\\" checked=checked />\\n <label for=\\\"publishToOaiPmh\\\">Default OAI-PMH Repository</label>\\n </li>\\n </ul>\\n </fieldset>\\n <fieldset>\\n <legend>Publish live stream:</legend>\\n <ul>\\n <li>\\n <input id=\\\"publishLive\\\" name=\\\"publishLive\\\" type=\\\"checkbox\\\" class=\\\"configField\\\" value=\\\"false\\\" />\\n <label for=\\\"publishLive\\\">Add live event to Opencast Media Module</label>\\n </li>\\n </ul>\\n </fieldset>\\n </div>\\n \\n \", \"operations\": [ { \"id\": \"defaults\", \"description\": \"Applying default configuration values\", \"configuration\": { \"publishLive\": \"false\", \"publishToOaiPmh\": \"true\", \"comment\": \"false\", \"publishToMediaModule\": \"true\", \"uploadedSearchPreview\": \"false\" }, \"unless\": \"\", \"if\": \"\", \"fail_workflow_on_error\": \"true\", \"error_handler_workflow\": \"\", \"retry_strategy\": \"none\", \"max_attempts\": \"1\" }, { \"id\": \"series\", \"description\": \"Applying access control entries from series\", \"configuration\": { \"apply-acl\": \"true\" }, \"unless\": \"\", \"if\": \"\", \"fail_workflow_on_error\": \"true\", \"error_handler_workflow\": \"partial_error\", \"retry_strategy\": \"none\", \"max_attempts\": \"1\" } ... ] } ]","title":"GET /api/workflow-definitions"},{"location":"api/workflow-api/#get-apiworkflow-definitionsworkflow_definition_id","text":"Returns a single workflow definition. This request supports the following query string parameters to include additional information directly in the response: Query String Parameter Type Description withoperations boolean Whether the workflow operations should be included in the response withconfigurationpanel boolean Whether the workflow configuration panel should be included in the response Response 200 (OK) : The workflow definition is returned as JSON object with the following fields: Field Type Description identifier string The unique identifier of this workflow definition title string The title of this workflow definition description string The description of this workflow definition tags array[string] The (potentially empty) list of workflow tags of this workflow definition configuration_panel string The configuration panel of this workflow definition operations array[operation_definition] The list of operations of this workflow definition 404 (NOT FOUND) : The specified workflow definition does not exist. Example { \"id\": \"fast\", \"title\": \"Fast Testing Workflow\", \"description\": \"\\n A minimal workflow that transcodes the media into distribution formats, then\\n sends the resulting distribution files, along with their associated metadata,\\n to the distribution channels.\\n \", \"tags\": [ \"schedule\", \"upload\" ], \"configuration_panel\": \"\\n \\n <div id=\\\"workflow-configuration\\\">\\n <fieldset>\\n <legend>Add a comment that the recording needs:</legend>\\n <ul>\\n <li>\\n <input id=\\\"comment\\\" name=\\\"comment\\\" type=\\\"checkbox\\\" class=\\\"configField\\\" value=\\\"true\\\" />\\n <label for=\\\"comment\\\">Review / Cutting</label>\\n </li>\\n </ul>\\n </fieldset>\\n <fieldset>\\n <legend>Immediately distribute the recording to:</legend>\\n <ul>\\n <li>\\n <input id=\\\"publishToMediaModule\\\" name=\\\"publishToMediaModule\\\" type=\\\"checkbox\\\" class=\\\"configField\\\" value=\\\"true\\\" checked=checked />\\n <label for=\\\"publishToMediaModule\\\">Opencast Media Module</label>\\n </li>\\n <li>\\n <input id=\\\"publishToOaiPmh\\\" name=\\\"publishToOaiPmh\\\" type=\\\"checkbox\\\" class=\\\"configField\\\" value=\\\"true\\\" checked=checked />\\n <label for=\\\"publishToOaiPmh\\\">Default OAI-PMH Repository</label>\\n </li>\\n </ul>\\n </fieldset>\\n <fieldset>\\n <legend>Publish live stream:</legend>\\n <ul>\\n <li>\\n <input id=\\\"publishLive\\\" name=\\\"publishLive\\\" type=\\\"checkbox\\\" class=\\\"configField\\\" value=\\\"false\\\" />\\n <label for=\\\"publishLive\\\">Add live event to Opencast Media Module</label>\\n </li>\\n </ul>\\n </fieldset>\\n </div>\\n \\n \", \"operations\": [ { \"id\": \"defaults\", \"description\": \"Applying default configuration values\", \"configuration\": { \"publishLive\": \"false\", \"publishToOaiPmh\": \"true\", \"comment\": \"false\", \"publishToMediaModule\": \"true\", \"uploadedSearchPreview\": \"false\" }, \"unless\": \"\", \"if\": \"\", \"fail_workflow_on_error\": \"true\", \"error_handler_workflow\": \"\", \"retry_strategy\": \"none\", \"max_attempts\": \"1\" }, { \"id\": \"series\", \"description\": \"Applying access control entries from series\", \"configuration\": { \"apply-acl\": \"true\" }, \"unless\": \"\", \"if\": \"\", \"fail_workflow_on_error\": \"true\", \"error_handler_workflow\": \"partial_error\", \"retry_strategy\": \"none\", \"max_attempts\": \"1\" } ... ] }","title":"GET /api/workflow-definitions/{workflow_definition_id}"},{"location":"decision-making/","text":"Decision Making The most important thing about engaging with Opencast is that all people with an opinion are entitled to express that opinion and, where appropriate, have it considered by the community. To some, the idea of having to establish consensus in a large and distributed team sounds inefficient and frustrating. Do not despair though, we have a set of simple processes to ensure things proceed at a good pace. In Opencast we do not like to vote. We reserve that for the few things that need official approval for legal or process reasons (e.g. a release or a new committer). Most of the time we work with the consensus building techniques documented below. Lazy Consensus Lazy consensus is the first, and possibly the most important, consensus building tool we have. Essentially lazy consensus means that you do not need to get explicit approval to proceed, but you need to be prepared to listen if someone objects. Consensus Building Sometimes lazy consensus is not appropriate. In such cases it is necessary to make a proposal to the mailing list and discuss options. There are mechanisms for quickly showing your support or otherwise for a proposal and building consensus amongst the community. Once there is a consensus people can proceed with the work under the lazy consensus model. Voting Occasionally a \"feel\" for consensus is not enough. Sometimes we need to have a measurable consensus. For example, when voting in new committers or to approve a release.","title":"Overview"},{"location":"decision-making/#decision-making","text":"The most important thing about engaging with Opencast is that all people with an opinion are entitled to express that opinion and, where appropriate, have it considered by the community. To some, the idea of having to establish consensus in a large and distributed team sounds inefficient and frustrating. Do not despair though, we have a set of simple processes to ensure things proceed at a good pace. In Opencast we do not like to vote. We reserve that for the few things that need official approval for legal or process reasons (e.g. a release or a new committer). Most of the time we work with the consensus building techniques documented below.","title":"Decision Making"},{"location":"decision-making/#lazy-consensus","text":"Lazy consensus is the first, and possibly the most important, consensus building tool we have. Essentially lazy consensus means that you do not need to get explicit approval to proceed, but you need to be prepared to listen if someone objects.","title":"Lazy Consensus"},{"location":"decision-making/#consensus-building","text":"Sometimes lazy consensus is not appropriate. In such cases it is necessary to make a proposal to the mailing list and discuss options. There are mechanisms for quickly showing your support or otherwise for a proposal and building consensus amongst the community. Once there is a consensus people can proceed with the work under the lazy consensus model.","title":"Consensus Building"},{"location":"decision-making/#voting","text":"Occasionally a \"feel\" for consensus is not enough. Sometimes we need to have a measurable consensus. For example, when voting in new committers or to approve a release.","title":"Voting"},{"location":"decision-making/consensus-building/","text":"Consensus Building In some cases there is no obvious path to take or you might want to have reassurance for the path you want to take. In these cases people will often need to build consensus by making proposals and eliciting responses. We want to avoid unnecessary discussion and the creation of significant amounts of unnecessary mail that everyone in the community needs to read. That is not to say that we want to avoid constructive discussion. This is the lifeblood of a successful project. However, there is a shorthand notation for showing support, or otherwise, for a proposal. Sending out a Proposal Proposals should be send to list. It is common to indicate proposals by including the string [#proposal] at the beginning of the subject to make it easier for the community to identify that the mail contains an important proposal. The list used should usually be the development list. In very rare cases, other lists may also be used: The users list when the matter discussed only targets adopters/uses. The committers list if the matter discussed needs to be kept private e.g. for personal or security reasons. Please avoid cross-posting a proposal on multiple lists. It will usually fracture the discussion. Expressing Approval or Disapproval First of all, it is important to understand that everyone is invited to express their opinion of any given action or proposal. Opencast is a community project in which no single individual has more power than any other single individual (except in a very few procedural situations). The notation used is +1 , -1 and 0 . It is also common to see +0 and -0 . So, what do these notations mean? +1 means \"I agree with this and will help make it happen\" +0 means \"I agree with this but probably won't make it happen, so my opinion is not that important\" -0 means \"I don't agree with this, but I'm offering no alternative so my opinion is not that important\" -1 means \"I don't agree and I am offering an alternative that I am able to help implement\" Many people will use fractions to indicate the strength of their feelings, e.g. +0.5 . Some will even indicate this is a definite yes with something like +1000 . The important thing is that this is not an exact science. It is just a shorthand way of communicating strength of feeling. Consensus Building is Not Voting The confusing thing about this notation is that it is the same notation used in a formal vote . Knowing when something is a vote and when it is a preference is important. It is easy to tell though, if the email's subject does not start with [#vote] then it is just an opinion. We try not to call votes, consensus building is much more inclusive. The reasons for this notation being common is that when someone wants to summarise a discussion thread they can mentally add up the strength of feeling of the community and decide if there is consensus or not. Once there is a clear consensus members of the community can proceed with the work under the lazy consensus model.","title":"Consensus Building"},{"location":"decision-making/consensus-building/#consensus-building","text":"In some cases there is no obvious path to take or you might want to have reassurance for the path you want to take. In these cases people will often need to build consensus by making proposals and eliciting responses. We want to avoid unnecessary discussion and the creation of significant amounts of unnecessary mail that everyone in the community needs to read. That is not to say that we want to avoid constructive discussion. This is the lifeblood of a successful project. However, there is a shorthand notation for showing support, or otherwise, for a proposal.","title":"Consensus Building"},{"location":"decision-making/consensus-building/#sending-out-a-proposal","text":"Proposals should be send to list. It is common to indicate proposals by including the string [#proposal] at the beginning of the subject to make it easier for the community to identify that the mail contains an important proposal. The list used should usually be the development list. In very rare cases, other lists may also be used: The users list when the matter discussed only targets adopters/uses. The committers list if the matter discussed needs to be kept private e.g. for personal or security reasons. Please avoid cross-posting a proposal on multiple lists. It will usually fracture the discussion.","title":"Sending out a Proposal"},{"location":"decision-making/consensus-building/#expressing-approval-or-disapproval","text":"First of all, it is important to understand that everyone is invited to express their opinion of any given action or proposal. Opencast is a community project in which no single individual has more power than any other single individual (except in a very few procedural situations). The notation used is +1 , -1 and 0 . It is also common to see +0 and -0 . So, what do these notations mean? +1 means \"I agree with this and will help make it happen\" +0 means \"I agree with this but probably won't make it happen, so my opinion is not that important\" -0 means \"I don't agree with this, but I'm offering no alternative so my opinion is not that important\" -1 means \"I don't agree and I am offering an alternative that I am able to help implement\" Many people will use fractions to indicate the strength of their feelings, e.g. +0.5 . Some will even indicate this is a definite yes with something like +1000 . The important thing is that this is not an exact science. It is just a shorthand way of communicating strength of feeling.","title":"Expressing Approval or Disapproval"},{"location":"decision-making/consensus-building/#consensus-building-is-not-voting","text":"The confusing thing about this notation is that it is the same notation used in a formal vote . Knowing when something is a vote and when it is a preference is important. It is easy to tell though, if the email's subject does not start with [#vote] then it is just an opinion. We try not to call votes, consensus building is much more inclusive. The reasons for this notation being common is that when someone wants to summarise a discussion thread they can mentally add up the strength of feeling of the community and decide if there is consensus or not. Once there is a clear consensus members of the community can proceed with the work under the lazy consensus model.","title":"Consensus Building is Not Voting"},{"location":"decision-making/lazy-consensus/","text":"Lazy Consensus The concept of \"Lazy Consensus\" is very important in our project. Lazy Consensus means that when you are convinced that you know what the community would like to see happen you can simply assume that you already have consensus and get on with the work. You do not have to insist people discuss and/or approve your plan, and you certainly do not need to call a vote to get approval. You just assume you have the community's support unless someone says otherwise. We have a time machine (git), this means that as long as you commit (or submit patches) early and often the community has plenty of opportunity to indicate disapproval. If you believe the community will support your action you can operate on lazy consensus as long as you are prepared to roll back any work should a valid objection be raised. Avoiding Unnecessary Discussion The key thing about lazy consensus is that it is easier for people to agree, by doing nothing, than it is to object, which requires an alternative to be proposed. This has two effects, firstly people are less likely to object for the sake of it and secondly it cuts down on the amount of unnecessary mail traffic and discussion. Lazy consensus means we can avoid waiting for a community based decision before proceeding. However, it does require everyone who cares for the health of the project to watch what is happening, as it is happening. Objecting too far down the road will cause upset, but objecting (or asking for clarification of intent) early is likely to be greeted with relief that someone is watching and cares. Stating Lazy Consensus Sometimes a member of the community will believe a specific action is the correct one for the project but is not sure that there will be consensus and may not wish to proceed the work without giving the community an opportunity to feedback. In these circumstances, they can make the proposal and state Lazy Consensus is in operation. Proposals should be sent to list, preferably the development list. It is common to indicate proposals by including the string [#proposal] at the beginning of the subject to make it easier for the community to identify that the mail contains an important proposal. This triggers the Lazy Consensus mechanism, by which the proposal is considered accepted if no one objects within 72 hours after the proposal submission. The period of 72 hours is chosen because it accounts for different timezones and any non-Opencast commitments the community members may have. In this approach the original proposer is not insisting that there is a discussion around the proposal, nor is it requested that the community explicitly supports their actions. This differs from assuming lazy consensus since it allows space and time to express support or objections and corrections to the proposal before work begins. Silence is Consent People may choose to indicate their support for the actions taken with a +1 mail - quick and easy to read and reassuring for the implementer. However, remember, in a lazy consensus world silence is the equivalent to support. This can take some time to get used to.","title":"Lazy Consensus"},{"location":"decision-making/lazy-consensus/#lazy-consensus","text":"The concept of \"Lazy Consensus\" is very important in our project. Lazy Consensus means that when you are convinced that you know what the community would like to see happen you can simply assume that you already have consensus and get on with the work. You do not have to insist people discuss and/or approve your plan, and you certainly do not need to call a vote to get approval. You just assume you have the community's support unless someone says otherwise. We have a time machine (git), this means that as long as you commit (or submit patches) early and often the community has plenty of opportunity to indicate disapproval. If you believe the community will support your action you can operate on lazy consensus as long as you are prepared to roll back any work should a valid objection be raised.","title":"Lazy Consensus"},{"location":"decision-making/lazy-consensus/#avoiding-unnecessary-discussion","text":"The key thing about lazy consensus is that it is easier for people to agree, by doing nothing, than it is to object, which requires an alternative to be proposed. This has two effects, firstly people are less likely to object for the sake of it and secondly it cuts down on the amount of unnecessary mail traffic and discussion. Lazy consensus means we can avoid waiting for a community based decision before proceeding. However, it does require everyone who cares for the health of the project to watch what is happening, as it is happening. Objecting too far down the road will cause upset, but objecting (or asking for clarification of intent) early is likely to be greeted with relief that someone is watching and cares.","title":"Avoiding Unnecessary Discussion"},{"location":"decision-making/lazy-consensus/#stating-lazy-consensus","text":"Sometimes a member of the community will believe a specific action is the correct one for the project but is not sure that there will be consensus and may not wish to proceed the work without giving the community an opportunity to feedback. In these circumstances, they can make the proposal and state Lazy Consensus is in operation. Proposals should be sent to list, preferably the development list. It is common to indicate proposals by including the string [#proposal] at the beginning of the subject to make it easier for the community to identify that the mail contains an important proposal. This triggers the Lazy Consensus mechanism, by which the proposal is considered accepted if no one objects within 72 hours after the proposal submission. The period of 72 hours is chosen because it accounts for different timezones and any non-Opencast commitments the community members may have. In this approach the original proposer is not insisting that there is a discussion around the proposal, nor is it requested that the community explicitly supports their actions. This differs from assuming lazy consensus since it allows space and time to express support or objections and corrections to the proposal before work begins.","title":"Stating Lazy Consensus"},{"location":"decision-making/lazy-consensus/#silence-is-consent","text":"People may choose to indicate their support for the actions taken with a +1 mail - quick and easy to read and reassuring for the implementer. However, remember, in a lazy consensus world silence is the equivalent to support. This can take some time to get used to.","title":"Silence is Consent"},{"location":"decision-making/voting/","text":"Voting Occasionally a feel for consensus is not enough. Sometimes we need to have a measurable consensus. For example, when voting new committers in or to approve a release. Preparing for a Vote Before calling a vote it is important to ensure that the community is given time to discuss the upcoming vote. This will be done by posting an email to the developers list indicating the intention to call a vote and the options available. By the time a vote is called there should already be consensus in the community. The vote itself is, normally, a formality. Calling a Vote Once it is time to call the vote, a mail is posted to the committers list with a subject starting with [#vote] . This enables the community members to ensure they do not miss an important vote thread. It also indicates that this is not consensus building but a formal vote. Casting Your Vote The notation used in voting is: +1 Yes I agree 0 I have no strong opinion -1 I object on the following grounds\u2026 If you object you must support your objection and provide an alternative course of action that you are willing and able to implement (where appropriate). Votes should generally be permitted to run for at least 72 hours to provide an opportunity for all concerned persons to participate regardless of their geographic locations. Publishing Results After the voting is done, the outcome should be published on the public developer list. A result may be kept private, if deemed necessary, for votes on security-relevant or personal topics. Binding Votes In Opencast, only committers have binding votes. All others are either discouraged from voting (to keep the noise down) or else have their votes considered in an advisory nature only. When to Vote There are essentially three occasions to vote: Releases Changes to the Committer body Significant changes to the development process Veto and Majority By default, all committers have a veto right when voting, meaning that a -1 will effectively stop whatever was voted for. After addressing the issue, a second vote may be called for. Depending on the discussion, at this point, the initiator may determine this to be a majority vote. In the rare case that a majority vote among committers is called for, the vote is a simple majority vote among participating committers without regards to any form of quorum. The vote will pass if there were more positive ( +1 ) votes then negative ones ( -1 ), implying that at least one committer needs to respond. Formal Votes for Code Changes Usually, code changes have consensus and there should be no need for voting. If in doubt, contributors can propose changes on list in advance. Additionally, consensus may be reached though discussion as part of the review process. There might be the rare case of a dispute between reviewer(s) and contributor(s) during the review process which cannot be resolved easily. In such a case, both parties can call a formal majority vote to settle the issue.","title":"Voting"},{"location":"decision-making/voting/#voting","text":"Occasionally a feel for consensus is not enough. Sometimes we need to have a measurable consensus. For example, when voting new committers in or to approve a release.","title":"Voting"},{"location":"decision-making/voting/#preparing-for-a-vote","text":"Before calling a vote it is important to ensure that the community is given time to discuss the upcoming vote. This will be done by posting an email to the developers list indicating the intention to call a vote and the options available. By the time a vote is called there should already be consensus in the community. The vote itself is, normally, a formality.","title":"Preparing for a Vote"},{"location":"decision-making/voting/#calling-a-vote","text":"Once it is time to call the vote, a mail is posted to the committers list with a subject starting with [#vote] . This enables the community members to ensure they do not miss an important vote thread. It also indicates that this is not consensus building but a formal vote.","title":"Calling a Vote"},{"location":"decision-making/voting/#casting-your-vote","text":"The notation used in voting is: +1 Yes I agree 0 I have no strong opinion -1 I object on the following grounds\u2026 If you object you must support your objection and provide an alternative course of action that you are willing and able to implement (where appropriate). Votes should generally be permitted to run for at least 72 hours to provide an opportunity for all concerned persons to participate regardless of their geographic locations.","title":"Casting Your Vote"},{"location":"decision-making/voting/#publishing-results","text":"After the voting is done, the outcome should be published on the public developer list. A result may be kept private, if deemed necessary, for votes on security-relevant or personal topics.","title":"Publishing Results"},{"location":"decision-making/voting/#binding-votes","text":"In Opencast, only committers have binding votes. All others are either discouraged from voting (to keep the noise down) or else have their votes considered in an advisory nature only.","title":"Binding Votes"},{"location":"decision-making/voting/#when-to-vote","text":"There are essentially three occasions to vote: Releases Changes to the Committer body Significant changes to the development process","title":"When to Vote"},{"location":"decision-making/voting/#veto-and-majority","text":"By default, all committers have a veto right when voting, meaning that a -1 will effectively stop whatever was voted for. After addressing the issue, a second vote may be called for. Depending on the discussion, at this point, the initiator may determine this to be a majority vote. In the rare case that a majority vote among committers is called for, the vote is a simple majority vote among participating committers without regards to any form of quorum. The vote will pass if there were more positive ( +1 ) votes then negative ones ( -1 ), implying that at least one committer needs to respond.","title":"Veto and Majority"},{"location":"decision-making/voting/#formal-votes-for-code-changes","text":"Usually, code changes have consensus and there should be no need for voting. If in doubt, contributors can propose changes on list in advance. Additionally, consensus may be reached though discussion as part of the review process. There might be the rare case of a dispute between reviewer(s) and contributor(s) during the review process which cannot be resolved easily. In such a case, both parties can call a formal majority vote to settle the issue.","title":"Formal Votes for Code Changes"},{"location":"infrastructure/","text":"Opencast Infrastructure List of Opencast project infrastructure and administrators. For detailed notes go here Infrastructure Test Servers Institution Hostname Admin (Software) Admin (Hardware) University of Osnabr\u00fcck develop.opencast.org Lars Kiesow Lars Kiesow ETH Z\u00fcrich stable.opencast.org Lars Kiesow Waldemar Smirnow Technische Universit\u00e4t Ilmenau legacy.opencast.org Lars Kiesow Daniel Konrad CI Servers Institution Hostname Admin (Software) Admin (Hardware) University of Cologne oc-com-admin.rrz.uni-koeln.de Greg Logan Ruth Lang University of Cologne oc-com-worker1.rrz.uni-koeln.de Greg Logan Ruth Lang University of Cologne oc-com-presentation.rrz.uni-koeln.de Greg Logan Ruth Lang Maven Repository Institution Hostname Admin (Software) Admin (Hardware) Notes Harvard DCE mvncache.opencast.org Lars Kiesow DCE Devel group Nginx cache, AWS University of Osnabr\u00fcck nexus.opencast.org Lars Kiesow Lars Kiesow nexus-oss Nexus administration: Greg Logan Lars Kiesow Other Hosted Services Institution Hostname Admin (Software) Admin (Hardware) University of Osnabr\u00fcck pkg.opencast.org Lars Kiesow Lars Kiesow University of Cologne ci.opencast.org Greg Logan Ruth Lang University of Osnabr\u00fcck docs.opencast.org Lars Kiesow Lars Kiesow University of Stuttgart testrailoc.tik.uni-stuttgart.de Release managers Per Pascal Grube Administrators What is an administractor, and how does that differ from a committer? An administrator is someone within the Opencast community who has administrative access to one or more of our major tools. These tools are GitHub Google Groups Crowdin While many of our administrators are committers, an administrator is not a committer by necessity. Administrators have important responsibilities within the community, but mainly work behind the scenes. These responsibilities include: Adding new committers to the relevant group(s) Removing old committers from the relevant group(s) Contacting support when required for hosted projects (Atlassian, Crowdin, Google) Adding or removing Committers While the committer body manages its own membership, it does not directly have the power to add or remove users from the appropriate groups across all of our hosted products. Administrators are required to modify the various groups in multiple places when a change is necessary. These changes are Modifying the GitHub committers group upon request Modifying the Google committers group Modifying the Crowdin committers group Modifying the list of committers on the Opencast website Current Administrators Administrators may not have complete access to all services, however we will coordinate to handle requests in a timely manner. If you need to contact an administrator for access to one of the services above, please contact them in this order: Greg Logan Lars Kiesow Olaf Schulte Video Conferencing BigBlueButton Conference rooms Password: welcome Recordings Other Services Other services and the primary contact for them: Google Greg Logan YouTube Greg Logan and ETH Staff Twitter Lars Kiesow Facebook R\u00fcdiger Rolf","title":"Overview"},{"location":"infrastructure/#opencast-infrastructure","text":"List of Opencast project infrastructure and administrators. For detailed notes go here","title":"Opencast Infrastructure"},{"location":"infrastructure/#infrastructure","text":"","title":"Infrastructure"},{"location":"infrastructure/#test-servers","text":"Institution Hostname Admin (Software) Admin (Hardware) University of Osnabr\u00fcck develop.opencast.org Lars Kiesow Lars Kiesow ETH Z\u00fcrich stable.opencast.org Lars Kiesow Waldemar Smirnow Technische Universit\u00e4t Ilmenau legacy.opencast.org Lars Kiesow Daniel Konrad","title":"Test Servers"},{"location":"infrastructure/#ci-servers","text":"Institution Hostname Admin (Software) Admin (Hardware) University of Cologne oc-com-admin.rrz.uni-koeln.de Greg Logan Ruth Lang University of Cologne oc-com-worker1.rrz.uni-koeln.de Greg Logan Ruth Lang University of Cologne oc-com-presentation.rrz.uni-koeln.de Greg Logan Ruth Lang","title":"CI Servers"},{"location":"infrastructure/#maven-repository","text":"Institution Hostname Admin (Software) Admin (Hardware) Notes Harvard DCE mvncache.opencast.org Lars Kiesow DCE Devel group Nginx cache, AWS University of Osnabr\u00fcck nexus.opencast.org Lars Kiesow Lars Kiesow nexus-oss Nexus administration: Greg Logan Lars Kiesow","title":"Maven Repository"},{"location":"infrastructure/#other-hosted-services","text":"Institution Hostname Admin (Software) Admin (Hardware) University of Osnabr\u00fcck pkg.opencast.org Lars Kiesow Lars Kiesow University of Cologne ci.opencast.org Greg Logan Ruth Lang University of Osnabr\u00fcck docs.opencast.org Lars Kiesow Lars Kiesow University of Stuttgart testrailoc.tik.uni-stuttgart.de Release managers Per Pascal Grube","title":"Other Hosted Services"},{"location":"infrastructure/#administrators","text":"","title":"Administrators"},{"location":"infrastructure/#what-is-an-administractor-and-how-does-that-differ-from-a-committer","text":"An administrator is someone within the Opencast community who has administrative access to one or more of our major tools. These tools are GitHub Google Groups Crowdin While many of our administrators are committers, an administrator is not a committer by necessity. Administrators have important responsibilities within the community, but mainly work behind the scenes. These responsibilities include: Adding new committers to the relevant group(s) Removing old committers from the relevant group(s) Contacting support when required for hosted projects (Atlassian, Crowdin, Google)","title":"What is an administractor, and how does that differ from a committer?"},{"location":"infrastructure/#adding-or-removing-committers","text":"While the committer body manages its own membership, it does not directly have the power to add or remove users from the appropriate groups across all of our hosted products. Administrators are required to modify the various groups in multiple places when a change is necessary. These changes are Modifying the GitHub committers group upon request Modifying the Google committers group Modifying the Crowdin committers group Modifying the list of committers on the Opencast website","title":"Adding or removing Committers"},{"location":"infrastructure/#current-administrators","text":"Administrators may not have complete access to all services, however we will coordinate to handle requests in a timely manner. If you need to contact an administrator for access to one of the services above, please contact them in this order: Greg Logan Lars Kiesow Olaf Schulte","title":"Current Administrators"},{"location":"infrastructure/#video-conferencing","text":"BigBlueButton Conference rooms Password: welcome Recordings","title":"Video Conferencing"},{"location":"infrastructure/#other-services","text":"Other services and the primary contact for them: Google Greg Logan YouTube Greg Logan and ETH Staff Twitter Lars Kiesow Facebook R\u00fcdiger Rolf","title":"Other Services"},{"location":"infrastructure/maven-repository/","text":"Opencast Maven Repository The Maven repository server maintains a copy of all the Java dependencies used by Opencast. Adding Libraries To The Repository Login as an administrator on the Opencast Nexus Master Select repository Select the artifact upload tab Fill in the details and upload the file Setting-up Another Maven Repository Having a repository server run in your local network can significantly improve the speed artifacts are retrieved while building Opencast. Docker There is a preconfigured Docker image for a Nexus server set-up for Opencast. To run an Opencast Nexus using Docker, follow these steps: docker run \\ --name mvncache \\ -p 8000:8000 \\ docker.io/lkiesow/opencast-maven-repository The -p option will map the internal port of the server in Docker to the port on the host machine. Prefer a Specific Repository If you did set-up a local repository or just want to select a specific global repository by default, you can use a custom Maven configuration. To do that, create asettings file in ~/.m2/settings.xml like this: <settings xmlns=\"http://maven.apache.org/SETTINGS/1.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd\"> <mirrors> <mirror> <id>opencast-osna</id> <name>Osnabr\u00fcck Opencast Repository</name> <url>https://nexus.opencast.org/nexus/content/groups/public</url> <mirrorOf>opencast</mirrorOf> </mirror> </mirrors> </settings> This example would add a mirror for the primary Opencast Maven repository, causing the Osnabr\u00fcck repository to be the preferred repository to use. You can find some example configurations in docs/maven/ . Pushing artifacts to a Maven repository Pushing to your local Maven repository The following command will add a file to your local Maven repository. This is useful for testing if your artifacts are correctly placed prior to pushing to the mainline Nexus repository. mvn install:install-file \\ -Dfile=$filename \\ -DgroupId=$groupId \\ -DartifactId=$artifactId \\ -Dpackaging=$packaging \\ -Dversion=$version \\ -DgeneratePom=$generatePom Variable Map Variable What it does Example filename The path to the local file you want in your repository audio.mp2 groupId The Opencast group ID org.opencastproject artifactId The artifact ID. This is the name of the artifact according to Maven audio packaging The file type (effectively), this should match the filename's extension mp2 version The artifact's version 1.1 generatePom Whether or not to generate a pom file automatically true Pushing to Maven Central Opencast previously hosted our own Maven repository at nexus.opencast.org, however starting with Opencast 6.6 we are transitioning to using Maven Central. There are a few steps prior to being able to push to Sonatype's repo: Create a GPG key, and push the public key to a key server Sign up for an account on Sonatype's JIRA instance Let the QA Coordinator know about your user, they will comment on our repo creation ticket or create a new issue to give your user permissions Put the following in your .m2/settings.xml file <settings> <servers> <server> <id>ossrh</id> <username>$username</username> <password>$password</password> </server> ... </servers> <profiles> <profile> <id>ossrh</id> <activation> <activeByDefault>true</activeByDefault> </activation> <properties> <gpg.keyname>$gpgKeyId</gpg.keyname> </properties> </profile> ... </profiles> </settings> Pushing Snapshots Snapshots are pushed automatically by the CI servers. For historical purposes, this is accomplished by: mvn deploy To verify, your artifacts can be found here and here . Note that you cannot (easily) drop bad snapshots. Instead, fix it and redeploy! Pushing Releases Note: Please read this section entirely before running any commands. Maven Central does not allow you to change a release once it has been closed! Pushing releases is similar to snapshots, with the added requirements that you also push: Javadocs Sources GPG signatures for the binaries, docs, and sources This is automated with the release profile. To push a release run mvn nexus-staging:deploy -P release This creates a staging repository (https://oss.sonatype.org/content/groups/staging/org/opencastproject/) for your artifacts. This is always safe to do - you can still rollback all changes with mvn nexus-staging:drop If things do not look ok, fix the issue and redeploy. Once you are confident that everything is ok, you can run mvn nexus-staging:close This closes the staging repository, and runs the Sonatype-side tests for things like GPG signatures. If this fails, correct the issue locally, and redeploy. Once this succeeds, you have two options: drop (to destroy the release) or: mvn nexus-staging:release to permanently release the binaries in their current states. Troubleshooting Sometimes the deploy or close will fail, timing out after 5 minutes waiting for Sonatype. It will complain about violations of deploy rules - this may or may not actually be true. If you're confident that this is caused by a simple timeout and not something you have done use one of the following. To reattempt a deploy use mvn nexus-staging:deploy-staged This will avoid recompiling, retesting, and resigning all of the binaries. To reattempt a close use mvn nexus-staging:close","title":"Maven Repository"},{"location":"infrastructure/maven-repository/#opencast-maven-repository","text":"The Maven repository server maintains a copy of all the Java dependencies used by Opencast.","title":"Opencast Maven Repository"},{"location":"infrastructure/maven-repository/#adding-libraries-to-the-repository","text":"Login as an administrator on the Opencast Nexus Master Select repository Select the artifact upload tab Fill in the details and upload the file","title":"Adding Libraries To The Repository"},{"location":"infrastructure/maven-repository/#setting-up-another-maven-repository","text":"Having a repository server run in your local network can significantly improve the speed artifacts are retrieved while building Opencast.","title":"Setting-up Another Maven Repository"},{"location":"infrastructure/maven-repository/#docker","text":"There is a preconfigured Docker image for a Nexus server set-up for Opencast. To run an Opencast Nexus using Docker, follow these steps: docker run \\ --name mvncache \\ -p 8000:8000 \\ docker.io/lkiesow/opencast-maven-repository The -p option will map the internal port of the server in Docker to the port on the host machine.","title":"Docker"},{"location":"infrastructure/maven-repository/#prefer-a-specific-repository","text":"If you did set-up a local repository or just want to select a specific global repository by default, you can use a custom Maven configuration. To do that, create asettings file in ~/.m2/settings.xml like this: <settings xmlns=\"http://maven.apache.org/SETTINGS/1.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd\"> <mirrors> <mirror> <id>opencast-osna</id> <name>Osnabr\u00fcck Opencast Repository</name> <url>https://nexus.opencast.org/nexus/content/groups/public</url> <mirrorOf>opencast</mirrorOf> </mirror> </mirrors> </settings> This example would add a mirror for the primary Opencast Maven repository, causing the Osnabr\u00fcck repository to be the preferred repository to use. You can find some example configurations in docs/maven/ .","title":"Prefer a Specific Repository"},{"location":"infrastructure/maven-repository/#pushing-artifacts-to-a-maven-repository","text":"","title":"Pushing artifacts to a Maven repository"},{"location":"infrastructure/maven-repository/#pushing-to-your-local-maven-repository","text":"The following command will add a file to your local Maven repository. This is useful for testing if your artifacts are correctly placed prior to pushing to the mainline Nexus repository. mvn install:install-file \\ -Dfile=$filename \\ -DgroupId=$groupId \\ -DartifactId=$artifactId \\ -Dpackaging=$packaging \\ -Dversion=$version \\ -DgeneratePom=$generatePom Variable Map Variable What it does Example filename The path to the local file you want in your repository audio.mp2 groupId The Opencast group ID org.opencastproject artifactId The artifact ID. This is the name of the artifact according to Maven audio packaging The file type (effectively), this should match the filename's extension mp2 version The artifact's version 1.1 generatePom Whether or not to generate a pom file automatically true","title":"Pushing to your local Maven repository"},{"location":"infrastructure/maven-repository/#pushing-to-maven-central","text":"Opencast previously hosted our own Maven repository at nexus.opencast.org, however starting with Opencast 6.6 we are transitioning to using Maven Central. There are a few steps prior to being able to push to Sonatype's repo: Create a GPG key, and push the public key to a key server Sign up for an account on Sonatype's JIRA instance Let the QA Coordinator know about your user, they will comment on our repo creation ticket or create a new issue to give your user permissions Put the following in your .m2/settings.xml file <settings> <servers> <server> <id>ossrh</id> <username>$username</username> <password>$password</password> </server> ... </servers> <profiles> <profile> <id>ossrh</id> <activation> <activeByDefault>true</activeByDefault> </activation> <properties> <gpg.keyname>$gpgKeyId</gpg.keyname> </properties> </profile> ... </profiles> </settings>","title":"Pushing to Maven Central"},{"location":"infrastructure/maven-repository/#pushing-snapshots","text":"Snapshots are pushed automatically by the CI servers. For historical purposes, this is accomplished by: mvn deploy To verify, your artifacts can be found here and here . Note that you cannot (easily) drop bad snapshots. Instead, fix it and redeploy!","title":"Pushing Snapshots"},{"location":"infrastructure/maven-repository/#pushing-releases","text":"Note: Please read this section entirely before running any commands. Maven Central does not allow you to change a release once it has been closed! Pushing releases is similar to snapshots, with the added requirements that you also push: Javadocs Sources GPG signatures for the binaries, docs, and sources This is automated with the release profile. To push a release run mvn nexus-staging:deploy -P release This creates a staging repository (https://oss.sonatype.org/content/groups/staging/org/opencastproject/) for your artifacts. This is always safe to do - you can still rollback all changes with mvn nexus-staging:drop If things do not look ok, fix the issue and redeploy. Once you are confident that everything is ok, you can run mvn nexus-staging:close This closes the staging repository, and runs the Sonatype-side tests for things like GPG signatures. If this fails, correct the issue locally, and redeploy. Once this succeeds, you have two options: drop (to destroy the release) or: mvn nexus-staging:release to permanently release the binaries in their current states.","title":"Pushing Releases"},{"location":"infrastructure/maven-repository/#troubleshooting","text":"Sometimes the deploy or close will fail, timing out after 5 minutes waiting for Sonatype. It will complain about violations of deploy rules - this may or may not actually be true. If you're confident that this is caused by a simple timeout and not something you have done use one of the following. To reattempt a deploy use mvn nexus-staging:deploy-staged This will avoid recompiling, retesting, and resigning all of the binaries. To reattempt a close use mvn nexus-staging:close","title":"Troubleshooting"},{"location":"infrastructure/notes/","text":"Infrastructure Notes This page contains notes about the current configuration of the Opencast servers around the world Harvard DCE Common Configuration Choices Unattended upgrade CentOS Linux release 7.x nexus.dcex.harvard.edu Using packaged Nexus ETH opencast-nexus.ethz.ch Unattended upgrade RHEL 7.x Using packaged Nexus University of Cologne Common Configuration Choices Unattended upgrade CentOS Linux release 7.x ci.opencast.org Buildbot installed and managed via ansible script in this repo University of Osnabr\u00fcck Common Configuration Choices Unattended upgrade Scientific Linux 7.x build.opencast.org Builds are triggered by cron, manual branch selection nexus.opencast.org, nexus.virtuos.uos.de GeoIP based redirect for all Nexus servers Using packaged Nexus octestallinone.virtuos.uos.de Using tarballs build from build.opencast.org repo.opencast.org, pkg.opencast.org, pullrequests.opencast.org Same server","title":"Notes"},{"location":"infrastructure/notes/#infrastructure-notes","text":"This page contains notes about the current configuration of the Opencast servers around the world","title":"Infrastructure Notes"},{"location":"infrastructure/notes/#harvard-dce","text":"","title":"Harvard DCE"},{"location":"infrastructure/notes/#common-configuration-choices","text":"Unattended upgrade CentOS Linux release 7.x","title":"Common Configuration Choices"},{"location":"infrastructure/notes/#nexusdcexharvardedu","text":"Using packaged Nexus","title":"nexus.dcex.harvard.edu"},{"location":"infrastructure/notes/#eth","text":"","title":"ETH"},{"location":"infrastructure/notes/#opencast-nexusethzch","text":"Unattended upgrade RHEL 7.x Using packaged Nexus","title":"opencast-nexus.ethz.ch"},{"location":"infrastructure/notes/#university-of-cologne","text":"","title":"University of Cologne"},{"location":"infrastructure/notes/#common-configuration-choices_1","text":"Unattended upgrade CentOS Linux release 7.x","title":"Common Configuration Choices"},{"location":"infrastructure/notes/#ciopencastorg","text":"Buildbot installed and managed via ansible script in this repo","title":"ci.opencast.org"},{"location":"infrastructure/notes/#university-of-osnabruck","text":"","title":"University of Osnabr\u00fcck"},{"location":"infrastructure/notes/#common-configuration-choices_2","text":"Unattended upgrade Scientific Linux 7.x","title":"Common Configuration Choices"},{"location":"infrastructure/notes/#buildopencastorg","text":"Builds are triggered by cron, manual branch selection","title":"build.opencast.org"},{"location":"infrastructure/notes/#nexusopencastorg-nexusvirtuosuosde","text":"GeoIP based redirect for all Nexus servers Using packaged Nexus","title":"nexus.opencast.org, nexus.virtuos.uos.de"},{"location":"infrastructure/notes/#octestallinonevirtuosuosde","text":"Using tarballs build from build.opencast.org","title":"octestallinone.virtuos.uos.de"},{"location":"infrastructure/notes/#repoopencastorg-pkgopencastorg-pullrequestsopencastorg","text":"Same server","title":"repo.opencast.org, pkg.opencast.org, pullrequests.opencast.org"},{"location":"installation/source-linux/","text":"Developer Installation guide These instructions outline how to install Opencast. This is meant for developers. For the installation of a production cluster, take a look at the admin guides. TL;DR $ git clone https://github.com/opencast/opencast.git $ cd opencast $ mvn clean install -Pdev $ cd build/opencast-dist-develop-* $ ./bin/start-opencast Opencast will then listen on 127.0.0.1:8080 Default credentials are: username: admin password: opencast Configuring Git $ git config --global user.name \"Example Name\" $ git config --global user.email example@domain.com $ ssh-keygen -t ed25519 -C \"Example Name <example@domain.com>\" $ cat ~/.ssh/id_ed25519.pub Go to: Github , click \"New SSH Key\" and paste your content of id_rsa.pub into the input field. It should look like: ssh-ed25519 at9/q0tR69TqQvwnFZuat90k0PY+z7mTyLB7UZXDnmpNHkU/MzO... Now press \"Add SSH Key\" and return to your terminal and: $ ssh -T git@github.com Clone Opencast You can get the Opencast source code by cloning the Git repository. Cloning the Git repository: $ git clone https://github.com/opencast/opencast.git Install Dependencies Please make sure to install the following dependencies. Required: java-1.8.0-openjdk-devel.x86_64 / openjdk-8-jdk (other jdk versions untested / Oracle JDK strongly not recommended) ffmpeg >= 3.2.4 maven >= 3.1 python firefox/chrome/some other major browser unzip gcc-c++ tar bzip2 Required as a service for running Opencast: ActiveMQ >= 5.10 elasticsearch = 7.9.x Required for some services. Some tests may be skipped and some features may not be usable if they are not installed. Hence, it's generally a good idea to install them. tesseract >= 3 hunspell >= 1.2.8 sox >= 14.4 synfig Ubuntu 18.04 Update System $ apt update $ apt upgrade -y Install Packages via APT $ apt install -y git openjdk-8-jdk maven gcc g++ build-essential cmake curl sox hunspell synfig ffmpeg Install NodeJS (optional) $ curl -sL https://deb.nodesource.com/setup_10.x | sudo -E bash - $ sudo apt-get install -y nodejs Install and start Elasticsearch and ActiveMQ with Docker You can use docker-compose to easily run both ActiveMQ and Elasticsearch: $ cd docs/scripts/devel-dependency-containers $ docker-compose up -d To shut the services down ahain, run: $ cd docs/scripts/devel-dependency-containers $ docker-compose down Set System Java JDK Choose the Java Version 1.8.0 by entering: $ update-alternatives --config java Fedora Update System $ dnf update -y Install Dependencies $ dnf install https://download1.rpmfusion.org/free/fedora/rpmfusion-free-release-$(rpm -E %fedora).noarch.rpm https://download1.rpmfusion.org/nonfree/fedora/rpmfusion-nonfree-release-$(rpm -E %fedora).noarch.rpm -y $ dnf group install 'Development Tools' $ dnf install -y java-1.8.0-openjdk ffmpeg maven tesseract hunspell sox synfig unzip gcc-c++ tar bzip2 nodejs macOS 10.14 Update System Try to install all updates via the App Store or the Apple Icon on the top left corner. Java JDK 8 Install the JDK 8 by downloading it from https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html XCode Install XCode over the App Store. It will be needed for building and for git. Install Packages via Homebrew The Homebrew Project adds a package manager to Mac OS. You can install it by: $ /usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" You can now install needed packages: $ brew install maven ffmpeg nodejs ActiveMQ with Homebrew Homebrew offers you an ActiveMQ Package. Please decide, if you want to use Homebrew for ActiveMQ or if you want to follow the general guide below and run it by downloading the binaries. If you want continue you can install ActiveMQ by $ brew install activemq Remember to copy the activemq.xml like mentioned below in the right directory. You have to find the right folder for that operation, Homebrew will put the ActiveMQ files in a different location. You could find it by $ sudo find / -name activemq.xml ```` After changing the configuration file you can list and start or stop you services with ```sh $ brew services list $ brew services start activemq Git Bash Completion In macOS you can not complete or suggest half typed commands with your Tab Key (like you probably know from linux). If you want to use bash completion, you have to install it by $ brew install bash-completion Find the location of the configuration file $ sudo find / -type f -name \"git-completion.bash\" Normally it should be in $ cp /Library/Developer/CommandLineTools/usr/share/git-core/git-completion.bash /usr/local/etc/bash_completion.d/ Then add following line to the bash_profile in home [ -f /usr/local/etc/bash_completion ] && . /usr/local/etc/bash_completion Finally apply your changes with $ source /usr/local/etc/bash_completion.d/git-completion.bash Install and Configure ActiveMQ Download the current version from https://activemq.apache.org/components/classic/download Extract and copy it to a directory, in this case you could use the opt directory. $ sudo tar -zxvf apache-activemq-*-bin.tar.gz -C /opt $ cd /opt && sudo mv apache-activemq-*/ activemq Copy the preconfigured XML from your opencast directory into your ActiveMQ configuration. In this example you have following folder structure: ~/Projects/opencast /opt/activemq With that folder structure you could use following command: $ cd && cd Projects && sudo cp opencast/docs/scripts/activemq/activemq.xml /opt/activemq/conf/activemq.xml If your folder structure is different from that example or you do decide to put it somewhere else, you should copy and replace the preconfigured XML from /location/to/your/opencast/docs/scripts/activemq/activemq.xml into /location/to/your/activemq/conf/activemq.xml You can start your ActiveMQ instance with: $ sudo ./location/to/your/activemq/bin/activemq start Build and Start Opencast You can build now opencast by changing your directory into your opencast location and by running: $ mvn clean install After the successfully compilation you can start opencast with: $ cd build/opencast-dist-develop-*/bin && ./start-opencast The -Pdev argument decreases the build time and skips the creation of multiple tarballs and turning on the developer tarball. $ cd opencast && mvn clean install -Pdev $ cd build/opencast-dist-develop-*/bin && ./start-opencast For further information visit Development Environment . Useful Commands for Testing Purposes For a quick build, you can use the following command to skip Opencast's tests. $ cd opencast $ mvn clean install -Pdev -DskipTests=true To see the whole stacktrace of the installation you can use the following command to disable the trimming. $ cd opencast $ mvn clean install -DtrimStackTrace=false If you want to start opencast in debug mode, you could use the debug argument: $ cd build/opencast-dist-develop-*/bin && ./start-opencast debug Modify Code and Build Changes After you modified your code you can go back to step \"Build and Start Opencast\" to rebuild Opencast. Common Build Errors or Fixes NPM Access Error To fix an npm access error ( example ), you can run $ sudo chown -R $USER:$(id -gn $USER) ~/.config && sudo chown -R $USER:$(id -gn $USER) ~/.npm JDK Version Some IDEs attempt to use the most recent version of the JDK. Make sure that your IDE is configured to use JDK 1.8.0. Waiting for ActiveMQ Opencast requires ActiveMQ to be both running and properly configured, otherwise it will wait forever to connect. See here for details on how to configure ActiveMQ. Make sure, that ActiveMQ runs without errors and with the right JAVA_HOME Variable (explained here ). Slow IDEA Fix Edit following file $ sudo nano /etc/sysctl.conf and copy this text into it fs.inotify.max_user_watches = 524288 Apply your changes with $ sudo sysctl -p --system Intellij Idea IDE Community Edition (optional) If you are currently on Fedora, you can install it with following command. Make sure, that the versions match, you probably have to change it depending on the most current version. $ cd && cd Downloads && wget https://download.jetbrains.com/idea/ideaIC-2019.2.tar.gz $ sudo tar -zxvf ideaIC-*.tar.gz -C /opt $ cd /opt && sudo mv idea-IC-*/ idea && sh /opt/idea/bin/idea.sh Otherwise install it by downloading and following the manufacturer guide, select Community Edition: IDEA Intellij Community Edition Follow the next steps, if you want to import opencast correctly Import project from external model Choose Maven Search for projects recursively Uncheck all listed profiles Check all projects to import Make sure not to select JDK 11, please select JDK 1.8.0, it should be somewhere around /usr/lib/jvm/java-1.8.0-openjdk depending on your current system Now Idea should import the projects, it could take some time, you can make it faster by following this . Import the opencast code style configuration by following the steps Go to settings Search for code style You should find it under Editor->Code Style Select Java and click on the gear icon Select Import Scheme and Intellij IDEA code style XML Import it from opencast/docs/intellij_settings/codestyle.xml Now your IDE should be ready for developing. Visual Studio Code Editor (optional) If you are currently on Fedora, you can install it with $ cd && cd Downloads && sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc && sudo sh -c 'echo -e \"[code]\\nname=Visual Studio Code\\nbaseurl=https://packages.microsoft.com/yumrepos/vscode\\nenabled=1\\ngpgcheck=1\\ngpgkey=https://packages.microsoft.com/keys/microsoft.asc\" > /etc/yum.repos.d/vscode.repo' && dnf check-update && sudo dnf install code -y Otherwise install it by downloading and following the manufacturer guide: Visual Studio Code After installation you can open a folder in bash with $ code . Recommended Extensions are ESLint by Dirk Baeumer AngularJs 1.x Code Snippets by alexandersage Debugger for Chrome by Microsoft","title":"Installation"},{"location":"installation/source-linux/#developer-installation-guide","text":"These instructions outline how to install Opencast. This is meant for developers. For the installation of a production cluster, take a look at the admin guides.","title":"Developer Installation guide"},{"location":"installation/source-linux/#tldr","text":"$ git clone https://github.com/opencast/opencast.git $ cd opencast $ mvn clean install -Pdev $ cd build/opencast-dist-develop-* $ ./bin/start-opencast Opencast will then listen on 127.0.0.1:8080 Default credentials are: username: admin password: opencast","title":"TL;DR"},{"location":"installation/source-linux/#configuring-git","text":"$ git config --global user.name \"Example Name\" $ git config --global user.email example@domain.com $ ssh-keygen -t ed25519 -C \"Example Name <example@domain.com>\" $ cat ~/.ssh/id_ed25519.pub Go to: Github , click \"New SSH Key\" and paste your content of id_rsa.pub into the input field. It should look like: ssh-ed25519 at9/q0tR69TqQvwnFZuat90k0PY+z7mTyLB7UZXDnmpNHkU/MzO... Now press \"Add SSH Key\" and return to your terminal and: $ ssh -T git@github.com","title":"Configuring Git"},{"location":"installation/source-linux/#clone-opencast","text":"You can get the Opencast source code by cloning the Git repository. Cloning the Git repository: $ git clone https://github.com/opencast/opencast.git","title":"Clone Opencast"},{"location":"installation/source-linux/#install-dependencies","text":"Please make sure to install the following dependencies. Required: java-1.8.0-openjdk-devel.x86_64 / openjdk-8-jdk (other jdk versions untested / Oracle JDK strongly not recommended) ffmpeg >= 3.2.4 maven >= 3.1 python firefox/chrome/some other major browser unzip gcc-c++ tar bzip2 Required as a service for running Opencast: ActiveMQ >= 5.10 elasticsearch = 7.9.x Required for some services. Some tests may be skipped and some features may not be usable if they are not installed. Hence, it's generally a good idea to install them. tesseract >= 3 hunspell >= 1.2.8 sox >= 14.4 synfig","title":"Install Dependencies"},{"location":"installation/source-linux/#ubuntu-1804","text":"","title":"Ubuntu 18.04"},{"location":"installation/source-linux/#update-system","text":"$ apt update $ apt upgrade -y","title":"Update System"},{"location":"installation/source-linux/#install-packages-via-apt","text":"$ apt install -y git openjdk-8-jdk maven gcc g++ build-essential cmake curl sox hunspell synfig ffmpeg","title":"Install Packages via APT"},{"location":"installation/source-linux/#install-nodejs-optional","text":"$ curl -sL https://deb.nodesource.com/setup_10.x | sudo -E bash - $ sudo apt-get install -y nodejs","title":"Install NodeJS (optional)"},{"location":"installation/source-linux/#install-and-start-elasticsearch-and-activemq-with-docker","text":"You can use docker-compose to easily run both ActiveMQ and Elasticsearch: $ cd docs/scripts/devel-dependency-containers $ docker-compose up -d To shut the services down ahain, run: $ cd docs/scripts/devel-dependency-containers $ docker-compose down","title":"Install and start Elasticsearch and ActiveMQ with Docker"},{"location":"installation/source-linux/#set-system-java-jdk","text":"Choose the Java Version 1.8.0 by entering: $ update-alternatives --config java","title":"Set System Java JDK"},{"location":"installation/source-linux/#fedora","text":"","title":"Fedora"},{"location":"installation/source-linux/#update-system_1","text":"$ dnf update -y","title":"Update System"},{"location":"installation/source-linux/#install-dependencies_1","text":"$ dnf install https://download1.rpmfusion.org/free/fedora/rpmfusion-free-release-$(rpm -E %fedora).noarch.rpm https://download1.rpmfusion.org/nonfree/fedora/rpmfusion-nonfree-release-$(rpm -E %fedora).noarch.rpm -y $ dnf group install 'Development Tools' $ dnf install -y java-1.8.0-openjdk ffmpeg maven tesseract hunspell sox synfig unzip gcc-c++ tar bzip2 nodejs","title":"Install Dependencies"},{"location":"installation/source-linux/#macos-1014","text":"","title":"macOS 10.14"},{"location":"installation/source-linux/#update-system_2","text":"Try to install all updates via the App Store or the Apple Icon on the top left corner.","title":"Update System"},{"location":"installation/source-linux/#java-jdk-8","text":"Install the JDK 8 by downloading it from https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html","title":"Java JDK 8"},{"location":"installation/source-linux/#xcode","text":"Install XCode over the App Store. It will be needed for building and for git.","title":"XCode"},{"location":"installation/source-linux/#install-packages-via-homebrew","text":"The Homebrew Project adds a package manager to Mac OS. You can install it by: $ /usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" You can now install needed packages: $ brew install maven ffmpeg nodejs","title":"Install Packages via Homebrew"},{"location":"installation/source-linux/#activemq-with-homebrew","text":"Homebrew offers you an ActiveMQ Package. Please decide, if you want to use Homebrew for ActiveMQ or if you want to follow the general guide below and run it by downloading the binaries. If you want continue you can install ActiveMQ by $ brew install activemq Remember to copy the activemq.xml like mentioned below in the right directory. You have to find the right folder for that operation, Homebrew will put the ActiveMQ files in a different location. You could find it by $ sudo find / -name activemq.xml ```` After changing the configuration file you can list and start or stop you services with ```sh $ brew services list $ brew services start activemq","title":"ActiveMQ with Homebrew"},{"location":"installation/source-linux/#git-bash-completion","text":"In macOS you can not complete or suggest half typed commands with your Tab Key (like you probably know from linux). If you want to use bash completion, you have to install it by $ brew install bash-completion Find the location of the configuration file $ sudo find / -type f -name \"git-completion.bash\" Normally it should be in $ cp /Library/Developer/CommandLineTools/usr/share/git-core/git-completion.bash /usr/local/etc/bash_completion.d/ Then add following line to the bash_profile in home [ -f /usr/local/etc/bash_completion ] && . /usr/local/etc/bash_completion Finally apply your changes with $ source /usr/local/etc/bash_completion.d/git-completion.bash","title":"Git Bash Completion"},{"location":"installation/source-linux/#install-and-configure-activemq","text":"Download the current version from https://activemq.apache.org/components/classic/download Extract and copy it to a directory, in this case you could use the opt directory. $ sudo tar -zxvf apache-activemq-*-bin.tar.gz -C /opt $ cd /opt && sudo mv apache-activemq-*/ activemq Copy the preconfigured XML from your opencast directory into your ActiveMQ configuration. In this example you have following folder structure: ~/Projects/opencast /opt/activemq With that folder structure you could use following command: $ cd && cd Projects && sudo cp opencast/docs/scripts/activemq/activemq.xml /opt/activemq/conf/activemq.xml If your folder structure is different from that example or you do decide to put it somewhere else, you should copy and replace the preconfigured XML from /location/to/your/opencast/docs/scripts/activemq/activemq.xml into /location/to/your/activemq/conf/activemq.xml You can start your ActiveMQ instance with: $ sudo ./location/to/your/activemq/bin/activemq start","title":"Install and Configure ActiveMQ"},{"location":"installation/source-linux/#build-and-start-opencast","text":"You can build now opencast by changing your directory into your opencast location and by running: $ mvn clean install After the successfully compilation you can start opencast with: $ cd build/opencast-dist-develop-*/bin && ./start-opencast The -Pdev argument decreases the build time and skips the creation of multiple tarballs and turning on the developer tarball. $ cd opencast && mvn clean install -Pdev $ cd build/opencast-dist-develop-*/bin && ./start-opencast For further information visit Development Environment .","title":"Build and Start Opencast"},{"location":"installation/source-linux/#useful-commands-for-testing-purposes","text":"For a quick build, you can use the following command to skip Opencast's tests. $ cd opencast $ mvn clean install -Pdev -DskipTests=true To see the whole stacktrace of the installation you can use the following command to disable the trimming. $ cd opencast $ mvn clean install -DtrimStackTrace=false If you want to start opencast in debug mode, you could use the debug argument: $ cd build/opencast-dist-develop-*/bin && ./start-opencast debug","title":"Useful Commands for Testing Purposes"},{"location":"installation/source-linux/#modify-code-and-build-changes","text":"After you modified your code you can go back to step \"Build and Start Opencast\" to rebuild Opencast.","title":"Modify Code and Build Changes"},{"location":"installation/source-linux/#common-build-errors-or-fixes","text":"","title":"Common Build Errors or Fixes"},{"location":"installation/source-linux/#npm-access-error","text":"To fix an npm access error ( example ), you can run $ sudo chown -R $USER:$(id -gn $USER) ~/.config && sudo chown -R $USER:$(id -gn $USER) ~/.npm","title":"NPM Access Error"},{"location":"installation/source-linux/#jdk-version","text":"Some IDEs attempt to use the most recent version of the JDK. Make sure that your IDE is configured to use JDK 1.8.0.","title":"JDK Version"},{"location":"installation/source-linux/#waiting-for-activemq","text":"Opencast requires ActiveMQ to be both running and properly configured, otherwise it will wait forever to connect. See here for details on how to configure ActiveMQ. Make sure, that ActiveMQ runs without errors and with the right JAVA_HOME Variable (explained here ).","title":"Waiting for ActiveMQ"},{"location":"installation/source-linux/#slow-idea-fix","text":"Edit following file $ sudo nano /etc/sysctl.conf and copy this text into it fs.inotify.max_user_watches = 524288 Apply your changes with $ sudo sysctl -p --system","title":"Slow IDEA Fix"},{"location":"installation/source-linux/#intellij-idea-ide-community-edition-optional","text":"If you are currently on Fedora, you can install it with following command. Make sure, that the versions match, you probably have to change it depending on the most current version. $ cd && cd Downloads && wget https://download.jetbrains.com/idea/ideaIC-2019.2.tar.gz $ sudo tar -zxvf ideaIC-*.tar.gz -C /opt $ cd /opt && sudo mv idea-IC-*/ idea && sh /opt/idea/bin/idea.sh Otherwise install it by downloading and following the manufacturer guide, select Community Edition: IDEA Intellij Community Edition Follow the next steps, if you want to import opencast correctly Import project from external model Choose Maven Search for projects recursively Uncheck all listed profiles Check all projects to import Make sure not to select JDK 11, please select JDK 1.8.0, it should be somewhere around /usr/lib/jvm/java-1.8.0-openjdk depending on your current system Now Idea should import the projects, it could take some time, you can make it faster by following this . Import the opencast code style configuration by following the steps Go to settings Search for code style You should find it under Editor->Code Style Select Java and click on the gear icon Select Import Scheme and Intellij IDEA code style XML Import it from opencast/docs/intellij_settings/codestyle.xml Now your IDE should be ready for developing.","title":"Intellij Idea IDE Community Edition (optional)"},{"location":"installation/source-linux/#visual-studio-code-editor-optional","text":"If you are currently on Fedora, you can install it with $ cd && cd Downloads && sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc && sudo sh -c 'echo -e \"[code]\\nname=Visual Studio Code\\nbaseurl=https://packages.microsoft.com/yumrepos/vscode\\nenabled=1\\ngpgcheck=1\\ngpgkey=https://packages.microsoft.com/keys/microsoft.asc\" > /etc/yum.repos.d/vscode.repo' && dnf check-update && sudo dnf install code -y Otherwise install it by downloading and following the manufacturer guide: Visual Studio Code After installation you can open a folder in bash with $ code . Recommended Extensions are ESLint by Dirk Baeumer AngularJs 1.x Code Snippets by alexandersage Debugger for Chrome by Microsoft","title":"Visual Studio Code Editor (optional)"},{"location":"modules/","text":"Modules Development Guides These guides will provide you information relevant for development of specific modules or subsystems of Opencast. Administrative User Interface Development Style Guide Capture Agent Capture Agent Test Cases Player Architecture Core Reference Events Persistent local storage Create a new Plugin Testing Stream Security","title":"Overview"},{"location":"modules/#modules-development-guides","text":"These guides will provide you information relevant for development of specific modules or subsystems of Opencast. Administrative User Interface Development Style Guide Capture Agent Capture Agent Test Cases Player Architecture Core Reference Events Persistent local storage Create a new Plugin Testing Stream Security","title":"Modules Development Guides"},{"location":"modules/oaipmh/","text":"OAI-PMH Overview OAI-PMH is an XML based protocol for metadata exchange using HTTP as the transport layer. An OAI-PMH system consists of two parts, a repository on the one and the harvester on the other end. The repository is an HTTP accessible server that exposes metadata to its client, the harvester. A repository is required to deliver metadata following the DublinCore element set version 1.1 metadata scheme. Additionally it may deliver metadata in any arbitrary format, that can be encoded as XML. For a more detailed introduction please see the OAI-PMH specification . Metadata Prefixes The Opencast OAI-PMH server supports three metadata prefixes: Prefix Description oai_dc Dublin core element set 1.1 as required by the OAI-PMH specification matterhorn Opencast media package representation matterhorn-inlined Opencast media package representation with embedded catalogs Note that the metadata prefix oai_dc is a standard metadata representation supported by all OAI-PMH servers and harverster, while the other prefixes are specific to Opencast. If you use an harvester in your third-party application, the havester will therefore need to be extended to support the Opencast-specific metadata prefixes. Glossary Term Description Repository An entity that holds a set of items to be disseminated via the OAI-PMH protocol. Different repositories may hold different sets of items. Channel The client's perspective on a repository. Each channel is backed by a single repository (1:1 relationship) so these terms may be used synonymously depending on the perspective. Item The base entity of a repository. In Opencast an item is equal to a mediapackage. Metadata format OAI-PMH repositories disseminate their content in various formats. The oai_dc format is mandatory to each OAI-PMH repository. Formats are expressed in XML. Metadata prefix The prefix identifies a metadata format. The terms are often used synonymously.","title":"OAI-PMH"},{"location":"modules/oaipmh/#oai-pmh","text":"","title":"OAI-PMH"},{"location":"modules/oaipmh/#overview","text":"OAI-PMH is an XML based protocol for metadata exchange using HTTP as the transport layer. An OAI-PMH system consists of two parts, a repository on the one and the harvester on the other end. The repository is an HTTP accessible server that exposes metadata to its client, the harvester. A repository is required to deliver metadata following the DublinCore element set version 1.1 metadata scheme. Additionally it may deliver metadata in any arbitrary format, that can be encoded as XML. For a more detailed introduction please see the OAI-PMH specification .","title":"Overview"},{"location":"modules/oaipmh/#metadata-prefixes","text":"The Opencast OAI-PMH server supports three metadata prefixes: Prefix Description oai_dc Dublin core element set 1.1 as required by the OAI-PMH specification matterhorn Opencast media package representation matterhorn-inlined Opencast media package representation with embedded catalogs Note that the metadata prefix oai_dc is a standard metadata representation supported by all OAI-PMH servers and harverster, while the other prefixes are specific to Opencast. If you use an harvester in your third-party application, the havester will therefore need to be extended to support the Opencast-specific metadata prefixes.","title":"Metadata Prefixes"},{"location":"modules/oaipmh/#glossary","text":"Term Description Repository An entity that holds a set of items to be disseminated via the OAI-PMH protocol. Different repositories may hold different sets of items. Channel The client's perspective on a repository. Each channel is backed by a single repository (1:1 relationship) so these terms may be used synonymously depending on the perspective. Item The base entity of a repository. In Opencast an item is equal to a mediapackage. Metadata format OAI-PMH repositories disseminate their content in various formats. The oai_dc format is mandatory to each OAI-PMH repository. Formats are expressed in XML. Metadata prefix The prefix identifies a metadata format. The terms are often used synonymously.","title":"Glossary"},{"location":"modules/stream-security/","text":"Stream Security Developer Guide To get an introduction to Stream Security, please read the sub section Stream Security in the section Modules of the Admin Guide. Opencast Signing Protocol The Signing Providers as well as the verification components that are developed by the Opencast community implement the policy and signature specified in the Opencast Signing Protocol. Policy The policy is a Base64 encoded JSON document. A human-readable version of the JSON document looks like this: { \"Statement\":{ \"Resource\":\"http:\\/\\/opencast.org\\/engage\\/resource.mp4\", \"Condition\":{ \"DateLessThan\":1425170777000, \"DateGreaterThan\":1425084379000, \"IpAddress\":\"10.0.0.1\" } } } Property Name Property Description Resource URL of the resource, must exactly match the requested URL including the schema. In case of a RTMP request, this is only the resource path, without the RTMP application name or the server. DateLessThan Unix epoch that a resource should expire on in milliseconds DateGreaterThan Unix epoch that a resource should become available in milliseconds IpAddress Client's IP address that will be accessing the resource Properties in bold are mandatory. Before the JSON document is Base64 encoded, all whitespaces need to be removed. The above sample document would then look like this: {\"Statement\":{\"Resource\":\"http:\\/\\/opencast.org\\/engage\\/resource.mp4\",\"Condition\":{\"DateLessThan\":1425170777000,\"DateGreaterThan\":1425084379000,\"IpAddress\":\"10.0.0.1\"}}} The Base64-encoding must be performed in a URL safe way which means that instead of using the characters \u2018+\u2019 and \u2018/\u2019 they have to be replaced by '-' and '_' respectively. The example above would be encoded into: eyJTdGF0ZW1lbnQiOnsiUmVzb3VyY2UiOiJodHRwOlwvXC9vcGVuY2FzdC5vcmdcL2VuZ2FnZVwvcmVzb3VyY2UubXA0IiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6MTQyNTE3MDc3NzAwMCwiRGF0ZUdyZWF0ZXJUaGFuIjoxNDI1MDg0Mzc5MDAwLCJJcEFkZHJlc3MiOiIxMC4wLjAuMSJ9fX0= The encoded policy must be sent to the server as a query parameter named \u2018policy\u2019, e.g. http://opencast.org/engage/resource.mp4?policy=eyJTdGF0ZW1lbnQiOnsiUmVzb3VyY2UiOiJodHRwOlwvXC9vcGVuY2FzdC5vcmdcL2VuZ2FnZVwvcmVzb3VyY2UubXA0IiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6MTQyNTE3MDc3NzAwMCwiRGF0ZUdyZWF0ZXJUaGFuIjoxNDI1MDg0Mzc5MDAwLCJJcEFkZHJlc3MiOiIxMC4wLjAuMSJ9fX0 Note: Be aware that Base64 encoding can have up to two \u2018=\u2019 characters at the end of the string to pad a message to a necessary length divisible by 3. All components should be able to handle Base64 encoded strings with or without this padding (Resources signed by Opencast will have the padding characters URL encoded to \u2018%3D\u2019). Signature The signature is a hash-based message authentication code (HMAC) based on a secret key. The algorithm used is HMAC-SHA-256. Only the encoded policy needs to be taken as input for the hash-calculation. The keys used are simple character strings without any special format. It could be something like \u2018AbCdEfGh\u2019, but it\u2019s recommended to use a key with a length of 256 bit like \u20182195265EE84ED1E1324D31F37F7E3\u2019. Each key must have a unique identifier, e.g. \u2018key1\u2019. In this example, the following key has been used: Key ID: demoKeyOne Secret Key: 6EDB5EDDCF994B7432C371D7C274F The HMAC for the signature from the previous section calculated based on the demoKey1 is c8712284aabc843f76a132a3a7c8997670414b2f89cb96b367d5f35d0f62a2e4 The signature must also be sent as a query parameter that forms part of the resource request. The example from above would now look like this: http://opencast.org/engage/resource.mp4?policy=eyJTdGF0ZW1lbnQiOnsiUmVzb3VyY2UiOiJodHRwOlwvXC9vcGVuY2FzdC5vcmdcL2VuZ2FnZVwvcmVzb3VyY2UubXA0IiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6MTQyNTE3MDc3NzAwMCwiRGF0ZUdyZWF0ZXJUaGFuIjoxNDI1MDg0Mzc5MDAwLCJJcEFkZHJlc3MiOiIxMC4wLjAuMSJ9fX0&signature=c8712284aabc843f76a132a3a7c8997670414b2f89cb96b367d5f35d0f62a2e4 The same is true for the key id, which needs to be included to determine which key was used to create the signature. http://opencast.org/engage/resource.mp4?policy=eyJTdGF0ZW1lbnQiOnsiUmVzb3VyY2UiOiJodHRwOlwvXC9vcGVuY2FzdC5vcmdcL2VuZ2FnZVwvcmVzb3VyY2UubXA0IiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6MTQyNTE3MDc3NzAwMCwiRGF0ZUdyZWF0ZXJUaGFuIjoxNDI1MDg0Mzc5MDAwLCJJcEFkZHJlc3MiOiIxMC4wLjAuMSJ9fX0&signature=c8712284aabc843f76a132a3a7c8997670414b2f89cb96b367d5f35d0f62a2e4&keyId=demoKeyOne Signing URLs from a 3rd party system URL signatures also need to be issued for resources presented on and linked from a third party system (such as a custom video portal). There are two options for signing 3rd party system URLs: Option #1: Use the existing URL Signing Service** If the third party system is based on Java, the existing URL Signing bundles/JARs can be reused. They do not have dependencies to other parts of Opencast and can therefore be used independently. These bundles are required: urlsigning-common urlsigning-service-api urlsigning-service-impl Code example: private UrlSigningService urlSigningService; /** OSGi DI */ void setUrlSigningService(UrlSigningService service) { this.urlSigningService = service; } \u2026 String urlToSign = \u201chttp://my.custom.url/with/path.mp4\u201d; long signedUrlExpiresDuration = 60; if (urlSigningService.accepts(urlToSign)) { try { String signedUrl = urlSigningService.sign( urlToSign, signedUrlExpiresDuration, null, null); ... } catch (UrlSigningException e) { // handle exception } } Option #2: Create custom URL Signing Service Based on the technical details outlined in the Opencast Signing Protocol, a URL Signing Service that is compatible with the other existing parts of the Stream Security system can be implemented. Option #3: Give Access to Third Party Systems to Signing REST Endpoints Opencast servers that have been configured to use URL signing service will have two REST endpoints at http://admin.opencast.edu:8080/signing/docs. The accepts endpoint will return true if the Opencast server can sign a particular URL. The sign endpoint will return a signed URL when the correct parameters are given. Due to the sensitive nature of these endpoints they are locked down to be only accessible by a user with ROLE_ADMIN privileges in the etc/security/mh_default_org.xml configuration file. Creating a new user with this role and accessing the endpoint using these credentials will allow a third party system to sign any URLs. Further information For an overview of Stream Security, please consult the sub section Stream Security in the section Modules of the Admin Guide. For information about how to configure stream security on your Opencast servers, please consult the sub section Stream Security in the section Configuration of the Admin Guide","title":"Stream Security"},{"location":"modules/stream-security/#stream-security-developer-guide","text":"To get an introduction to Stream Security, please read the sub section Stream Security in the section Modules of the Admin Guide.","title":"Stream Security Developer Guide"},{"location":"modules/stream-security/#opencast-signing-protocol","text":"The Signing Providers as well as the verification components that are developed by the Opencast community implement the policy and signature specified in the Opencast Signing Protocol.","title":"Opencast Signing Protocol"},{"location":"modules/stream-security/#policy","text":"The policy is a Base64 encoded JSON document. A human-readable version of the JSON document looks like this: { \"Statement\":{ \"Resource\":\"http:\\/\\/opencast.org\\/engage\\/resource.mp4\", \"Condition\":{ \"DateLessThan\":1425170777000, \"DateGreaterThan\":1425084379000, \"IpAddress\":\"10.0.0.1\" } } } Property Name Property Description Resource URL of the resource, must exactly match the requested URL including the schema. In case of a RTMP request, this is only the resource path, without the RTMP application name or the server. DateLessThan Unix epoch that a resource should expire on in milliseconds DateGreaterThan Unix epoch that a resource should become available in milliseconds IpAddress Client's IP address that will be accessing the resource Properties in bold are mandatory. Before the JSON document is Base64 encoded, all whitespaces need to be removed. The above sample document would then look like this: {\"Statement\":{\"Resource\":\"http:\\/\\/opencast.org\\/engage\\/resource.mp4\",\"Condition\":{\"DateLessThan\":1425170777000,\"DateGreaterThan\":1425084379000,\"IpAddress\":\"10.0.0.1\"}}} The Base64-encoding must be performed in a URL safe way which means that instead of using the characters \u2018+\u2019 and \u2018/\u2019 they have to be replaced by '-' and '_' respectively. The example above would be encoded into: eyJTdGF0ZW1lbnQiOnsiUmVzb3VyY2UiOiJodHRwOlwvXC9vcGVuY2FzdC5vcmdcL2VuZ2FnZVwvcmVzb3VyY2UubXA0IiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6MTQyNTE3MDc3NzAwMCwiRGF0ZUdyZWF0ZXJUaGFuIjoxNDI1MDg0Mzc5MDAwLCJJcEFkZHJlc3MiOiIxMC4wLjAuMSJ9fX0= The encoded policy must be sent to the server as a query parameter named \u2018policy\u2019, e.g. http://opencast.org/engage/resource.mp4?policy=eyJTdGF0ZW1lbnQiOnsiUmVzb3VyY2UiOiJodHRwOlwvXC9vcGVuY2FzdC5vcmdcL2VuZ2FnZVwvcmVzb3VyY2UubXA0IiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6MTQyNTE3MDc3NzAwMCwiRGF0ZUdyZWF0ZXJUaGFuIjoxNDI1MDg0Mzc5MDAwLCJJcEFkZHJlc3MiOiIxMC4wLjAuMSJ9fX0 Note: Be aware that Base64 encoding can have up to two \u2018=\u2019 characters at the end of the string to pad a message to a necessary length divisible by 3. All components should be able to handle Base64 encoded strings with or without this padding (Resources signed by Opencast will have the padding characters URL encoded to \u2018%3D\u2019).","title":"Policy"},{"location":"modules/stream-security/#signature","text":"The signature is a hash-based message authentication code (HMAC) based on a secret key. The algorithm used is HMAC-SHA-256. Only the encoded policy needs to be taken as input for the hash-calculation. The keys used are simple character strings without any special format. It could be something like \u2018AbCdEfGh\u2019, but it\u2019s recommended to use a key with a length of 256 bit like \u20182195265EE84ED1E1324D31F37F7E3\u2019. Each key must have a unique identifier, e.g. \u2018key1\u2019. In this example, the following key has been used: Key ID: demoKeyOne Secret Key: 6EDB5EDDCF994B7432C371D7C274F The HMAC for the signature from the previous section calculated based on the demoKey1 is c8712284aabc843f76a132a3a7c8997670414b2f89cb96b367d5f35d0f62a2e4 The signature must also be sent as a query parameter that forms part of the resource request. The example from above would now look like this: http://opencast.org/engage/resource.mp4?policy=eyJTdGF0ZW1lbnQiOnsiUmVzb3VyY2UiOiJodHRwOlwvXC9vcGVuY2FzdC5vcmdcL2VuZ2FnZVwvcmVzb3VyY2UubXA0IiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6MTQyNTE3MDc3NzAwMCwiRGF0ZUdyZWF0ZXJUaGFuIjoxNDI1MDg0Mzc5MDAwLCJJcEFkZHJlc3MiOiIxMC4wLjAuMSJ9fX0&signature=c8712284aabc843f76a132a3a7c8997670414b2f89cb96b367d5f35d0f62a2e4 The same is true for the key id, which needs to be included to determine which key was used to create the signature. http://opencast.org/engage/resource.mp4?policy=eyJTdGF0ZW1lbnQiOnsiUmVzb3VyY2UiOiJodHRwOlwvXC9vcGVuY2FzdC5vcmdcL2VuZ2FnZVwvcmVzb3VyY2UubXA0IiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6MTQyNTE3MDc3NzAwMCwiRGF0ZUdyZWF0ZXJUaGFuIjoxNDI1MDg0Mzc5MDAwLCJJcEFkZHJlc3MiOiIxMC4wLjAuMSJ9fX0&signature=c8712284aabc843f76a132a3a7c8997670414b2f89cb96b367d5f35d0f62a2e4&keyId=demoKeyOne","title":"Signature"},{"location":"modules/stream-security/#signing-urls-from-a-3rd-party-system","text":"URL signatures also need to be issued for resources presented on and linked from a third party system (such as a custom video portal). There are two options for signing 3rd party system URLs:","title":"Signing URLs from a 3rd party system"},{"location":"modules/stream-security/#option-1-use-the-existing-url-signing-service","text":"If the third party system is based on Java, the existing URL Signing bundles/JARs can be reused. They do not have dependencies to other parts of Opencast and can therefore be used independently. These bundles are required: urlsigning-common urlsigning-service-api urlsigning-service-impl Code example: private UrlSigningService urlSigningService; /** OSGi DI */ void setUrlSigningService(UrlSigningService service) { this.urlSigningService = service; } \u2026 String urlToSign = \u201chttp://my.custom.url/with/path.mp4\u201d; long signedUrlExpiresDuration = 60; if (urlSigningService.accepts(urlToSign)) { try { String signedUrl = urlSigningService.sign( urlToSign, signedUrlExpiresDuration, null, null); ... } catch (UrlSigningException e) { // handle exception } }","title":"Option #1: Use the existing URL Signing Service**"},{"location":"modules/stream-security/#option-2-create-custom-url-signing-service","text":"Based on the technical details outlined in the Opencast Signing Protocol, a URL Signing Service that is compatible with the other existing parts of the Stream Security system can be implemented.","title":"Option #2: Create custom URL Signing Service"},{"location":"modules/stream-security/#option-3-give-access-to-third-party-systems-to-signing-rest-endpoints","text":"Opencast servers that have been configured to use URL signing service will have two REST endpoints at http://admin.opencast.edu:8080/signing/docs. The accepts endpoint will return true if the Opencast server can sign a particular URL. The sign endpoint will return a signed URL when the correct parameters are given. Due to the sensitive nature of these endpoints they are locked down to be only accessible by a user with ROLE_ADMIN privileges in the etc/security/mh_default_org.xml configuration file. Creating a new user with this role and accessing the endpoint using these credentials will allow a third party system to sign any URLs.","title":"Option #3: Give Access to Third Party Systems to Signing REST Endpoints"},{"location":"modules/stream-security/#further-information","text":"For an overview of Stream Security, please consult the sub section Stream Security in the section Modules of the Admin Guide. For information about how to configure stream security on your Opencast servers, please consult the sub section Stream Security in the section Configuration of the Admin Guide","title":"Further information"},{"location":"modules/admin-ui/development/","text":"Style Guide The style guide defines a set of guidelines that the design follows to maintain a consistent look and feel. It is defined to be flexible, easy to update and consistent. Before delving deeper into the UI or developing additional features we recommend familiarizing yourself with some of the items. Prerequisites For admin interface development, it may come handy to run npm or grunt from the command line. For that, you can either install those tools globally on your machine, or use the versions Opencast installs locally when building via Maven. If you want to do that, you may want to add the installation folders to your path variable: cd modules/admin-ui-frontend export PATH=node:node_modules/.bin:$PATH Debugging Javascript unit tests Our Javascript unit tests are built in Jasmine (a behavior-driven development framework for testing JavaScript code), and live in modules/admin-ui-frontend/test/test/unit . Occasionally something breaks, or you need to disable or focus on a single test. While reading the Jasmine, Karma and Grunt docs are encouraged, here are a few common recipes that might be useful: Disabling a unit test temporarily Add x to the broken test. For example: Before After it('runs a test', function () { x it('runs a test', function () { Running a single unit test Add f (for focus) to the relevant test. For example: Before After it('runs a test', function () { f it('runs a test', function () { Triggering a browser debugging session This triggers an instance of the selected browser(s) to open and begin running the tests. There will be a Debug button which will open another tab where the JavaScript has not been minified, use this second tab for debugging. Refreshing the debugging page will rerun the tests. To run Karma Chrome npm run test-chrome Firefox npm run test-firefox IE npm run test-ie Additional browsers are supported, the full list can be found at https://karma-runner.github.io/ . Live working with a running Opencast In order to speed up the UI development process, you can test the code without building the module with Maven. There is a Grunt task for starting a standalone web server offering the UI from the source and a separate task that will monitoring any change to the Sass, JavaScript and HTML files and reload the page dynamically. Be warned that some functionality in this live setup can be limited. Before reporting an issue, please test if you can reproduced the issue with a built Opencast. This setup may be configured as follows: Follow the instructions in the Prerequisites section. Start your Opencast instance. Change to the Admin UI module directory. cd modules/admin-ui-frontend Install project dependencies. npm install && bower install Start the standalone web server by running: grunt proxy --proxy.host=http://localhost:8080 --proxy.username=opencast_system_account --proxy.password=CHANGE_ME Note: host, username and password have to match your configuration ../etc/custom.properties Grunt should print out the URL where you can see the standalone page running from source. Started connect web server on http://localhost:9000 To run the watcher that updates the displayed page dynamically, run in the same folder: grunt watch Which should then display: Running \"watch\" task Waiting... The watch process monitors the js , scss and sass files for changes and should dynamically reload the page. Note: A refresh of the page might be required to start the live reload script Live working with a Mockup If you do not want to keep a running Opencast instance for developing the Admin UI NG, you can start a mockup. Be warned that a lot of this mockup's functionality acts very differently from an actual Opencast instance This setup may be configured as follows: Follow the instructions in the Prerequisites section. Change to the Admin UI module directory. cd modules/admin-ui-frontend Install project dependencies. npm install && bower install Start the mockup webserver by running: grunt serve Grunt should print out the URL where you can see the standalone page running from source. Started connect web server on http://localhost:9000 If you make changes to the Admin UI NG source files, the page should auto reload to display the changes. Update Node Dependencies Installing npm-check-updates and running it at the start of developing / improving a component can ensure that the node modules stays up-to-date and dependency bugs are reduced. Note: Test the build ( mvn install , npm install , grunt ) thoroughly when upgrading modules as this might cause some unexpected build failures (resetting the grunt version to \"grunt\": \"^0.4.0\" might resolve some of the initial issues). Installation. npm install -g npm-check-updates Show any new dependencies for the project in the current directory. ncu Upgrade a project's package file. ncu -u A detailed reference of the command-line tool can be found at https://www.npmjs.com/package/npm-check-updates .","title":"Development"},{"location":"modules/admin-ui/development/#style-guide","text":"The style guide defines a set of guidelines that the design follows to maintain a consistent look and feel. It is defined to be flexible, easy to update and consistent. Before delving deeper into the UI or developing additional features we recommend familiarizing yourself with some of the items.","title":"Style Guide"},{"location":"modules/admin-ui/development/#prerequisites","text":"For admin interface development, it may come handy to run npm or grunt from the command line. For that, you can either install those tools globally on your machine, or use the versions Opencast installs locally when building via Maven. If you want to do that, you may want to add the installation folders to your path variable: cd modules/admin-ui-frontend export PATH=node:node_modules/.bin:$PATH","title":"Prerequisites"},{"location":"modules/admin-ui/development/#debugging-javascript-unit-tests","text":"Our Javascript unit tests are built in Jasmine (a behavior-driven development framework for testing JavaScript code), and live in modules/admin-ui-frontend/test/test/unit . Occasionally something breaks, or you need to disable or focus on a single test. While reading the Jasmine, Karma and Grunt docs are encouraged, here are a few common recipes that might be useful:","title":"Debugging Javascript unit tests"},{"location":"modules/admin-ui/development/#disabling-a-unit-test-temporarily","text":"Add x to the broken test. For example: Before After it('runs a test', function () { x it('runs a test', function () {","title":"Disabling a unit test temporarily"},{"location":"modules/admin-ui/development/#running-a-single-unit-test","text":"Add f (for focus) to the relevant test. For example: Before After it('runs a test', function () { f it('runs a test', function () {","title":"Running a single unit test"},{"location":"modules/admin-ui/development/#triggering-a-browser-debugging-session","text":"This triggers an instance of the selected browser(s) to open and begin running the tests. There will be a Debug button which will open another tab where the JavaScript has not been minified, use this second tab for debugging. Refreshing the debugging page will rerun the tests. To run Karma Chrome npm run test-chrome Firefox npm run test-firefox IE npm run test-ie Additional browsers are supported, the full list can be found at https://karma-runner.github.io/ .","title":"Triggering a browser debugging session"},{"location":"modules/admin-ui/development/#live-working-with-a-running-opencast","text":"In order to speed up the UI development process, you can test the code without building the module with Maven. There is a Grunt task for starting a standalone web server offering the UI from the source and a separate task that will monitoring any change to the Sass, JavaScript and HTML files and reload the page dynamically. Be warned that some functionality in this live setup can be limited. Before reporting an issue, please test if you can reproduced the issue with a built Opencast. This setup may be configured as follows: Follow the instructions in the Prerequisites section. Start your Opencast instance. Change to the Admin UI module directory. cd modules/admin-ui-frontend Install project dependencies. npm install && bower install Start the standalone web server by running: grunt proxy --proxy.host=http://localhost:8080 --proxy.username=opencast_system_account --proxy.password=CHANGE_ME Note: host, username and password have to match your configuration ../etc/custom.properties Grunt should print out the URL where you can see the standalone page running from source. Started connect web server on http://localhost:9000 To run the watcher that updates the displayed page dynamically, run in the same folder: grunt watch Which should then display: Running \"watch\" task Waiting... The watch process monitors the js , scss and sass files for changes and should dynamically reload the page. Note: A refresh of the page might be required to start the live reload script","title":"Live working with a running Opencast"},{"location":"modules/admin-ui/development/#live-working-with-a-mockup","text":"If you do not want to keep a running Opencast instance for developing the Admin UI NG, you can start a mockup. Be warned that a lot of this mockup's functionality acts very differently from an actual Opencast instance This setup may be configured as follows: Follow the instructions in the Prerequisites section. Change to the Admin UI module directory. cd modules/admin-ui-frontend Install project dependencies. npm install && bower install Start the mockup webserver by running: grunt serve Grunt should print out the URL where you can see the standalone page running from source. Started connect web server on http://localhost:9000 If you make changes to the Admin UI NG source files, the page should auto reload to display the changes.","title":"Live working with a Mockup"},{"location":"modules/admin-ui/development/#update-node-dependencies","text":"Installing npm-check-updates and running it at the start of developing / improving a component can ensure that the node modules stays up-to-date and dependency bugs are reduced. Note: Test the build ( mvn install , npm install , grunt ) thoroughly when upgrading modules as this might cause some unexpected build failures (resetting the grunt version to \"grunt\": \"^0.4.0\" might resolve some of the initial issues). Installation. npm install -g npm-check-updates Show any new dependencies for the project in the current directory. ncu Upgrade a project's package file. ncu -u A detailed reference of the command-line tool can be found at https://www.npmjs.com/package/npm-check-updates .","title":"Update Node Dependencies"},{"location":"modules/admin-ui/style/","text":"Administrative User Interface: Style Guide Color Palette Typeface View Structure Logo Navigation Modals Forms Icons Buttons Dropdowns Alerts and Indicators References","title":"Overview"},{"location":"modules/admin-ui/style/#administrative-user-interface-style-guide","text":"Color Palette Typeface View Structure Logo Navigation Modals Forms Icons Buttons Dropdowns Alerts and Indicators References","title":"Administrative User Interface: Style Guide"},{"location":"modules/admin-ui/style/alerts-indicators/","text":"Alerts Alerts are used to tell users when actions are incomplete, or rejected. Alerts should be placed at the top of the display view to have an importance in hierarchy. Alerts are represented in \"Success\", \"Info\", \"Warning\" and \"Danger\" (Error) states with their corresponding colors below. .alert { font-weight: 600; min-height: 40px; } .alert .close { font-size: 20px; } /* $state-success-text - $state-success-bg - $state-success-border */ .alert.success { background: #dff0d8 none repeat scroll 0 0; border: 1px solid #d4eacb; } .alert.success p { color: #3c763d; } /* $state-info-text - $state-info-bg - $state-info-border */ .alert.info { background: #d9edf7 none repeat scroll 0 0; border: 1px solid #c3e1f0; } .alert.info p { color: #387fa2; } /* $state-warning-text - $state-warning-bg - $state-warning-border */ .alert.warning { background: #fcf8e3 none repeat scroll 0 0; border: 1px solid #f5f0cc; } .alert.warning p { color: #8a6d3b; } /* $state-danger-text - $state-danger-bg - $state-danger-border */ .alert.danger { background: #f2dede none repeat scroll 0 0; border: 1px solid #ebd1d1; } .alert.danger p { color: #a94442; } Indicators Indicator lights should be visible to represent the health of an action or system. Active - Green: #37c180 $green Suspended - Yellow: #e4d12e $yellow Error - Red: #fa1919 $red Running - Blue: #378dd4 $blue","title":"Alerts and Indicators"},{"location":"modules/admin-ui/style/alerts-indicators/#alerts","text":"Alerts are used to tell users when actions are incomplete, or rejected. Alerts should be placed at the top of the display view to have an importance in hierarchy. Alerts are represented in \"Success\", \"Info\", \"Warning\" and \"Danger\" (Error) states with their corresponding colors below. .alert { font-weight: 600; min-height: 40px; } .alert .close { font-size: 20px; } /* $state-success-text - $state-success-bg - $state-success-border */ .alert.success { background: #dff0d8 none repeat scroll 0 0; border: 1px solid #d4eacb; } .alert.success p { color: #3c763d; } /* $state-info-text - $state-info-bg - $state-info-border */ .alert.info { background: #d9edf7 none repeat scroll 0 0; border: 1px solid #c3e1f0; } .alert.info p { color: #387fa2; } /* $state-warning-text - $state-warning-bg - $state-warning-border */ .alert.warning { background: #fcf8e3 none repeat scroll 0 0; border: 1px solid #f5f0cc; } .alert.warning p { color: #8a6d3b; } /* $state-danger-text - $state-danger-bg - $state-danger-border */ .alert.danger { background: #f2dede none repeat scroll 0 0; border: 1px solid #ebd1d1; } .alert.danger p { color: #a94442; }","title":"Alerts"},{"location":"modules/admin-ui/style/alerts-indicators/#indicators","text":"Indicator lights should be visible to represent the health of an action or system. Active - Green: #37c180 $green Suspended - Yellow: #e4d12e $yellow Error - Red: #fa1919 $red Running - Blue: #378dd4 $blue","title":"Indicators"},{"location":"modules/admin-ui/style/buttons/","text":"Main Action Buttons Main Action buttons appear in the action bar. These buttons are generally to start an action like exporting or importing a list, creating a new event or series. /* [ Main Button ] */ .action-nav-bar .btn-group .add { color : #FFFFFF ; background-color : #39c985 ; background-image : linear-gradient( #39c985 , #2d9b67 ) ; border : 1px solid #193043 ; } Modal Buttons /* [ Modal Button - Accept ] */ .modal a { color : #FFFFFF ; background-color : #39c985 ; background-image : linear-gradient( #39c985 , #2d9b67 ) ; border : 1px solid #20724b ; } /* [ Modal Button - Cancel / Return ] */ .modal a .cancel { color : #666666 ; background-color : #FFFFFF ; background-image : linear-gradient( #FFFFFF , #f7f7f7 ) ; border : 1px solid #c9d0d3 ; } Action buttons Action buttons are used to allow the user to control steps in a process. Usually placed within a table view, actions include options to delete, process and view statistics. These action buttons will generally open a modal and should be placed in a specific hierarchical order for consistency. Detail Actions Sub Actions Delete Actions","title":"Buttons"},{"location":"modules/admin-ui/style/buttons/#main-action-buttons","text":"Main Action buttons appear in the action bar. These buttons are generally to start an action like exporting or importing a list, creating a new event or series. /* [ Main Button ] */ .action-nav-bar .btn-group .add { color : #FFFFFF ; background-color : #39c985 ; background-image : linear-gradient( #39c985 , #2d9b67 ) ; border : 1px solid #193043 ; }","title":"Main Action Buttons"},{"location":"modules/admin-ui/style/buttons/#modal-buttons","text":"/* [ Modal Button - Accept ] */ .modal a { color : #FFFFFF ; background-color : #39c985 ; background-image : linear-gradient( #39c985 , #2d9b67 ) ; border : 1px solid #20724b ; } /* [ Modal Button - Cancel / Return ] */ .modal a .cancel { color : #666666 ; background-color : #FFFFFF ; background-image : linear-gradient( #FFFFFF , #f7f7f7 ) ; border : 1px solid #c9d0d3 ; }","title":"Modal Buttons"},{"location":"modules/admin-ui/style/buttons/#action-buttons","text":"Action buttons are used to allow the user to control steps in a process. Usually placed within a table view, actions include options to delete, process and view statistics. These action buttons will generally open a modal and should be placed in a specific hierarchical order for consistency. Detail Actions Sub Actions Delete Actions","title":"Action buttons"},{"location":"modules/admin-ui/style/color-palette/","text":"Color Palette The color palette is a very cool feeling range of icy blues and crisp greens with cold grey accents. The subtle differences between the colors offer a comfortable viewing experience without losing an ease of differentiation between elements. The blend of colors work together in a mature fashion presenting a more professional environment for users. Percentage tints can be used of any of these colors. #37C180 #378DD4 #24425C #8C939B #FAFAFA #268559 #2c9966 #31ad73 #37c180 #47cb8d #5bd099 #6fd6a5 #0e1b25 #162837 #1d354a #24425c #2b4f6e #325c81 #3a6993 Secondary Color Palette The Secondary Palette should be used as a guide for the tone of a color. It is intended to be ever evolving, offering an endless palette to choose from. #4da1f7 Bright Blue HEX #4da1f7 RGB 77, 161, 247 SCSS $bright-blue #fa1919 Red HEX #fa1919 RGB 250, 25, 25 SCSS $red #e45253 Alternate Red HEX #e45253 RGB 228, 82, 83 SCSS $alt-red #e4d12e Yellow HEX #e4d12e RGB 228, 209, 46 SCSS $yellow Shades of gray A selection of grayscale colors for background, border or text color use. #222222 #4b4b4b #646464 #7d7d7d #969696 #c8c8c8 #e1e1e1 #fafafa","title":"Color Palette"},{"location":"modules/admin-ui/style/color-palette/#color-palette","text":"The color palette is a very cool feeling range of icy blues and crisp greens with cold grey accents. The subtle differences between the colors offer a comfortable viewing experience without losing an ease of differentiation between elements. The blend of colors work together in a mature fashion presenting a more professional environment for users. Percentage tints can be used of any of these colors. #37C180 #378DD4 #24425C #8C939B #FAFAFA #268559 #2c9966 #31ad73 #37c180 #47cb8d #5bd099 #6fd6a5 #0e1b25 #162837 #1d354a #24425c #2b4f6e #325c81 #3a6993","title":"Color Palette"},{"location":"modules/admin-ui/style/color-palette/#secondary-color-palette","text":"The Secondary Palette should be used as a guide for the tone of a color. It is intended to be ever evolving, offering an endless palette to choose from. #4da1f7 Bright Blue HEX #4da1f7 RGB 77, 161, 247 SCSS $bright-blue #fa1919 Red HEX #fa1919 RGB 250, 25, 25 SCSS $red #e45253 Alternate Red HEX #e45253 RGB 228, 82, 83 SCSS $alt-red #e4d12e Yellow HEX #e4d12e RGB 228, 209, 46 SCSS $yellow","title":"Secondary Color Palette"},{"location":"modules/admin-ui/style/color-palette/#shades-of-gray","text":"A selection of grayscale colors for background, border or text color use. #222222 #4b4b4b #646464 #7d7d7d #969696 #c8c8c8 #e1e1e1 #fafafa","title":"Shades of gray"},{"location":"modules/admin-ui/style/dropdowns/","text":"Dropdowns Dropdowns are used when update actions are required or to trigger bulk actions. .drop-down-container { color : #666666 ; background-color : #FFFFFF ; background-image : linear-gradient( #ffffff , #f7f7f7 ) ; border : 1px solid #c9d0d3 ; font-size : 12px ; font-weight : 600 ; } .drop-down-container { color : #666666 ; background-color : #FFFFFF ; background-image : linear-gradient( #ffffff , #f7f7f7 ) ; border : 1px solid #c9d0d3 ; font-size : 12px ; font-weight : 600 ; } .df-profile-filters .filters-list header { background-image : linear-gradient( #ffffff , #f7f7f7 ) ; border : 1px solid #c9d0d3 ; } .df-profile-filters .input-container .save { color : #ffffff ; background-color : #39c985 ; background-image : linear-gradient( #39c985 , #2d9b67 ) ; border : 1px solid #c9d0d3 ; }","title":"Dropdowns"},{"location":"modules/admin-ui/style/dropdowns/#dropdowns","text":"Dropdowns are used when update actions are required or to trigger bulk actions. .drop-down-container { color : #666666 ; background-color : #FFFFFF ; background-image : linear-gradient( #ffffff , #f7f7f7 ) ; border : 1px solid #c9d0d3 ; font-size : 12px ; font-weight : 600 ; } .drop-down-container { color : #666666 ; background-color : #FFFFFF ; background-image : linear-gradient( #ffffff , #f7f7f7 ) ; border : 1px solid #c9d0d3 ; font-size : 12px ; font-weight : 600 ; } .df-profile-filters .filters-list header { background-image : linear-gradient( #ffffff , #f7f7f7 ) ; border : 1px solid #c9d0d3 ; } .df-profile-filters .input-container .save { color : #ffffff ; background-color : #39c985 ; background-image : linear-gradient( #39c985 , #2d9b67 ) ; border : 1px solid #c9d0d3 ; }","title":"Dropdowns"},{"location":"modules/admin-ui/style/forms/","text":"Forms There are currently 2 form conventions used in the system and they are closely associated with the modal type that the form is displayed in. Step-by-Step Modal The form displayed in the step-by-step modal is displayed in a 2 column table; where each row has a label (left column) and an input field (right column). To complete the form the required fields ( * ) need to be filled in and so forth until the completion of the modal action. The form inputs are read-only on initial display and if editable will activate when the user clicks on the value (underlined text) or on the edit button located on the right side of the form field. Dropdown selection boxes will appear on editing/interaction of the field, if the field is so defined. input[type=\"date\"], input[type=\"time\"], input[type=\"text\"], input[type=\"email\"], input[type=\"password\"], input[type=\"search\"], textarea { background-color: #fff; border: 1px solid #c9d0d3; border-radius: 4px; color: #666; font-family: \"Open Sans\",Helvetica,sans-serif; font-size: 13px; font-weight: 600; height: 40px; margin: 0 auto; padding: 0 0 0 10px; width: 100%; } .edit { /* Edit button - fa-pencil-square */ font-size: 14px; margin: 5px; } span.editable, td.editable span { /* Clickable element for field editing */ border-bottom: 1px dashed #999; font-size: 12px; font-weight: 400; color: #666; line-height: 25px; } .chosen-container { /* Drop down selector */ font-size: 13px; font-weight: 400; width: 250px; } .td span { /* Table Label */ font-size: 12px; font-weight: 400; color: #666; line-height: 25px; } Tabbed The tabbed modal is normally a much smaller modal and the fields required to complete the action fewer and more specific. The form has larger input fields and the labels are positioned above them. .form-container label { font-family: \"Open Sans\",\u200bHelvetica,\u200bsans-serif; font-size: 14px; font-weight: 400; color: #666; line-height: 14px; } .form-container input { /* Inherits style as defined for inputs (above) */ padding: 12px 20px 12px 15px; margin: 10px 0px 5px 0px; width: 100%; line-height: 18px; } input#search { background-image: url(\"../img/search.png\"); background-position: 14px center; background-repeat: no-repeat; height: 40px; padding: 0 20px 0 40px !important; } select[multiple] { background: #fff none repeat scroll 0 0; border: 1px solid #c9d0d3; border-radius: 4px; color: #666; font-size: 13px; font-weight: 400; padding: 10px; }","title":"Forms"},{"location":"modules/admin-ui/style/forms/#forms","text":"There are currently 2 form conventions used in the system and they are closely associated with the modal type that the form is displayed in. Step-by-Step Modal The form displayed in the step-by-step modal is displayed in a 2 column table; where each row has a label (left column) and an input field (right column). To complete the form the required fields ( * ) need to be filled in and so forth until the completion of the modal action. The form inputs are read-only on initial display and if editable will activate when the user clicks on the value (underlined text) or on the edit button located on the right side of the form field. Dropdown selection boxes will appear on editing/interaction of the field, if the field is so defined. input[type=\"date\"], input[type=\"time\"], input[type=\"text\"], input[type=\"email\"], input[type=\"password\"], input[type=\"search\"], textarea { background-color: #fff; border: 1px solid #c9d0d3; border-radius: 4px; color: #666; font-family: \"Open Sans\",Helvetica,sans-serif; font-size: 13px; font-weight: 600; height: 40px; margin: 0 auto; padding: 0 0 0 10px; width: 100%; } .edit { /* Edit button - fa-pencil-square */ font-size: 14px; margin: 5px; } span.editable, td.editable span { /* Clickable element for field editing */ border-bottom: 1px dashed #999; font-size: 12px; font-weight: 400; color: #666; line-height: 25px; } .chosen-container { /* Drop down selector */ font-size: 13px; font-weight: 400; width: 250px; } .td span { /* Table Label */ font-size: 12px; font-weight: 400; color: #666; line-height: 25px; } Tabbed The tabbed modal is normally a much smaller modal and the fields required to complete the action fewer and more specific. The form has larger input fields and the labels are positioned above them. .form-container label { font-family: \"Open Sans\",\u200bHelvetica,\u200bsans-serif; font-size: 14px; font-weight: 400; color: #666; line-height: 14px; } .form-container input { /* Inherits style as defined for inputs (above) */ padding: 12px 20px 12px 15px; margin: 10px 0px 5px 0px; width: 100%; line-height: 18px; } input#search { background-image: url(\"../img/search.png\"); background-position: 14px center; background-repeat: no-repeat; height: 40px; padding: 0 20px 0 40px !important; } select[multiple] { background: #fff none repeat scroll 0 0; border: 1px solid #c9d0d3; border-radius: 4px; color: #666; font-size: 13px; font-weight: 400; padding: 10px; }","title":"Forms"},{"location":"modules/admin-ui/style/icons/","text":"Icons The icon set is carefully designed for simplicity to convey a clear and concise user experience. Having a simple but effective icon set is crucial for ease of use within sophisticated software environments. Some icons are saved as pre-rendered images and can be found in the resource folder ( .../src/main/webapp/img ). Other icons are based on Font Awesome which gives us scalable vector icons that can be customized by size, color, drop shadow, and anything that is provided by CSS. For more links and license information refer to References - Font Awesome . Note: Font Awesome contains over 630 icons, the cheat sheet of icons lists the icons and the corresponding CSS class. normal fa-rotate-90 fa-rotate-180 fa-rotate-270 fa-flip-horizontal fa-flip-vertical fa-twitter on fa-square-o fa-flag on fa-circle fa-terminal on fa-square fa-ban on fa-camera Section Navigation Tab Icons These icons are to represent different sections within the UI and should only be represented in a \u201cflat\u201d graphic style. Simplicity is key in the design of section icons. The user must easily identify the icons. Extension of these icons must represent a similar style. /* [ Inactive ] */ color : #C6C6C6 ; /* [ Active ] */ color : #A1A1A1 ; Country Icons Country icons are located at the top-right of the interface beside the user dropdown to indicate language that the UI is represented in. Each flag should: be public domain be an svg image have an aspect ratio of 3:2 be named according to the Crowdin languages be without additional decoration (official flags) A good source for these flags are the national flag articles of Wikipedia. E.g. https://en.wikipedia.org/wiki/Flag_of_Germany .nav-dd-container .lang img { border: 1px solid gray; border-radius: 1px; height: 18px; vertical-align: middle; }","title":"Icons"},{"location":"modules/admin-ui/style/icons/#icons","text":"The icon set is carefully designed for simplicity to convey a clear and concise user experience. Having a simple but effective icon set is crucial for ease of use within sophisticated software environments. Some icons are saved as pre-rendered images and can be found in the resource folder ( .../src/main/webapp/img ). Other icons are based on Font Awesome which gives us scalable vector icons that can be customized by size, color, drop shadow, and anything that is provided by CSS. For more links and license information refer to References - Font Awesome . Note: Font Awesome contains over 630 icons, the cheat sheet of icons lists the icons and the corresponding CSS class. normal fa-rotate-90 fa-rotate-180 fa-rotate-270 fa-flip-horizontal fa-flip-vertical fa-twitter on fa-square-o fa-flag on fa-circle fa-terminal on fa-square fa-ban on fa-camera","title":"Icons"},{"location":"modules/admin-ui/style/icons/#section-navigation-tab-icons","text":"These icons are to represent different sections within the UI and should only be represented in a \u201cflat\u201d graphic style. Simplicity is key in the design of section icons. The user must easily identify the icons. Extension of these icons must represent a similar style. /* [ Inactive ] */ color : #C6C6C6 ; /* [ Active ] */ color : #A1A1A1 ;","title":"Section Navigation Tab Icons"},{"location":"modules/admin-ui/style/icons/#country-icons","text":"Country icons are located at the top-right of the interface beside the user dropdown to indicate language that the UI is represented in. Each flag should: be public domain be an svg image have an aspect ratio of 3:2 be named according to the Crowdin languages be without additional decoration (official flags) A good source for these flags are the national flag articles of Wikipedia. E.g. https://en.wikipedia.org/wiki/Flag_of_Germany .nav-dd-container .lang img { border: 1px solid gray; border-radius: 1px; height: 18px; vertical-align: middle; }","title":"Country Icons"},{"location":"modules/admin-ui/style/modals/","text":"Modals Modals are a very important part of this interface. Modals are used throughout the software to control a specific function. Modals should be used for every instance of setting controls, user confirmations, exporting or importing data and other related tasks. Sizing of modals depend solely on how complex the action is. 1000 pixel width For use with complex modals. 850 pixel width For use with moderately complex modals. 600 pixel width For use with less complex modals. 400 pixel width For use with confirmation modals. Modal Types A variety of modal types are available for use based on what the modal is to be used for. When extending, please ensure only the following types of modals are used. Step-by-Step The step-by-step modal guides the user through the required steps to complete a particular task. The sequence of forms might have an impact on subsequent steps. The create event modal is a good example. Tabbed The tabbed modal is used to display information divided up into logical sections. It is not that important that the sequence of forms be maintained, validation is done on all the forms before completion of the task. The edit/create user modal as shown above demonstrates the usage of a tabbed modal. Single View Single view modals are used to display a singular form or perform a simple action.","title":"Modals"},{"location":"modules/admin-ui/style/modals/#modals","text":"Modals are a very important part of this interface. Modals are used throughout the software to control a specific function. Modals should be used for every instance of setting controls, user confirmations, exporting or importing data and other related tasks. Sizing of modals depend solely on how complex the action is.","title":"Modals"},{"location":"modules/admin-ui/style/modals/#1000-pixel-width","text":"For use with complex modals.","title":"1000 pixel width"},{"location":"modules/admin-ui/style/modals/#850-pixel-width","text":"For use with moderately complex modals.","title":"850 pixel width"},{"location":"modules/admin-ui/style/modals/#600-pixel-width","text":"For use with less complex modals.","title":"600 pixel width"},{"location":"modules/admin-ui/style/modals/#400-pixel-width","text":"For use with confirmation modals.","title":"400 pixel width"},{"location":"modules/admin-ui/style/modals/#modal-types","text":"A variety of modal types are available for use based on what the modal is to be used for. When extending, please ensure only the following types of modals are used. Step-by-Step The step-by-step modal guides the user through the required steps to complete a particular task. The sequence of forms might have an impact on subsequent steps. The create event modal is a good example. Tabbed The tabbed modal is used to display information divided up into logical sections. It is not that important that the sequence of forms be maintained, validation is done on all the forms before completion of the task. The edit/create user modal as shown above demonstrates the usage of a tabbed modal. Single View Single view modals are used to display a singular form or perform a simple action.","title":"Modal Types"},{"location":"modules/admin-ui/style/navigation/","text":"Navigation Tab The section navigation tab is placed on the left side of the viewing area for a user to easily access other sections of the Admin UI. By default the main menu is hidden to allow for the maximum viewing area and to have a clean navigation area displaying the current important location. Clicking on the main menu button displays the menu bar with all of the appropriate menu icons (Shown below). The expanded main menu consists of a number of icons, each representing an available section in the Admin UI. The main menu button consists of an anchor ( a ) tag that wraps around a text ( i ) element which has a background-image to display the icon. The text ( i ) element also contains data-title attribute that displays the tooltip (on hover over the icon) and is updated by the page translator object.","title":"Navigation"},{"location":"modules/admin-ui/style/navigation/#navigation-tab","text":"The section navigation tab is placed on the left side of the viewing area for a user to easily access other sections of the Admin UI. By default the main menu is hidden to allow for the maximum viewing area and to have a clean navigation area displaying the current important location. Clicking on the main menu button displays the menu bar with all of the appropriate menu icons (Shown below). The expanded main menu consists of a number of icons, each representing an available section in the Admin UI. The main menu button consists of an anchor ( a ) tag that wraps around a text ( i ) element which has a background-image to display the icon. The text ( i ) element also contains data-title attribute that displays the tooltip (on hover over the icon) and is updated by the page translator object.","title":"Navigation Tab"},{"location":"modules/admin-ui/style/references/","text":"Initial Admin UI Design The initial release of the Admin UI design was completed in the early part of 2015 thanks to the combined efforts of Entwine , ETH Zurich , SWITCH and University of Manchester . The result of this effort is a more technically advanced and intuitive interface that results in a capture and media management solution that is both powerful and extensible. The main focus for the new administrative ui was design and usability. While the functionality of the ui has been conceived by the solutions architects at Entwine, a professional team of designers at Espress Labs defined a consistent design language and provided the development team with design templates for the individual sections of the ui. The resulting style guide clearly describes how individual parts and pieces are to be designed, which fonts and colors are available, how notifications should be displayed to the user and much more. On top of that, the design team delivered HTML templates for each screen and dialog so that there was little left to do for the frontend engineers but turn the templates into a great looking interactive application. Original Design Document: Admin UI - A Guide to Style (Admin-UI-Style-Guide-v3.pdf) Links: Entwine (http://entwinemedia.com/) ETH Zurich (https://www.ethz.ch/en.html) SWITCH (https://www.switch.ch/) University of Manchester (http://www.manchester.ac.uk/) Espress Labs (http://espresslabs.com/) Open Sans Open Sans is a humanist sans serif typeface designed by Steve Matteson, Type Director of Ascender Corp. This version contains the complete 897 character set, which includes the standard ISO Latin 1, Latin CE, Greek and Cyrillic character sets. Open Sans was designed with an upright stress, open forms and a neutral, yet friendly appearance. It was optimized for print, web, and mobile interfaces, and has excellent legibility characteristics in its letterforms. Link: Google Fonts - Open Sans (https://fonts.google.com/specimen/Open+Sans) License: Apache License, Version 2.0 (http://www.apache.org/licenses/LICENSE-2.0) Font Awesome Font Awesome gives you scalable vector icons that can instantly be customized \u2014 size, color, drop shadow, and anything that can be done with the power of CSS. Font Awesome is fully open source and is GPL friendly. You can use it for commercial projects, open source projects, or really just about whatever you want. Link: Font Awesome (http://fontawesome.io/) License: Font Awesome - License (http://fontawesome.io/license/) Sass Sass is the most mature, stable, and powerful professional grade CSS extension language in the world. Sass is completely compatible with all versions of CSS. We take this compatibility seriously, so that you can seamlessly use any available CSS libraries. Link: Sass (http://sass-lang.com/) HTML HTML5 is a markup language used for structuring and presenting content on the World Wide Web. It is the fifth and current version of the HTML standard. It was published in October 2014 by the World Wide Web Consortium (W3C) to improve the language with support for the latest multimedia, while keeping it both easily readable by humans and consistently understood by computers and devices such as web browsers, parsers, etc. HTML5 is intended to subsume not only HTML 4, but also XHTML 1 and DOM Level 2 HTML. HTML5 includes detailed processing models to encourage more interoperable implementations; it extends, improves and rationalizes the markup available for documents, and introduces markup and application programming interfaces (APIs) for complex web applications. For the same reasons, HTML5 is also a candidate for cross-platform mobile applications, because it includes features designed with low-powered devices in mind. Links: HTML 5 (https://www.w3.org/TR/html5/) HTML Validator (https://validator.w3.org/)","title":"References"},{"location":"modules/admin-ui/style/references/#initial-admin-ui-design","text":"The initial release of the Admin UI design was completed in the early part of 2015 thanks to the combined efforts of Entwine , ETH Zurich , SWITCH and University of Manchester . The result of this effort is a more technically advanced and intuitive interface that results in a capture and media management solution that is both powerful and extensible. The main focus for the new administrative ui was design and usability. While the functionality of the ui has been conceived by the solutions architects at Entwine, a professional team of designers at Espress Labs defined a consistent design language and provided the development team with design templates for the individual sections of the ui. The resulting style guide clearly describes how individual parts and pieces are to be designed, which fonts and colors are available, how notifications should be displayed to the user and much more. On top of that, the design team delivered HTML templates for each screen and dialog so that there was little left to do for the frontend engineers but turn the templates into a great looking interactive application. Original Design Document: Admin UI - A Guide to Style (Admin-UI-Style-Guide-v3.pdf) Links: Entwine (http://entwinemedia.com/) ETH Zurich (https://www.ethz.ch/en.html) SWITCH (https://www.switch.ch/) University of Manchester (http://www.manchester.ac.uk/) Espress Labs (http://espresslabs.com/)","title":"Initial Admin UI Design"},{"location":"modules/admin-ui/style/references/#open-sans","text":"Open Sans is a humanist sans serif typeface designed by Steve Matteson, Type Director of Ascender Corp. This version contains the complete 897 character set, which includes the standard ISO Latin 1, Latin CE, Greek and Cyrillic character sets. Open Sans was designed with an upright stress, open forms and a neutral, yet friendly appearance. It was optimized for print, web, and mobile interfaces, and has excellent legibility characteristics in its letterforms. Link: Google Fonts - Open Sans (https://fonts.google.com/specimen/Open+Sans) License: Apache License, Version 2.0 (http://www.apache.org/licenses/LICENSE-2.0)","title":"Open Sans"},{"location":"modules/admin-ui/style/references/#font-awesome","text":"Font Awesome gives you scalable vector icons that can instantly be customized \u2014 size, color, drop shadow, and anything that can be done with the power of CSS. Font Awesome is fully open source and is GPL friendly. You can use it for commercial projects, open source projects, or really just about whatever you want. Link: Font Awesome (http://fontawesome.io/) License: Font Awesome - License (http://fontawesome.io/license/)","title":"Font Awesome"},{"location":"modules/admin-ui/style/references/#sass","text":"Sass is the most mature, stable, and powerful professional grade CSS extension language in the world. Sass is completely compatible with all versions of CSS. We take this compatibility seriously, so that you can seamlessly use any available CSS libraries. Link: Sass (http://sass-lang.com/)","title":"Sass"},{"location":"modules/admin-ui/style/references/#html","text":"HTML5 is a markup language used for structuring and presenting content on the World Wide Web. It is the fifth and current version of the HTML standard. It was published in October 2014 by the World Wide Web Consortium (W3C) to improve the language with support for the latest multimedia, while keeping it both easily readable by humans and consistently understood by computers and devices such as web browsers, parsers, etc. HTML5 is intended to subsume not only HTML 4, but also XHTML 1 and DOM Level 2 HTML. HTML5 includes detailed processing models to encourage more interoperable implementations; it extends, improves and rationalizes the markup available for documents, and introduces markup and application programming interfaces (APIs) for complex web applications. For the same reasons, HTML5 is also a candidate for cross-platform mobile applications, because it includes features designed with low-powered devices in mind. Links: HTML 5 (https://www.w3.org/TR/html5/) HTML Validator (https://validator.w3.org/)","title":"HTML"},{"location":"modules/admin-ui/style/spacing/","text":"Logo Spacing The Logo is placed at the very top-left of the view with a height of 20 pixels. The color fill for the logo is a single distinctive color ( #FFFFFF ) to be in contrast to the background color of the main header bar.","title":"Logo"},{"location":"modules/admin-ui/style/spacing/#logo-spacing","text":"The Logo is placed at the very top-left of the view with a height of 20 pixels. The color fill for the logo is a single distinctive color ( #FFFFFF ) to be in contrast to the background color of the main header bar.","title":"Logo Spacing"},{"location":"modules/admin-ui/style/typeface/","text":"Typeface Open Sans is an extremely versatile Humanist Sans-Serif typeface designed by Steve Matteson. Delivering a crystal clear look and feel, Open Sans enables users to quickly navigate the software without any clutter. The strengths of this typeface surpass both web and print applications offering excellent legibility for both forms of media. Open Sans is a widely available free font offered through Google Fonts and is widely seen as a web standard face. Please see References - Open Sans for license information. Light Aa Bb Cc Dd Ee Ff Gg Hh Ii Jj Kk Ll Mm Nn Oo Pp Qq Rr Ss Tt Uu Vv Ww Xx Yy Zz font-weight : 300 ; font-style : normal ; Light Italic Aa Bb Cc Dd Ee Ff Gg Hh Ii Jj Kk Ll Mm Nn Oo Pp Qq Rr Ss Tt Uu Vv Ww Xx Yy Zz font-weight : 300 ; font-style : italic ; Regular Aa Bb Cc Dd Ee Ff Gg Hh Ii Jj Kk Ll Mm Nn Oo Pp Qq Rr Ss Tt Uu Vv Ww Xx Yy Zz font-weight : 400 ; font-style : normal ; Regular Italic Aa Bb Cc Dd Ee Ff Gg Hh Ii Jj Kk Ll Mm Nn Oo Pp Qq Rr Ss Tt Uu Vv Ww Xx Yy Zz font-weight : 400 ; font-style : italic ; Semibold Aa Bb Cc Dd Ee Ff Gg Hh Ii Jj Kk Ll Mm Nn Oo Pp Qq Rr Ss Tt Uu Vv Ww Xx Yy Zz font-weight : 600 ; font-style : normal ; Semibold Italic Aa Bb Cc Dd Ee Ff Gg Hh Ii Jj Kk Ll Mm Nn Oo Pp Qq Rr Ss Tt Uu Vv Ww Xx Yy Zz font-weight : 600 ; font-style : italic ; Bold Aa Bb Cc Dd Ee Ff Gg Hh Ii Jj Kk Ll Mm Nn Oo Pp Qq Rr Ss Tt Uu Vv Ww Xx Yy Zz font-weight : 700 ; font-style : normal ; Bold Italic Aa Bb Cc Dd Ee Ff Gg Hh Ii Jj Kk Ll Mm Nn Oo Pp Qq Rr Ss Tt Uu Vv Ww Xx Yy Zz font-weight : 700 ; font-style : italic ; Extrabold Aa Bb Cc Dd Ee Ff Gg Hh Ii Jj Kk Ll Mm Nn Oo Pp Qq Rr Ss Tt Uu Vv Ww Xx Yy font-weight : 800 ; font-style : normal ; Extrabold Italic Aa Bb Cc Dd Ee Ff Gg Hh Ii Jj Kk Ll Mm Nn Oo Pp Qq Rr Ss Tt Uu Vv Ww Xx Yy Zz font-weight : 800 ; font-style : italic ; Usage body { font-family: \"Open Sans\",Helvetica,sans-serif; font-size: 14px; font-weight: 400; font-style: normal; line-height: 1; } Header Bar and Navigation #user-dd { /* [ User Profile ] */ font-family: \"Open Sans\",Helvetica,sans-serif; font-size: 12px; font-weight: 400; font-style: normal; color: #ffffff; /* rgb(255, 255, 255) */ } button { /* [ Action Button ] */ font-family: \"Open Sans\",Helvetica,sans-serif; font-size: 12px; font-weight: 600; font-style: normal; color: #ffffff; /* rgb(255, 255, 255) */ } nav a { /* [ Navigation Link - standard ] */ font-family: \"Open Sans\",Helvetica,sans-serif; font-size: 14px; font-weight: 600; font-style: normal; color: #5d7589; /* rgb(93,\u200b 117,\u200b 137) */ } nav a.active { /* [ Navigation Link - current / active ] */ color: #fafafa; /* rgb(250,\u200b 250,\u200b 250) */ } Table View h1 { /* [ Table Header ] */ font-size: 23px; font-weight: 100; font-style: normal; color: #46647e; /* rgb(70,\u200b 100,\u200b 126) */ } h4 { /* [ Table sub-heading ] */ font-family: \"Open Sans\",Helvetica,sans-serif; font-size: 11px; font-weight: 400; font-style: normal; color: #666666; /* rgb(102,\u200b 102,\u200b 102) */ } .filters { /* [ Filter text ] */ font-family: \"Open Sans\",Helvetica,sans-serif; font-size: 12px; font-weight: 600; font-style: normal; color: #666666; /* rgb(102,\u200b 102,\u200b 102) */ } input#search { /* [ search input box ] */ font-family: \"Open Sans\",Helvetica,sans-serif; font-size: 13px; font-weight: 600; font-style: normal; color: #666666; /* rgb(102,\u200b 102,\u200b 102) */ } .filters .ng-multi-value { /* [ value to be filtered by ] */ font-family: \"Open Sans\",Helvetica,sans-serif; font-size: 11px; font-weight: 400; font-style: normal; color: #ffffff; /* rgb(255, 255, 255) */ } th { /* [ Table header ] */ font-family: \"Open Sans\",Helvetica,sans-serif; font-size: 13px; font-weight: 600; font-style: normal; color: #666666; /* rgb(102,\u200b 102,\u200b 102) */ } td { /* [ table data / cell ] */ font-family: \"Open Sans\",Helvetica,sans-serif; font-size: 12px; font-weight: 400; font-style: normal; color: #666666; /* rgb(102,\u200b 102,\u200b 102) */ } .action-bar a { /* [ Edit button on table header ] */ font-family: \"Open Sans\",Helvetica,sans-serif; font-size: 14px; font-weight: 400; font-style: normal; color: #3aa5ef; /* rgb(58,\u200b 165,\u200b 239) */ }","title":"Typeface"},{"location":"modules/admin-ui/style/typeface/#typeface","text":"Open Sans is an extremely versatile Humanist Sans-Serif typeface designed by Steve Matteson. Delivering a crystal clear look and feel, Open Sans enables users to quickly navigate the software without any clutter. The strengths of this typeface surpass both web and print applications offering excellent legibility for both forms of media. Open Sans is a widely available free font offered through Google Fonts and is widely seen as a web standard face. Please see References - Open Sans for license information. Light Aa Bb Cc Dd Ee Ff Gg Hh Ii Jj Kk Ll Mm Nn Oo Pp Qq Rr Ss Tt Uu Vv Ww Xx Yy Zz font-weight : 300 ; font-style : normal ; Light Italic Aa Bb Cc Dd Ee Ff Gg Hh Ii Jj Kk Ll Mm Nn Oo Pp Qq Rr Ss Tt Uu Vv Ww Xx Yy Zz font-weight : 300 ; font-style : italic ; Regular Aa Bb Cc Dd Ee Ff Gg Hh Ii Jj Kk Ll Mm Nn Oo Pp Qq Rr Ss Tt Uu Vv Ww Xx Yy Zz font-weight : 400 ; font-style : normal ; Regular Italic Aa Bb Cc Dd Ee Ff Gg Hh Ii Jj Kk Ll Mm Nn Oo Pp Qq Rr Ss Tt Uu Vv Ww Xx Yy Zz font-weight : 400 ; font-style : italic ; Semibold Aa Bb Cc Dd Ee Ff Gg Hh Ii Jj Kk Ll Mm Nn Oo Pp Qq Rr Ss Tt Uu Vv Ww Xx Yy Zz font-weight : 600 ; font-style : normal ; Semibold Italic Aa Bb Cc Dd Ee Ff Gg Hh Ii Jj Kk Ll Mm Nn Oo Pp Qq Rr Ss Tt Uu Vv Ww Xx Yy Zz font-weight : 600 ; font-style : italic ; Bold Aa Bb Cc Dd Ee Ff Gg Hh Ii Jj Kk Ll Mm Nn Oo Pp Qq Rr Ss Tt Uu Vv Ww Xx Yy Zz font-weight : 700 ; font-style : normal ; Bold Italic Aa Bb Cc Dd Ee Ff Gg Hh Ii Jj Kk Ll Mm Nn Oo Pp Qq Rr Ss Tt Uu Vv Ww Xx Yy Zz font-weight : 700 ; font-style : italic ; Extrabold Aa Bb Cc Dd Ee Ff Gg Hh Ii Jj Kk Ll Mm Nn Oo Pp Qq Rr Ss Tt Uu Vv Ww Xx Yy font-weight : 800 ; font-style : normal ; Extrabold Italic Aa Bb Cc Dd Ee Ff Gg Hh Ii Jj Kk Ll Mm Nn Oo Pp Qq Rr Ss Tt Uu Vv Ww Xx Yy Zz font-weight : 800 ; font-style : italic ;","title":"Typeface"},{"location":"modules/admin-ui/style/typeface/#usage","text":"body { font-family: \"Open Sans\",Helvetica,sans-serif; font-size: 14px; font-weight: 400; font-style: normal; line-height: 1; }","title":"Usage"},{"location":"modules/admin-ui/style/typeface/#header-bar-and-navigation","text":"#user-dd { /* [ User Profile ] */ font-family: \"Open Sans\",Helvetica,sans-serif; font-size: 12px; font-weight: 400; font-style: normal; color: #ffffff; /* rgb(255, 255, 255) */ } button { /* [ Action Button ] */ font-family: \"Open Sans\",Helvetica,sans-serif; font-size: 12px; font-weight: 600; font-style: normal; color: #ffffff; /* rgb(255, 255, 255) */ } nav a { /* [ Navigation Link - standard ] */ font-family: \"Open Sans\",Helvetica,sans-serif; font-size: 14px; font-weight: 600; font-style: normal; color: #5d7589; /* rgb(93,\u200b 117,\u200b 137) */ } nav a.active { /* [ Navigation Link - current / active ] */ color: #fafafa; /* rgb(250,\u200b 250,\u200b 250) */ }","title":"Header Bar and Navigation"},{"location":"modules/admin-ui/style/typeface/#table-view","text":"h1 { /* [ Table Header ] */ font-size: 23px; font-weight: 100; font-style: normal; color: #46647e; /* rgb(70,\u200b 100,\u200b 126) */ } h4 { /* [ Table sub-heading ] */ font-family: \"Open Sans\",Helvetica,sans-serif; font-size: 11px; font-weight: 400; font-style: normal; color: #666666; /* rgb(102,\u200b 102,\u200b 102) */ } .filters { /* [ Filter text ] */ font-family: \"Open Sans\",Helvetica,sans-serif; font-size: 12px; font-weight: 600; font-style: normal; color: #666666; /* rgb(102,\u200b 102,\u200b 102) */ } input#search { /* [ search input box ] */ font-family: \"Open Sans\",Helvetica,sans-serif; font-size: 13px; font-weight: 600; font-style: normal; color: #666666; /* rgb(102,\u200b 102,\u200b 102) */ } .filters .ng-multi-value { /* [ value to be filtered by ] */ font-family: \"Open Sans\",Helvetica,sans-serif; font-size: 11px; font-weight: 400; font-style: normal; color: #ffffff; /* rgb(255, 255, 255) */ } th { /* [ Table header ] */ font-family: \"Open Sans\",Helvetica,sans-serif; font-size: 13px; font-weight: 600; font-style: normal; color: #666666; /* rgb(102,\u200b 102,\u200b 102) */ } td { /* [ table data / cell ] */ font-family: \"Open Sans\",Helvetica,sans-serif; font-size: 12px; font-weight: 400; font-style: normal; color: #666666; /* rgb(102,\u200b 102,\u200b 102) */ } .action-bar a { /* [ Edit button on table header ] */ font-family: \"Open Sans\",Helvetica,sans-serif; font-size: 14px; font-weight: 400; font-style: normal; color: #3aa5ef; /* rgb(58,\u200b 165,\u200b 239) */ }","title":"Table View"},{"location":"modules/admin-ui/style/view-structure/","text":"Main View Structure The Admin UI is divided up into four main sections. 1 The header bar contains the logo, link to help and playback, user profile and language options. 2 The action bar is made up of navigational elements such as the navigation tabs, main menu and/or action buttons. 3 This is the main \"work\" area of the page where the content is loaded; replacing the existing elements depending on the menu item that is selected. 4 The footer holds the version information. Table View Structure The table view structure is used to display the majority of the information of the system and is used as entry point to the display of information in a modal or more intricate displays. 1 The header of the table view contains a section that shows the current table heading (with a sub-heading) and a section to filter and/or search the table. To filter the table click on the add filter icon and select the column and value to filter the table on. Multiple filters can be selected at any one time. Filters and search values can be saved and given a custom name and description for future use. 2 As with any other HTML table the structure is divided into a header and a body. The header consists of columns, that are sortable, and a link to edit the columns and their order. The table body contains the information of an item in a row and the action buttons that allows a user to interact with that item. 3 The table footer contains a page selector and a dropdown to change the number of rows that are shown.","title":"View Structure"},{"location":"modules/admin-ui/style/view-structure/#main-view-structure","text":"The Admin UI is divided up into four main sections. 1 The header bar contains the logo, link to help and playback, user profile and language options. 2 The action bar is made up of navigational elements such as the navigation tabs, main menu and/or action buttons. 3 This is the main \"work\" area of the page where the content is loaded; replacing the existing elements depending on the menu item that is selected. 4 The footer holds the version information.","title":"Main View Structure"},{"location":"modules/admin-ui/style/view-structure/#table-view-structure","text":"The table view structure is used to display the majority of the information of the system and is used as entry point to the display of information in a modal or more intricate displays. 1 The header of the table view contains a section that shows the current table heading (with a sub-heading) and a section to filter and/or search the table. To filter the table click on the add filter icon and select the column and value to filter the table on. Multiple filters can be selected at any one time. Filters and search values can be saved and given a custom name and description for future use. 2 As with any other HTML table the structure is divided into a header and a body. The header consists of columns, that are sortable, and a link to edit the columns and their order. The table body contains the information of an item in a row and the action buttons that allows a user to interact with that item. 3 The table footer contains a page selector and a dropdown to change the number of rows that are shown.","title":"Table View Structure"},{"location":"modules/capture-agent/capture-agent/","text":"Introduction This guide describes the communication protocol between Opencast and any Capture Agent. For the sake of simplicity, the following variables are used throughout this guide: $HOST is your core's base URL $AGENT_NAME is the agent's name $RECORDING_ID is the recording's ID Basic Rules The core MUST NOT attempt to connect to the agent, communication is always happening from agent to core The agent MUST get the endpoint location from the service registry The agent MUST try to send its recording states to the core on a regular basis during recordings The agent SHOULD send its state to the core on a semi regular basis The agent MAY send its capabilities to the core on a semi regular basis The agent MUST attempt to update its calendaring data regularly The agent MUST must capture with all available inputs if no inputs are selected The agent MAY tell the core the address of its web interface Quick Overview The following list gives a short overview over the communication between agent and core. Remember, it is up to the agent to initiate the connections. Also note that ideally some operations may run in parallel. request rest endpoints register agent and set status set the agents capabilities repeat: while no recording get schedule/calendar set agent and recording state start recording set agent and recording state update ingest endpoints ingest recording set agent and recording state API Stability The Opencast community takes care to avoid any disruptive modifications to the APIs described in this document to prevent hardware integrations from breaking since they are notorious hard to fix. This means that you can assume the API to be stable for a long time. The same is not true for other parts of the API and you should therefore avoid integrating hardware with other parts of Opencast's API Authentication Opencast supports two types of authentication which can be used by capture agents: (Backend) HTTP Digest authentication which is historically used for machine-to-machine communication. (General) HTTP Basic (or session-based) authentication used for both front-end users and integrations. HTTP Digest authentication is the legacy option for capture agents. It is still widely used today and will continue to be supported. HTTP Digest is more complicated and has the disadvantage that users need to be specified separately in the backend. There is a global HTTP Digest user with admin privileges but specific capture agent users with limited privileges can be defined as well. When using HTTP Digest authentication, you need to send the additional header X-Requested-Auth: Digest : curl --digest -u opencast_system_account:CHANGE_ME -H \"X-Requested-Auth: Digest\" \\ https://develop.opencast.org/info/me.json HTTP Basic authentication can be used with users defined via web-interface or via any regular user provider. A request using HTTP Basic does not need to specify any additional headers: curl -u admin:opencast https://develop.opencast.org/info/me.json Generally, we recommend using HTTP Basic authentication since it's easier for adopters to manage capture agent users via the admin interface and does not rely on hidden users while being less complicated at the same time. Whatever authentication method you choose to implement \u2013 maybe even both, allowing users to choose for themselves \u2013 please clearly specify what authentication you expect since users have to provide different types of users which can easily lead to usability problems if not clearly marked. Action Details This is a detailed description of the steps described in the quick overview section. For details about the REST endpoints, please always consult the Opencast REST documentation which can be found in the top-right corner of the administrative user interface of each running Opencast instance. Note that most endpoints can handle both JSON and XML although throughout this guide, for all examples, only JSON is used. Get Endpoint Locations From Service Registry First, you need to get the locations of the REST endpoints to talk to. These information can be retrieved from the central service registry. It is likely that Opencast is running as a distributed system which means you cannot expect all endpoints on a single host. Three endpoint locations need to be requested: The capture-admin endpoint to register the agent and set status and configuration: org.opencastproject.capture.admin The scheduler endpoint to get the current schedule for the agent from: org.opencastproject.scheduler The ingest endpoint to upload the recordings to once they are done: org.opencastproject.ingest To ask for an endpoint you would send an HTTP GET request to ${HOST}/services/available.json?serviceType=<SERVICE> A result would look like { \"services\" : { \"service\" : { \"active\" : true, \"host\" : \"http://example.opencast.org:8080\", \"path\" : \"/capture-admin\", ... } } } These endpoints should be requested once when starting up the capture agent. While the capture-admin and scheduler endpoints may then be assumed to never change during the runtime of the capture agent, the ingest endpoint may change and the data should be re-requested every time before uploading (ingesting) data to the core. Register the Capture Agent and set Current Status Once the endpoints to talk to are known, it is time to register the capture agent at the core so that scheduled events can be added. This can be done by sending an HTTP POST request to: ${CAPTURE-ADMIN-ENDPOINT}/agents/<name> \u2026including the following data fields: state=idle address=http(s)://<ca-web-ui> The name has to be a unique identifier of a single capture agent. Using the same name for several agents would mean sharing the same schedule and status which in general should be avoided. Sending this request will register the capture agent. After this, the capture agent should appear in the admin interface and schedules can be added for this agent. Setting address is optional. It can be used to link an administrative web interface of a capture agent. Setting Agent Capabilities Additional to registering, the agent may set its capabilities allowing users to select possible inputs in the user interface of Opencast when scheduling events. The configuration may be set as XML or JSON representation of a Java properties file and can be set via an HTTP POST request to: /capture-admin/agents/$AGENT_NAME/configuration Getting the Calendar/Schedule The calendar can be retrieved by sending an HTTP GET request to ${SCHEDULER-ENDPOINT}/calendars?agentid=<name> The format returned is iCal. The file contains all scheduled upcoming recordings the capture agent should handle. The output will look like this (base64 encoded parts are shortened): BEGIN:VCALENDAR PRODID:Opencast Calendar File 0.5 VERSION:2.0 CALSCALE:GREGORIAN BEGIN:VEVENT DTSTAMP:20200616T124513Z DTSTART:20200616T124200Z DTEND:20200616T124400Z SUMMARY:Demo event UID:68b19d11-6aca-413e-b2b3-eda72eebc65a LAST-MODIFIED:20200616T124119Z DESCRIPTION:demo LOCATION:my_capture_agent_name ATTACH;FMTTYPE=application/xml;VALUE=BINARY;ENCODING=BASE64;X-APPLE-FILENAME=episode.xml:... ATTACH;FMTTYPE=application/text;VALUE=BINARY;ENCODING=BASE64;X-APPLE-FILENAME=org.opencastproject.capture.agent.properties:... END:VEVENT END:VCALENDAR The iCal event contains start and end dates, all meta data catalogs for the event, the UID which is the %RECORDING_ID and which important when uploading media later on and additional capture agent properties which should be passed on. Note that most meta data like the dublin core catalogs can in most cases be ignored. In most cases, this means the data you are interested in per event are: DTSTART : Date to starte the recording at DTEND : Date at which to stop the recording UID : Recording identifier used for updating the recording status and associating uploads with a scheduled event LOCATION : This should always match your agent's name ATTACH;...FILENAME=org.opencastproject.capture.agent.properties : Agent properties to pass on in case of workflow properties Depending on the amount of recordings scheduled for the particular capture agent, this file may become very large. That is why there are two way of limiting the amount of necessary data to transfer and process: Sending the optional parameter cutoff to limit the schedule to a particular time span in the future. ${SCHEDULER-ENDPOINT}/calendars?agentid= &cutoff= The value for cutoff is a Unix timestamp in milliseconds from now. Events beginning after this time will not be included in the returned schedule. Use the HTTP ETag and If-Not-Modified header to have Opencast only sent schedules when they have actually changed. Set Agent and Recording State Setting the agent state is identical to the registration of the capture agent and done by sending an HTTP POST request to: ${CAPTURE-ADMIN-ENDPOINT}/agents/<name> \u2026including the following data fields: state=capturing address=http(s)://<ca-web-ui> Additionally, set the recording state using the event's UID obtained from the schedule with an HTTP POST request to ${CAPTURE-ADMIN-ENDPOINT}/recordings/<recording_id> \u2026including the data field: state=capturing Recording This task is device specific. Use whatever means necessary to get the recording done. Set Agent and Recording State This step is identical to the previous status update but for the state. If the recording has failed, the recording state is updated with capture_error while the agent's state is set back to idle if the error is non-permanent and to error if it is permanent and block further recordings. If the recording was successful, both states are set to uploading . Get Ingest Endpoint Locations From Service Registry This step is identical to first request to the service registry expect that it is sufficient to request the location for the service org.opencastproject.ingest . If this request fails, assume the old data to be valid. Ingest (Upload) Recording Use the ingest endpoint to upload the recording. There are multiple different methods to ingest media. Please refer to the REST endpoint documentation for details of these methods. The most commonly used are: Single request ingest using an HTTP POST request to ${INGEST-ENDPOINT}/addMediaPackage Multi request ingest using HTTP POST requests to ${INGEST-ENDPOINT}/createMediaPackage ${INGEST-ENDPOINT}/addDCCatalog ${INGEST-ENDPOINT}/addTrack ${INGEST-ENDPOINT}/ingest Please make sure that the event's UID is passed on as workfloeInstanceId to the final call to /ingest/ingest to match the scheduled event to the media being uploaded. If possible, please follow these additional rules about recording files: Recordings may be deleted if the ingest was successful. Recordings should be stored in case of a failure. Upload Metadata The calendar (iCal) with the scheduled events retrieved in an earlier step also contains metadata catalogs as attached files. To modify metadata, these catalogs can be modified and ingested as well. Opencast's default setting is to use these for updating the existing metadata in the system. If no metadata modifications are required (usual case), please do not modify these files and do not upload them. In short: Ignore these attachments Additional note for Opencast \u2264 3.x: Opencast only creates events in the database after ingesting the files. Scheduled data are kept separately. That is why for these Opencast versions, all metadata files need to be ingested. Usually, that means to take the metadata catalogs from the schedule and ingest them unmodified using for example the /addDCCatalog endpoint. Set Agent and Recording State Again, this step is identical to the previous status updates except for the state. If the upload has failed, the recording state is updated with upload_error while the agents state is set back to idle if the error is non-permanent or to error otherwise. If the upload was successful, the recording status is set to upload_finished while the agents state is set back to idle . Agent State And Configuration This section describes some additional aspects of the communication between capture agent and the Opencast core. Creating An Agent On The Core An agent record is created on the core the first time the agent communicates with the core. There is no special endpoint or registration required, just send the state and the agent record will be created. Agent State Additional to the required status updates outlined above, the agent should continue to send this status information on a regular basis to allow Opencast to determine that the agent is still active. If the agent fails to do so, it may be marked as offline in the Opencast user interface after a certain amount of time (The default is 120min). Agent Configuration If a special configuration is required, the agent should send its configuration data in a regular interval to ensure Opencast has the updated configuration even if the core is reset in the meantime. It should also send the configuration when the agent's configuration changes to avoid conflicts between selected and available options. The format of this XML structure is the following: <?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?> <!DOCTYPE properties SYSTEM \"http://java.sun.com/dtd/properties.dtd\\\"> <properties> <comment>Capabilities for $AGENT_NAME</comment> <entry key=\"$KEY_NAME\">$VALUE</entry> \u2026 </properties> If sent as JSON, the format is a simple JSON object: { 'key':'value', 'key2':'value2' } To specify inputs the user can select, the special key capture.device.names is used. It is a comma separated list of inputs which will be presented in the Opencast user interface. Recording State If the agent is processing (recording) a previously scheduled event, it must send the recording's state to the core. It may do this on a regular basis but at least should do this once the state of the recording changes since, for example, the recording process has started. Note that these status changes are used in the administrative user interface and failing to set a state may cause the interface to display a warning such as \u201cThe agent may have failed to start this recording\u201d. To send the recording's state to the core, a valid state (as defined here) is sent via HTTP POST to: ${CAPTURE-ADMIN-ENDPOINT}/recordings/<recording_id> Calendaring Data Agent's are expected to understand Opencast's iCalendar implementation. They should poll the calendar endpoint in a regular interval to update their internal schedule. Agent's should use a permanent cache (e.g. disk or database) for the cached schedule to be able to handle power and/or network failures gracefully. This also allows an agent to be used in a network-less environment, for example for mobile recordings: Merely cache the calendar data once after which the agent is brought to its destination where it will capture and cache the pre-scheduled recordings. To retrieve the calendar for an agent an HTTP GET is performed to ${SCHEDULER-ENDPOINT}/calendars?agentid=<name> Note that the schedule has ETag support, which is very useful to speed up the processing of larger calendars. Capture Agent Configuration File TODO: Verify that this is still necessary with \u2265 4.0 One file attached to each scheduled event is org.opencastproject.capture.agent.properties . This file contains the capture agent configuration directives (e.g. turning inputs on and off) as well as workflow directives which are important for the ingest process without which the core may misbehave. All keys in this file contain prefixes identifying the type of property. For example, workflow directives are prefixed by org.opencastproject.workflow.config . When passing the configuration directive to the core using the file ingest REST endpoints, the agent must usually strip this prefix from the parameter. For example, org.opencastproject.workflow.config.trimHold=true should be passed as trimHold=true . The org.opencastproject.workflow.definition directive is important as well, this is the workflow definition identifier and should be passed as a parameter during the ingest operation. Example configuration file: #Capture Agent specific data #Tue May 22 17:34:22 CST 2012 org.opencastproject.workflow.config.trimHold=true capture.device.names=MOCK_SCREEN,MOCK_PRESENTER,MOCK_MICROPHONE org.opencastproject.workflow.definition=full event.title=Test Capture event.location=demo_capture_agent Ingesting Media Opencast provides several different methods to ingest media, with each having some advantages and disadvantages. The following description will give a short overview of the different methods. For more details, again, have a look at the REST endpoint documentation of the ingest service. Multi Request Ingest This is the recommended way to use for most capture agents. It offers the most features to use and does not require any pre- and post-processing of ingested material. Using this method, a number of successive HTTP calls are made during the ingestion of media. The result of a successful call is the newly updated media package. This media package is created by one call, then amended by a number of other calls, each adding additional elements like tracks, attachments or metadata catalohs. Finally, it is then passed to the last endpoint to begin processing. The advantage to this process is that in case of a network failure only one particular element needs to be repeated in contrast to repeating the whole process required by all other ingest methods. To begin, the agent must first generate a valid media package. This is done via an HTTP GET request to ${INGEST-ENDPOINT}/createMediaPackage . The resulting media package will contain the base skeleton used in later calls. Each following call will require a media package as input and will modify and return it to be used for the next call. The next step(s) vary. Essentially, each generated file for a recording must be added, one at a time, to the media package. For this, an agent may use the following REST endpoints: ${INGEST-ENDPOINT}/addTrack to add media files (video, audio, \u2026) used for processing and/or publication ${INGEST-ENDPOINT}/addDCCatalog to add the dublin core metadata catalogs like the basic episode metadata (title, \u2026) ${INGEST-ENDPOINT}/addCatalog to add all types of metadata catalogs. This is a more general version of addDCCatalog and is seldom necessary. ${INGEST-ENDPOINT}/addAttachment to add arbitrary attachments (cover images, access control catalogs, \u2026) to the media package. Finally, once you have added all files, it is time to ingest the media package and begin processing. After this, no further files can be added. To ingest a recording, an HTTP POST is sent to ${INGEST-SERVICE}/ingest . Single Request Ingest The single request ingest will, as its name implies, handle the whole process as part of a single HTTP request. This is a convenient way of adding smaller ingest since the implementation does not require to store any internal state. The operation is atomic after all: Either it succeeds or fails. The disadvantage to this is that the complexity of ingests is limited, e.g. no attachments can be added to the media package this way, and a failure means that all files need to be re-transferred. For this method, the agent posts all data to ${INGEST-ENDPOINT}/addMediaPackage . Zipped Media Ingest In general, the use of this method is discouraged because of the additional load for packing and unpacking the material compared to the negligible gain. For this method, the captured media, along with some metadata files is zipped and then HTTP POSTed to the core. The core then unzips the media package and begins processing. This unzipping operation is quite disk intensive, and the REST endpoint does not return until the unzipping is done. Thus, please beware of proxy timeouts and additional disk utilization. To ingest a zipped media package an HTTP POST is performed to ${INGEST-ENDPOINT}/addZippedMediaPackage . The BODY of the POST must contain the zipped media package. Further Reading The communication involve several REST endpoints. Additional documentation about these can be found in the REST docs of the specific service. The REST documentation can be found at /rest_docs.html in every Opencast instance to reflect that servers unique capabilities. Services involved in the communication with the capture agent are: The capture admin service used to register the capture agent and set its current status. The scheduler service to get scheduled recordings for an agent. The ingest service to upload recording files and start processing.","title":"Capture Agent"},{"location":"modules/capture-agent/capture-agent/#introduction","text":"This guide describes the communication protocol between Opencast and any Capture Agent. For the sake of simplicity, the following variables are used throughout this guide: $HOST is your core's base URL $AGENT_NAME is the agent's name $RECORDING_ID is the recording's ID","title":"Introduction"},{"location":"modules/capture-agent/capture-agent/#basic-rules","text":"The core MUST NOT attempt to connect to the agent, communication is always happening from agent to core The agent MUST get the endpoint location from the service registry The agent MUST try to send its recording states to the core on a regular basis during recordings The agent SHOULD send its state to the core on a semi regular basis The agent MAY send its capabilities to the core on a semi regular basis The agent MUST attempt to update its calendaring data regularly The agent MUST must capture with all available inputs if no inputs are selected The agent MAY tell the core the address of its web interface","title":"Basic Rules"},{"location":"modules/capture-agent/capture-agent/#quick-overview","text":"The following list gives a short overview over the communication between agent and core. Remember, it is up to the agent to initiate the connections. Also note that ideally some operations may run in parallel. request rest endpoints register agent and set status set the agents capabilities repeat: while no recording get schedule/calendar set agent and recording state start recording set agent and recording state update ingest endpoints ingest recording set agent and recording state","title":"Quick Overview"},{"location":"modules/capture-agent/capture-agent/#api-stability","text":"The Opencast community takes care to avoid any disruptive modifications to the APIs described in this document to prevent hardware integrations from breaking since they are notorious hard to fix. This means that you can assume the API to be stable for a long time. The same is not true for other parts of the API and you should therefore avoid integrating hardware with other parts of Opencast's API","title":"API Stability"},{"location":"modules/capture-agent/capture-agent/#authentication","text":"Opencast supports two types of authentication which can be used by capture agents: (Backend) HTTP Digest authentication which is historically used for machine-to-machine communication. (General) HTTP Basic (or session-based) authentication used for both front-end users and integrations. HTTP Digest authentication is the legacy option for capture agents. It is still widely used today and will continue to be supported. HTTP Digest is more complicated and has the disadvantage that users need to be specified separately in the backend. There is a global HTTP Digest user with admin privileges but specific capture agent users with limited privileges can be defined as well. When using HTTP Digest authentication, you need to send the additional header X-Requested-Auth: Digest : curl --digest -u opencast_system_account:CHANGE_ME -H \"X-Requested-Auth: Digest\" \\ https://develop.opencast.org/info/me.json HTTP Basic authentication can be used with users defined via web-interface or via any regular user provider. A request using HTTP Basic does not need to specify any additional headers: curl -u admin:opencast https://develop.opencast.org/info/me.json Generally, we recommend using HTTP Basic authentication since it's easier for adopters to manage capture agent users via the admin interface and does not rely on hidden users while being less complicated at the same time. Whatever authentication method you choose to implement \u2013 maybe even both, allowing users to choose for themselves \u2013 please clearly specify what authentication you expect since users have to provide different types of users which can easily lead to usability problems if not clearly marked.","title":"Authentication"},{"location":"modules/capture-agent/capture-agent/#action-details","text":"This is a detailed description of the steps described in the quick overview section. For details about the REST endpoints, please always consult the Opencast REST documentation which can be found in the top-right corner of the administrative user interface of each running Opencast instance. Note that most endpoints can handle both JSON and XML although throughout this guide, for all examples, only JSON is used.","title":"Action Details"},{"location":"modules/capture-agent/capture-agent/#get-endpoint-locations-from-service-registry","text":"First, you need to get the locations of the REST endpoints to talk to. These information can be retrieved from the central service registry. It is likely that Opencast is running as a distributed system which means you cannot expect all endpoints on a single host. Three endpoint locations need to be requested: The capture-admin endpoint to register the agent and set status and configuration: org.opencastproject.capture.admin The scheduler endpoint to get the current schedule for the agent from: org.opencastproject.scheduler The ingest endpoint to upload the recordings to once they are done: org.opencastproject.ingest To ask for an endpoint you would send an HTTP GET request to ${HOST}/services/available.json?serviceType=<SERVICE> A result would look like { \"services\" : { \"service\" : { \"active\" : true, \"host\" : \"http://example.opencast.org:8080\", \"path\" : \"/capture-admin\", ... } } } These endpoints should be requested once when starting up the capture agent. While the capture-admin and scheduler endpoints may then be assumed to never change during the runtime of the capture agent, the ingest endpoint may change and the data should be re-requested every time before uploading (ingesting) data to the core.","title":"Get Endpoint Locations From Service Registry"},{"location":"modules/capture-agent/capture-agent/#register-the-capture-agent-and-set-current-status","text":"Once the endpoints to talk to are known, it is time to register the capture agent at the core so that scheduled events can be added. This can be done by sending an HTTP POST request to: ${CAPTURE-ADMIN-ENDPOINT}/agents/<name> \u2026including the following data fields: state=idle address=http(s)://<ca-web-ui> The name has to be a unique identifier of a single capture agent. Using the same name for several agents would mean sharing the same schedule and status which in general should be avoided. Sending this request will register the capture agent. After this, the capture agent should appear in the admin interface and schedules can be added for this agent. Setting address is optional. It can be used to link an administrative web interface of a capture agent.","title":"Register the Capture Agent and set Current Status"},{"location":"modules/capture-agent/capture-agent/#setting-agent-capabilities","text":"Additional to registering, the agent may set its capabilities allowing users to select possible inputs in the user interface of Opencast when scheduling events. The configuration may be set as XML or JSON representation of a Java properties file and can be set via an HTTP POST request to: /capture-admin/agents/$AGENT_NAME/configuration","title":"Setting Agent Capabilities"},{"location":"modules/capture-agent/capture-agent/#getting-the-calendarschedule","text":"The calendar can be retrieved by sending an HTTP GET request to ${SCHEDULER-ENDPOINT}/calendars?agentid=<name> The format returned is iCal. The file contains all scheduled upcoming recordings the capture agent should handle. The output will look like this (base64 encoded parts are shortened): BEGIN:VCALENDAR PRODID:Opencast Calendar File 0.5 VERSION:2.0 CALSCALE:GREGORIAN BEGIN:VEVENT DTSTAMP:20200616T124513Z DTSTART:20200616T124200Z DTEND:20200616T124400Z SUMMARY:Demo event UID:68b19d11-6aca-413e-b2b3-eda72eebc65a LAST-MODIFIED:20200616T124119Z DESCRIPTION:demo LOCATION:my_capture_agent_name ATTACH;FMTTYPE=application/xml;VALUE=BINARY;ENCODING=BASE64;X-APPLE-FILENAME=episode.xml:... ATTACH;FMTTYPE=application/text;VALUE=BINARY;ENCODING=BASE64;X-APPLE-FILENAME=org.opencastproject.capture.agent.properties:... END:VEVENT END:VCALENDAR The iCal event contains start and end dates, all meta data catalogs for the event, the UID which is the %RECORDING_ID and which important when uploading media later on and additional capture agent properties which should be passed on. Note that most meta data like the dublin core catalogs can in most cases be ignored. In most cases, this means the data you are interested in per event are: DTSTART : Date to starte the recording at DTEND : Date at which to stop the recording UID : Recording identifier used for updating the recording status and associating uploads with a scheduled event LOCATION : This should always match your agent's name ATTACH;...FILENAME=org.opencastproject.capture.agent.properties : Agent properties to pass on in case of workflow properties Depending on the amount of recordings scheduled for the particular capture agent, this file may become very large. That is why there are two way of limiting the amount of necessary data to transfer and process: Sending the optional parameter cutoff to limit the schedule to a particular time span in the future. ${SCHEDULER-ENDPOINT}/calendars?agentid= &cutoff= The value for cutoff is a Unix timestamp in milliseconds from now. Events beginning after this time will not be included in the returned schedule. Use the HTTP ETag and If-Not-Modified header to have Opencast only sent schedules when they have actually changed.","title":"Getting the Calendar/Schedule"},{"location":"modules/capture-agent/capture-agent/#set-agent-and-recording-state","text":"Setting the agent state is identical to the registration of the capture agent and done by sending an HTTP POST request to: ${CAPTURE-ADMIN-ENDPOINT}/agents/<name> \u2026including the following data fields: state=capturing address=http(s)://<ca-web-ui> Additionally, set the recording state using the event's UID obtained from the schedule with an HTTP POST request to ${CAPTURE-ADMIN-ENDPOINT}/recordings/<recording_id> \u2026including the data field: state=capturing","title":"Set Agent and Recording State"},{"location":"modules/capture-agent/capture-agent/#recording","text":"This task is device specific. Use whatever means necessary to get the recording done.","title":"Recording"},{"location":"modules/capture-agent/capture-agent/#set-agent-and-recording-state_1","text":"This step is identical to the previous status update but for the state. If the recording has failed, the recording state is updated with capture_error while the agent's state is set back to idle if the error is non-permanent and to error if it is permanent and block further recordings. If the recording was successful, both states are set to uploading .","title":"Set Agent and Recording State"},{"location":"modules/capture-agent/capture-agent/#get-ingest-endpoint-locations-from-service-registry","text":"This step is identical to first request to the service registry expect that it is sufficient to request the location for the service org.opencastproject.ingest . If this request fails, assume the old data to be valid.","title":"Get Ingest Endpoint Locations From Service Registry"},{"location":"modules/capture-agent/capture-agent/#ingest-upload-recording","text":"Use the ingest endpoint to upload the recording. There are multiple different methods to ingest media. Please refer to the REST endpoint documentation for details of these methods. The most commonly used are: Single request ingest using an HTTP POST request to ${INGEST-ENDPOINT}/addMediaPackage Multi request ingest using HTTP POST requests to ${INGEST-ENDPOINT}/createMediaPackage ${INGEST-ENDPOINT}/addDCCatalog ${INGEST-ENDPOINT}/addTrack ${INGEST-ENDPOINT}/ingest Please make sure that the event's UID is passed on as workfloeInstanceId to the final call to /ingest/ingest to match the scheduled event to the media being uploaded. If possible, please follow these additional rules about recording files: Recordings may be deleted if the ingest was successful. Recordings should be stored in case of a failure.","title":"Ingest (Upload) Recording"},{"location":"modules/capture-agent/capture-agent/#upload-metadata","text":"The calendar (iCal) with the scheduled events retrieved in an earlier step also contains metadata catalogs as attached files. To modify metadata, these catalogs can be modified and ingested as well. Opencast's default setting is to use these for updating the existing metadata in the system. If no metadata modifications are required (usual case), please do not modify these files and do not upload them. In short: Ignore these attachments Additional note for Opencast \u2264 3.x: Opencast only creates events in the database after ingesting the files. Scheduled data are kept separately. That is why for these Opencast versions, all metadata files need to be ingested. Usually, that means to take the metadata catalogs from the schedule and ingest them unmodified using for example the /addDCCatalog endpoint.","title":"Upload Metadata"},{"location":"modules/capture-agent/capture-agent/#set-agent-and-recording-state_2","text":"Again, this step is identical to the previous status updates except for the state. If the upload has failed, the recording state is updated with upload_error while the agents state is set back to idle if the error is non-permanent or to error otherwise. If the upload was successful, the recording status is set to upload_finished while the agents state is set back to idle .","title":"Set Agent and Recording State"},{"location":"modules/capture-agent/capture-agent/#agent-state-and-configuration","text":"This section describes some additional aspects of the communication between capture agent and the Opencast core.","title":"Agent State And Configuration"},{"location":"modules/capture-agent/capture-agent/#creating-an-agent-on-the-core","text":"An agent record is created on the core the first time the agent communicates with the core. There is no special endpoint or registration required, just send the state and the agent record will be created.","title":"Creating An Agent On The Core"},{"location":"modules/capture-agent/capture-agent/#agent-state","text":"Additional to the required status updates outlined above, the agent should continue to send this status information on a regular basis to allow Opencast to determine that the agent is still active. If the agent fails to do so, it may be marked as offline in the Opencast user interface after a certain amount of time (The default is 120min).","title":"Agent State"},{"location":"modules/capture-agent/capture-agent/#agent-configuration","text":"If a special configuration is required, the agent should send its configuration data in a regular interval to ensure Opencast has the updated configuration even if the core is reset in the meantime. It should also send the configuration when the agent's configuration changes to avoid conflicts between selected and available options. The format of this XML structure is the following: <?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?> <!DOCTYPE properties SYSTEM \"http://java.sun.com/dtd/properties.dtd\\\"> <properties> <comment>Capabilities for $AGENT_NAME</comment> <entry key=\"$KEY_NAME\">$VALUE</entry> \u2026 </properties> If sent as JSON, the format is a simple JSON object: { 'key':'value', 'key2':'value2' } To specify inputs the user can select, the special key capture.device.names is used. It is a comma separated list of inputs which will be presented in the Opencast user interface.","title":"Agent Configuration"},{"location":"modules/capture-agent/capture-agent/#recording-state","text":"If the agent is processing (recording) a previously scheduled event, it must send the recording's state to the core. It may do this on a regular basis but at least should do this once the state of the recording changes since, for example, the recording process has started. Note that these status changes are used in the administrative user interface and failing to set a state may cause the interface to display a warning such as \u201cThe agent may have failed to start this recording\u201d. To send the recording's state to the core, a valid state (as defined here) is sent via HTTP POST to: ${CAPTURE-ADMIN-ENDPOINT}/recordings/<recording_id>","title":"Recording State"},{"location":"modules/capture-agent/capture-agent/#calendaring-data","text":"Agent's are expected to understand Opencast's iCalendar implementation. They should poll the calendar endpoint in a regular interval to update their internal schedule. Agent's should use a permanent cache (e.g. disk or database) for the cached schedule to be able to handle power and/or network failures gracefully. This also allows an agent to be used in a network-less environment, for example for mobile recordings: Merely cache the calendar data once after which the agent is brought to its destination where it will capture and cache the pre-scheduled recordings. To retrieve the calendar for an agent an HTTP GET is performed to ${SCHEDULER-ENDPOINT}/calendars?agentid=<name> Note that the schedule has ETag support, which is very useful to speed up the processing of larger calendars.","title":"Calendaring Data"},{"location":"modules/capture-agent/capture-agent/#capture-agent-configuration-file","text":"TODO: Verify that this is still necessary with \u2265 4.0 One file attached to each scheduled event is org.opencastproject.capture.agent.properties . This file contains the capture agent configuration directives (e.g. turning inputs on and off) as well as workflow directives which are important for the ingest process without which the core may misbehave. All keys in this file contain prefixes identifying the type of property. For example, workflow directives are prefixed by org.opencastproject.workflow.config . When passing the configuration directive to the core using the file ingest REST endpoints, the agent must usually strip this prefix from the parameter. For example, org.opencastproject.workflow.config.trimHold=true should be passed as trimHold=true . The org.opencastproject.workflow.definition directive is important as well, this is the workflow definition identifier and should be passed as a parameter during the ingest operation. Example configuration file: #Capture Agent specific data #Tue May 22 17:34:22 CST 2012 org.opencastproject.workflow.config.trimHold=true capture.device.names=MOCK_SCREEN,MOCK_PRESENTER,MOCK_MICROPHONE org.opencastproject.workflow.definition=full event.title=Test Capture event.location=demo_capture_agent","title":"Capture Agent Configuration File"},{"location":"modules/capture-agent/capture-agent/#ingesting-media","text":"Opencast provides several different methods to ingest media, with each having some advantages and disadvantages. The following description will give a short overview of the different methods. For more details, again, have a look at the REST endpoint documentation of the ingest service.","title":"Ingesting Media"},{"location":"modules/capture-agent/capture-agent/#multi-request-ingest","text":"This is the recommended way to use for most capture agents. It offers the most features to use and does not require any pre- and post-processing of ingested material. Using this method, a number of successive HTTP calls are made during the ingestion of media. The result of a successful call is the newly updated media package. This media package is created by one call, then amended by a number of other calls, each adding additional elements like tracks, attachments or metadata catalohs. Finally, it is then passed to the last endpoint to begin processing. The advantage to this process is that in case of a network failure only one particular element needs to be repeated in contrast to repeating the whole process required by all other ingest methods. To begin, the agent must first generate a valid media package. This is done via an HTTP GET request to ${INGEST-ENDPOINT}/createMediaPackage . The resulting media package will contain the base skeleton used in later calls. Each following call will require a media package as input and will modify and return it to be used for the next call. The next step(s) vary. Essentially, each generated file for a recording must be added, one at a time, to the media package. For this, an agent may use the following REST endpoints: ${INGEST-ENDPOINT}/addTrack to add media files (video, audio, \u2026) used for processing and/or publication ${INGEST-ENDPOINT}/addDCCatalog to add the dublin core metadata catalogs like the basic episode metadata (title, \u2026) ${INGEST-ENDPOINT}/addCatalog to add all types of metadata catalogs. This is a more general version of addDCCatalog and is seldom necessary. ${INGEST-ENDPOINT}/addAttachment to add arbitrary attachments (cover images, access control catalogs, \u2026) to the media package. Finally, once you have added all files, it is time to ingest the media package and begin processing. After this, no further files can be added. To ingest a recording, an HTTP POST is sent to ${INGEST-SERVICE}/ingest .","title":"Multi Request Ingest"},{"location":"modules/capture-agent/capture-agent/#single-request-ingest","text":"The single request ingest will, as its name implies, handle the whole process as part of a single HTTP request. This is a convenient way of adding smaller ingest since the implementation does not require to store any internal state. The operation is atomic after all: Either it succeeds or fails. The disadvantage to this is that the complexity of ingests is limited, e.g. no attachments can be added to the media package this way, and a failure means that all files need to be re-transferred. For this method, the agent posts all data to ${INGEST-ENDPOINT}/addMediaPackage .","title":"Single Request Ingest"},{"location":"modules/capture-agent/capture-agent/#zipped-media-ingest","text":"In general, the use of this method is discouraged because of the additional load for packing and unpacking the material compared to the negligible gain. For this method, the captured media, along with some metadata files is zipped and then HTTP POSTed to the core. The core then unzips the media package and begins processing. This unzipping operation is quite disk intensive, and the REST endpoint does not return until the unzipping is done. Thus, please beware of proxy timeouts and additional disk utilization. To ingest a zipped media package an HTTP POST is performed to ${INGEST-ENDPOINT}/addZippedMediaPackage . The BODY of the POST must contain the zipped media package.","title":"Zipped Media Ingest"},{"location":"modules/capture-agent/capture-agent/#further-reading","text":"The communication involve several REST endpoints. Additional documentation about these can be found in the REST docs of the specific service. The REST documentation can be found at /rest_docs.html in every Opencast instance to reflect that servers unique capabilities. Services involved in the communication with the capture agent are: The capture admin service used to register the capture agent and set its current status. The scheduler service to get scheduled recordings for an agent. The ingest service to upload recording files and start processing.","title":"Further Reading"},{"location":"modules/capture-agent/testcases/","text":"Test Cases for Capture Agents This document describes a set of simple test cases which can be used to test new capture agents. As a vendor, you might want to use these to test new devices before their release. They are meant for a setup where the capture agent is firmly installed in a venue, it runs 24/7 and automatically captures scheduled lectures. Test Expected Result Connection to Opencast Server Agent shows up at Opencast's admin interface with status online Manual recording on agent Event successfully recorded and uploaded Schedule recording Event successfully recorded and uploaded Schedule multiple recordings Events successfully recorded and uploaded Record 6 hours Event successfully recorded and uploaded Network loss before recording starts Recording starts and stops without network connection Network loss while recording Event successfully recorded and uploaded Power loss The capture agent starts up again Change input signal during recording Event successfully recorded and uploaded HTTPS Devices supports HTTPS connections to Opencast Test Servers You can use Opencast's official test servers like stable.opencast.org , but keep in mind that these servers are reset on a daily basis at about 2am (Europe/Berlin timezone). The default credentials for these servers are: User interface user: admin password: opencast Capture agent user: opencast_system_account password: CHANGE_ME","title":"Test Cases"},{"location":"modules/capture-agent/testcases/#test-cases-for-capture-agents","text":"This document describes a set of simple test cases which can be used to test new capture agents. As a vendor, you might want to use these to test new devices before their release. They are meant for a setup where the capture agent is firmly installed in a venue, it runs 24/7 and automatically captures scheduled lectures. Test Expected Result Connection to Opencast Server Agent shows up at Opencast's admin interface with status online Manual recording on agent Event successfully recorded and uploaded Schedule recording Event successfully recorded and uploaded Schedule multiple recordings Events successfully recorded and uploaded Record 6 hours Event successfully recorded and uploaded Network loss before recording starts Recording starts and stops without network connection Network loss while recording Event successfully recorded and uploaded Power loss The capture agent starts up again Change input signal during recording Event successfully recorded and uploaded HTTPS Devices supports HTTPS connections to Opencast","title":"Test Cases for Capture Agents"},{"location":"modules/capture-agent/testcases/#test-servers","text":"You can use Opencast's official test servers like stable.opencast.org , but keep in mind that these servers are reset on a daily basis at about 2am (Europe/Berlin timezone). The default credentials for these servers are: User interface user: admin password: opencast Capture agent user: opencast_system_account password: CHANGE_ME","title":"Test Servers"},{"location":"modules/player/architecture/","text":"Architecture Overview The architecture of the theodul player has a plugin based structure based around a core. The core and the plugins have been realized as OSGi modules. Each plugin can be separately build. The following figure shows the OSGi architecture of the player. All Theodul OSGi modules are stored under: modules/engage-theodul-* #Core module modules/engage-theodul-api/ modules/engage-theodul-core/ #A plugin module modules/engage-theodul-plugin-* modules/engage-theodul-plugin-tab-description/ Plugin Manager The main workflow is implemented by the core, which recognizes new plugins, collects information about the plugin type and resources, runs the JavaScript logic and inserts the first compiled templates into the HTML DOM. The Plugin Manager Endpoint recognizes the OSGi modules. Each plugin has some information about its name and its resources. The Plugin Manager collects these information and publishes them via a REST endpoint. The following URL links to an example REST endpoint: http://localhost:8080/engage/theodul/manager/list.json The documentation and test forms of the endpoint can be found on the Opencast start page. The following data in JSON shows an example list of plugins, which are used by the player and provided by the Plugin Manager Endpoint. { \"pluginlist\":{ \"plugins\":[ { \"name\":\"EngagePluginTabSlidetext\", \"id\":\"6\", \"description\":\"Simple implementation of a tab with the text of the slides\", \"static-path\":\"6\\/static\" }, { \"name\":\"EngagePluginControlsMockup\", \"id\":\"5\", \"description\":\"Simple implementation of a control bar\", \"static-path\":\"5\\/static\" }] } } Next to the Plugin Manager there is the Theodul Core module, which publishes the main HTML page, core.html. UI Core The core.html is the main entry point and starts the Javascript core logic. Following listing shows the directory structure of core in the engage-theodul-core OSGi module. |-src |---main |-----java #Java impl of the plugin manager |-----resources |-------ui #UI of the core, core.html and engage_init.js |---------css #Global CSS Styles |---------js #JavaScript logic |-----------engage #Core logic, engage_core.js and engage_model.js |-----------lib #External libraries, backbone.js, jquery.js, require.js and underscore.js |---test #Unit Tests |-----resources |-------ui #JavaScript Unit Tests |---------js |-----------spec All Theodul JavaScript components are defined as a RequireJS module. The file engage_init.js is loaded firstly and contains the configuration of RequireJS. This init script additionally loads the core module, which is defined in the engage_core.js . The core module initializes the main HTML view. This view is realized as a BackboneJS view and is linked to a global Backbone model, which is stored in the model module in engage_model.js . The view is returned by the core module, so every other module, which has a dependency to the core module, has a reference to the view (simply called Engage in the plugins) and its functions. See the Core Reference for more information about the functions of the core view. Plugins Plugins in the Theodul player are developed and distributed in own OSGi modules. Every plugin has a special UI type. In dependency of this type the core injects the plugin to the right position of the player. The following plugin types are possible: Plugin Type Description Characteristics Module Name JS Plugin Type Name Maven Plugin Type Name Controls Implements the main controls of the top of the player Only one plugin per player possible. engage-theodul-plugin-controls engage_controls controls Timeline Timeline information below the main controls. Good for processing time-based data like user tracking, slide previews or annotations. Optional plugin, more than one possible. engage-theodul-plugin-timeline- engage_timeline timeline Videodisplay Implementation of the video display. Currently only one plugin per player possible, but in the future more video displays should be possible. engage-theodul-plugin-video- engage_video video Description/Label A plugin below the video display, good to show simple information about the video, like a title and the creator. Only one plugin per player possible. engage-theodul-plugin-description engage_description description Tab Shows a tab in the tab view at the bottom of the player. Optional plugin, more than one possible. engage-theodul-plugin-tab- engage_tab tab Custom A custom plugin without a relationship to an UI element. Good for a custom REST endpoint, global data representation or to load custom JS code or libraries. Optional plugin, more than one possible. No connection to a preserved UI element. engage-theodul-plugin-custom- The following listing shows the directory structure of a plugin module: |-src |---main |-----java |-------org |---------opencastproject |-----------engage |-------------theodul |---------------plugin |-----------------controls #Simple Java class, and optional REST endpoint |-----resources |-------OSGI-INF #OSGi information about the plugin |-------static #web ressources, contains the main.js entry point of the plugin |---------images #plugin ressources |---------js #plugin js libs |-----------bootstrap |-----------jqueryui |---test #Jasmine test ressources |-----resources |-------js |---------engage #Test Wrapper of the core |---------lib #Required test libs |---------spec #Jasmine test specs The main JavaScript entry point of the plugin is main.js in the static folder. This contains the RequireJS module definition of the plugin and the main logic. All other plugin logic can be implemented as a RequireJS module and loaded in the main module. The main module should have a dependency to the core, the Engage object. With this object you have access to main features of the core. See the Core Reference for more information about that. After the initialization process of the plugin, the plugin returns a plugin object with information about the plugin, like the type, the name, the ui template etc. This object is used by the core to decide about the UI type/location of plugin. The Core Reference describes the plugin object, before and after it is being processed by the core. Have a look to the code of a plugin to get an impression about the plugin implementation. Model View Controller Support The Theodul player supports MVC design patterns for each plugin based on methods and objects of the BackboneJS library. It is not necessary to design a plugin in MVC style but it is highly recommended. An overview of the methods and objects of the BackboneJS library is listed on the official website of BackboneJS. Each plugin with a visual component has a reference to its view container and its template to fill the view container. Have a look at the Core Reference how to access the container and the template data. With this information the plugin can create a Backbone view with a reference to the to div container and a render function to compile the template. The next step is the creation of a model, which is being bound to the view. An usual way is to create a Backbone model, which is being passed by the view. In the initialization function of the view, the view binds the model change event to his render function: Bind the \"change\" event always to the render function of a view // bind the render function always to the view _.bindAll(this, \"render\"); // listen for changes of the model and bind the render function to this this.model.bind(\"change\", this.render); The model can only be visible by the plugin itself or it can be added to the global Engage model of the core. Adding the model to the Engage model has the advantage, that on the one hand data can be used by other plugins and on the other hand it is able to listen to change- or add-events. So other plugins are able to listen to a change of data in another model and can react to it by e.g. re-render its view. This feature is e.g. used by the \"mhConnection\" custom plugin. The plugin receives data of Opencast endpoints and saves them to a model, which is being added to the Engage Model. Each time the plugin gets newer endpoint data and updates its model's data, each plugin gets a notification and can re-render its view. A typical way to add a model to the Engage model is to add the model in the initialization function of the plugin after all other initializations. Here is an example of the video plugin: Add a custom model to the Engage Model Engage.model.set(\"videoDataModel\", new VideoDataModel(videoDisplays, videoSources, duration)); In the same initialization function an event handler should be added to notice the addition of the model. Has the model successfully been added, a view with this model and other data can be created: Model Event Handler Engage.model.on(\"change:videoDataModel\", function() { new VideoDataView(this.get(\"videoDataModel\"), plugin.template, videojs_swf); }); If another plugin wants to use the defined \"videoDataModel\" model, it has to list it in its own initialization process: Engage.model.on(\"change:videoDataModel\", function() { initCount -= 1; if (initCount === 0) { initPlugin(); } }); Have a look at the full implementation of the VideoJS Plugin and the Controls Plugin to get an idea how the Backbone MVC design works. For completeness' sake, the \"Controller\" does not have an extra Object in the Backbone MVC design. The \"Controller\" is usually used as the render function in the view. This function can be very complex and should link to other functions, which are short and easy to be tested by the Jasmine Test Framework.","title":"Architecture"},{"location":"modules/player/architecture/#architecture","text":"","title":"Architecture"},{"location":"modules/player/architecture/#overview","text":"The architecture of the theodul player has a plugin based structure based around a core. The core and the plugins have been realized as OSGi modules. Each plugin can be separately build. The following figure shows the OSGi architecture of the player. All Theodul OSGi modules are stored under: modules/engage-theodul-* #Core module modules/engage-theodul-api/ modules/engage-theodul-core/ #A plugin module modules/engage-theodul-plugin-* modules/engage-theodul-plugin-tab-description/","title":"Overview"},{"location":"modules/player/architecture/#plugin-manager","text":"The main workflow is implemented by the core, which recognizes new plugins, collects information about the plugin type and resources, runs the JavaScript logic and inserts the first compiled templates into the HTML DOM. The Plugin Manager Endpoint recognizes the OSGi modules. Each plugin has some information about its name and its resources. The Plugin Manager collects these information and publishes them via a REST endpoint. The following URL links to an example REST endpoint: http://localhost:8080/engage/theodul/manager/list.json The documentation and test forms of the endpoint can be found on the Opencast start page. The following data in JSON shows an example list of plugins, which are used by the player and provided by the Plugin Manager Endpoint. { \"pluginlist\":{ \"plugins\":[ { \"name\":\"EngagePluginTabSlidetext\", \"id\":\"6\", \"description\":\"Simple implementation of a tab with the text of the slides\", \"static-path\":\"6\\/static\" }, { \"name\":\"EngagePluginControlsMockup\", \"id\":\"5\", \"description\":\"Simple implementation of a control bar\", \"static-path\":\"5\\/static\" }] } } Next to the Plugin Manager there is the Theodul Core module, which publishes the main HTML page, core.html.","title":"Plugin Manager"},{"location":"modules/player/architecture/#ui-core","text":"The core.html is the main entry point and starts the Javascript core logic. Following listing shows the directory structure of core in the engage-theodul-core OSGi module. |-src |---main |-----java #Java impl of the plugin manager |-----resources |-------ui #UI of the core, core.html and engage_init.js |---------css #Global CSS Styles |---------js #JavaScript logic |-----------engage #Core logic, engage_core.js and engage_model.js |-----------lib #External libraries, backbone.js, jquery.js, require.js and underscore.js |---test #Unit Tests |-----resources |-------ui #JavaScript Unit Tests |---------js |-----------spec All Theodul JavaScript components are defined as a RequireJS module. The file engage_init.js is loaded firstly and contains the configuration of RequireJS. This init script additionally loads the core module, which is defined in the engage_core.js . The core module initializes the main HTML view. This view is realized as a BackboneJS view and is linked to a global Backbone model, which is stored in the model module in engage_model.js . The view is returned by the core module, so every other module, which has a dependency to the core module, has a reference to the view (simply called Engage in the plugins) and its functions. See the Core Reference for more information about the functions of the core view.","title":"UI Core"},{"location":"modules/player/architecture/#plugins","text":"Plugins in the Theodul player are developed and distributed in own OSGi modules. Every plugin has a special UI type. In dependency of this type the core injects the plugin to the right position of the player. The following plugin types are possible: Plugin Type Description Characteristics Module Name JS Plugin Type Name Maven Plugin Type Name Controls Implements the main controls of the top of the player Only one plugin per player possible. engage-theodul-plugin-controls engage_controls controls Timeline Timeline information below the main controls. Good for processing time-based data like user tracking, slide previews or annotations. Optional plugin, more than one possible. engage-theodul-plugin-timeline- engage_timeline timeline Videodisplay Implementation of the video display. Currently only one plugin per player possible, but in the future more video displays should be possible. engage-theodul-plugin-video- engage_video video Description/Label A plugin below the video display, good to show simple information about the video, like a title and the creator. Only one plugin per player possible. engage-theodul-plugin-description engage_description description Tab Shows a tab in the tab view at the bottom of the player. Optional plugin, more than one possible. engage-theodul-plugin-tab- engage_tab tab Custom A custom plugin without a relationship to an UI element. Good for a custom REST endpoint, global data representation or to load custom JS code or libraries. Optional plugin, more than one possible. No connection to a preserved UI element. engage-theodul-plugin-custom- The following listing shows the directory structure of a plugin module: |-src |---main |-----java |-------org |---------opencastproject |-----------engage |-------------theodul |---------------plugin |-----------------controls #Simple Java class, and optional REST endpoint |-----resources |-------OSGI-INF #OSGi information about the plugin |-------static #web ressources, contains the main.js entry point of the plugin |---------images #plugin ressources |---------js #plugin js libs |-----------bootstrap |-----------jqueryui |---test #Jasmine test ressources |-----resources |-------js |---------engage #Test Wrapper of the core |---------lib #Required test libs |---------spec #Jasmine test specs The main JavaScript entry point of the plugin is main.js in the static folder. This contains the RequireJS module definition of the plugin and the main logic. All other plugin logic can be implemented as a RequireJS module and loaded in the main module. The main module should have a dependency to the core, the Engage object. With this object you have access to main features of the core. See the Core Reference for more information about that. After the initialization process of the plugin, the plugin returns a plugin object with information about the plugin, like the type, the name, the ui template etc. This object is used by the core to decide about the UI type/location of plugin. The Core Reference describes the plugin object, before and after it is being processed by the core. Have a look to the code of a plugin to get an impression about the plugin implementation.","title":"Plugins"},{"location":"modules/player/architecture/#model-view-controller-support","text":"The Theodul player supports MVC design patterns for each plugin based on methods and objects of the BackboneJS library. It is not necessary to design a plugin in MVC style but it is highly recommended. An overview of the methods and objects of the BackboneJS library is listed on the official website of BackboneJS. Each plugin with a visual component has a reference to its view container and its template to fill the view container. Have a look at the Core Reference how to access the container and the template data. With this information the plugin can create a Backbone view with a reference to the to div container and a render function to compile the template. The next step is the creation of a model, which is being bound to the view. An usual way is to create a Backbone model, which is being passed by the view. In the initialization function of the view, the view binds the model change event to his render function: Bind the \"change\" event always to the render function of a view // bind the render function always to the view _.bindAll(this, \"render\"); // listen for changes of the model and bind the render function to this this.model.bind(\"change\", this.render); The model can only be visible by the plugin itself or it can be added to the global Engage model of the core. Adding the model to the Engage model has the advantage, that on the one hand data can be used by other plugins and on the other hand it is able to listen to change- or add-events. So other plugins are able to listen to a change of data in another model and can react to it by e.g. re-render its view. This feature is e.g. used by the \"mhConnection\" custom plugin. The plugin receives data of Opencast endpoints and saves them to a model, which is being added to the Engage Model. Each time the plugin gets newer endpoint data and updates its model's data, each plugin gets a notification and can re-render its view. A typical way to add a model to the Engage model is to add the model in the initialization function of the plugin after all other initializations. Here is an example of the video plugin:","title":"Model View Controller Support"},{"location":"modules/player/architecture/#add-a-custom-model-to-the-engage-model","text":"Engage.model.set(\"videoDataModel\", new VideoDataModel(videoDisplays, videoSources, duration)); In the same initialization function an event handler should be added to notice the addition of the model. Has the model successfully been added, a view with this model and other data can be created: Model Event Handler Engage.model.on(\"change:videoDataModel\", function() { new VideoDataView(this.get(\"videoDataModel\"), plugin.template, videojs_swf); }); If another plugin wants to use the defined \"videoDataModel\" model, it has to list it in its own initialization process: Engage.model.on(\"change:videoDataModel\", function() { initCount -= 1; if (initCount === 0) { initPlugin(); } }); Have a look at the full implementation of the VideoJS Plugin and the Controls Plugin to get an idea how the Backbone MVC design works. For completeness' sake, the \"Controller\" does not have an extra Object in the Backbone MVC design. The \"Controller\" is usually used as the render function in the view. This function can be very complex and should link to other functions, which are short and easy to be tested by the Jasmine Test Framework.","title":"Add a custom model to the Engage Model"},{"location":"modules/player/core.reference/","text":"Core Reference Engage Core RequireJS Path 'engage/engage_core' Inherited object functions of the BackboneJS view, see http://backbonejs.org/#View Added object functions and properties: Name Parameters Type Description log(value):void value:String function function to log via the core cross browser Event:EngageEvent none property Returns the EngageEvent object prototype, the see EngageEvent Object for more information trigger(event):void event:EngageEvent function triggers an EngageEvent on(event, handler, context):void event:EngageEvent, handler:function, context:object function install an event handler on a EngageEvent model:EngageModel none property Returns the singleton engage model for this session, see EngageModel for more information's getPluginPath(pluginName):String pluginName:String function Returns the absolute path of a plugin by name. EngageEvent Object Name Paramters Type Description EngageEvent(name, description, type) name:String, description:String, type:String constructor Create a new unbound EngageEvent, with a name, description and a type. For Example: var myEvent = new EngageEvent('play', 'plays the video', 'trigger') getName:String none function Gets the name getDescription:String none function Gets the description getType:String none function Gets the Type, can be a \"handler\", \"trigger\" or \"both\" toString:String none function Build a string that describes the event Engage Model Inherited object functions of the BackboneJS model, see http://backbonejs.org/#Model, how to use BackboneJS models. This model is a global singleton object and can be used by each plugin to add new models which can be used by another plugin again. No special functions are added, but the model is filled with some default data. This default data can be used by each plugin, which has a reference to the EngageModel. Property Name Type Description pluginsInfo Backbone Model Contains Information's of each plugin pluginModels Backbone Collection Contains the plugin models urlParameters Object Contains the data of the URL parameters. Plugin Object Each plugin must create and return a object with some properties which are set by the plugin itself. It is recommend to keep a reference to the object because some properties are set by the core after the plugin is processed. Property Name Type Description name String Name of the plugin, e.g. \"Engage Controls\". This property is set by the plugin. type String Type of the plugin, e.g. \"engage_controls\", see the plugin table in Architecture for the other plugin types. This property is set by the plugin. version String Version of plugin. This property is set by the plugin. styles Array of Strings Array of the paths of css files relative to the static folder of each plugin . This property is set by the plugin. template String Before the plugin object is returned by the plugin logic, the template property contains the path to the template relative to the static folder. The path property is set first by the plugin . After the plugin object is returned and the Theodul core processed the plugin, the template property is filled with the real template data and can be used to re-render the view. container String Contains the ID of the HTML div container, which contains the rendered template. This can be used to re-render the view. This property is set by the core. pluginPath String Contains the absolute path of the plugin. This property is set by the core. events Object Contains all events which are used of this plugin. Each handled and each triggered event.","title":"Core Reference"},{"location":"modules/player/core.reference/#core-reference","text":"","title":"Core Reference"},{"location":"modules/player/core.reference/#engage-core","text":"RequireJS Path 'engage/engage_core' Inherited object functions of the BackboneJS view, see http://backbonejs.org/#View Added object functions and properties: Name Parameters Type Description log(value):void value:String function function to log via the core cross browser Event:EngageEvent none property Returns the EngageEvent object prototype, the see EngageEvent Object for more information trigger(event):void event:EngageEvent function triggers an EngageEvent on(event, handler, context):void event:EngageEvent, handler:function, context:object function install an event handler on a EngageEvent model:EngageModel none property Returns the singleton engage model for this session, see EngageModel for more information's getPluginPath(pluginName):String pluginName:String function Returns the absolute path of a plugin by name.","title":"Engage Core"},{"location":"modules/player/core.reference/#engageevent-object","text":"Name Paramters Type Description EngageEvent(name, description, type) name:String, description:String, type:String constructor Create a new unbound EngageEvent, with a name, description and a type. For Example: var myEvent = new EngageEvent('play', 'plays the video', 'trigger') getName:String none function Gets the name getDescription:String none function Gets the description getType:String none function Gets the Type, can be a \"handler\", \"trigger\" or \"both\" toString:String none function Build a string that describes the event","title":"EngageEvent Object"},{"location":"modules/player/core.reference/#engage-model","text":"Inherited object functions of the BackboneJS model, see http://backbonejs.org/#Model, how to use BackboneJS models. This model is a global singleton object and can be used by each plugin to add new models which can be used by another plugin again. No special functions are added, but the model is filled with some default data. This default data can be used by each plugin, which has a reference to the EngageModel. Property Name Type Description pluginsInfo Backbone Model Contains Information's of each plugin pluginModels Backbone Collection Contains the plugin models urlParameters Object Contains the data of the URL parameters.","title":"Engage Model"},{"location":"modules/player/core.reference/#plugin-object","text":"Each plugin must create and return a object with some properties which are set by the plugin itself. It is recommend to keep a reference to the object because some properties are set by the core after the plugin is processed. Property Name Type Description name String Name of the plugin, e.g. \"Engage Controls\". This property is set by the plugin. type String Type of the plugin, e.g. \"engage_controls\", see the plugin table in Architecture for the other plugin types. This property is set by the plugin. version String Version of plugin. This property is set by the plugin. styles Array of Strings Array of the paths of css files relative to the static folder of each plugin . This property is set by the plugin. template String Before the plugin object is returned by the plugin logic, the template property contains the path to the template relative to the static folder. The path property is set first by the plugin . After the plugin object is returned and the Theodul core processed the plugin, the template property is filled with the real template data and can be used to re-render the view. container String Contains the ID of the HTML div container, which contains the rendered template. This can be used to re-render the view. This property is set by the core. pluginPath String Contains the absolute path of the plugin. This property is set by the core. events Object Contains all events which are used of this plugin. Each handled and each triggered event.","title":"Plugin Object"},{"location":"modules/player/events/","text":"Theodul Pass Player - Events A Theodul Pass Player plugin can trigger and/or subscribe to events. An event is defined in the events section of the plugin and looks like this: NAME: new Engage.Event(\"MODULE:NAME\", \"DESCRIPTION\", \"OPTION\") The event has the event name \"MODULE:NAME\", the description DESCRIPTION and one of the options \"trigger\", \"handler\" or \"both\" as OPTION. When the plugin just triggers the event, the option is \"trigger\", when it just handles the events the option is \"handler\" and when it does both - trigger and handle it - the option is \"both\". An event can be triggered via Engage.trigger(plugin.events.NAME.getName(), [parameter(s)]); and can be subscribed to via Engage.on(plugin.events.NAME.getName(), function () {}); The following list contains all events of the Core + of all official plugins, sorted alphabetically after \"Event name\" for version 1.0 of Feb 12, 2015. Currently official plugins are Controls MHConnection Notifications Usertracking Description Description (Tab) Slide text (Tab) Shortcuts (Tab) Timeline statistics Videodisplay Name Event name Additional parameters Description Triggered in Handled in coreInit Core:init Core plugin_load_done Core:plugin_load_done Core Core, Controls, MHConnection, Notifications, Usertracking, Description, Description (Tab), Slide text (Tab), Shortcuts (Tab), Timeline statistics, Videodisplay timelineplugin_closed Engage:timelineplugin_closed Note: No \"Engage Event\", just use as string, example: Engage.on(\"Engage:timelineplugin_closed\", function() {}); when the timeline plugin container closed Core timelineplugin_opened Engage:timelineplugin_opened Note: No \"Engage Event\", just use as string, example: Engage.on(\"Engage:timelineplugin_opened\", function() {}); when the timeline plugin container opened Core Timeline statistics getMediaInfo MhConnection:getMediaInfo MHConnection getMediaPackage MhConnection:getMediaPackage MHConnection mediaPackageModelError MhConnection:mediaPackageModelError MHConnection Core, Controls, Notifications, Usertracking, Description, Description (Tab), Slide text (Tab), Shortcuts (Tab), Timeline statistics, Videodisplay customError Notification:customError msg: The message to display an error occurred Core, Controls, Videodisplay Notifications customNotification Notification:customNotification msg: The message to display a custom message Videodisplay Notifications customOKMessage Notification:customOKMessage msg: The message to display a custom message with an OK button Controls Notifications customSuccess Notification:customSuccess msg: The message to display a custom success message Core, Controls Notifications segmentMouseout Segment:mouseOut no: Segment number the mouse is off a segment Controls, Slide text (Tab) Controls, Slide text (Tab) segmentMouseover Segment:mouseOver no: Segment number the mouse is over a segment Controls, Slide text (Tab) Controls, Slide text (Tab) sliderMousein Slider:mouseIn the mouse entered the slider Controls sliderMouseout Slider:mouseOut the mouse is off the slider Controls sliderMousemove Slider:mouseMoved timeInMs: The time on the hovered position in ms the mouse is moving over the slider Controls sliderStart Slider:start slider started Controls sliderStop Slider:stop time: The time the slider stopped at slider stopped Controls Videodisplay aspectRatioSet Video:aspectRatioSet as: (array) as[0] = width, as[1] = height, as[2] = aspect ratio in % the aspect ratio has been calculated Videodisplay Controls audioCodecNotSupported Video:audioCodecNotSupported when the audio codec seems not to be supported by the browser Videodisplay Notifications autoplay Video:autoplay autoplay the video Core Videodisplay bufferedAndAutoplaying Video:bufferedAndAutoplaying buffering successful, was playing, autoplaying now Videodisplay Notifications bufferedButNotAutoplaying Video:bufferedButNotAutoplaying buffering successful, was not playing, not autoplaying now Videodisplay Notifications buffering Video:buffering video is buffering Videodisplay Notifications ended Video:ended triggeredByMaster: Whether or not the event has been triggered by master video ended Videodisplay Controls fullscreenCancel Video:fullscreenCancel cancel fullscreen Controls, Videodisplay Videodisplay fullscreenChange Video:fullscreenChange a fullscreen change happened Videodisplay Controls fullscreenEnable Video:fullscreenEnable enable fullscreen Controls, Core Controls, Videodisplay isAudioOnly Video:isAudioOnly audio: true if audio only, false else whether it's audio only or not Videodisplay Controls, Notifications initialSeek Video:initialSeek time: The time to seek to Seeks initially after all plugins have been loaded after a short delay Core Videodisplay mute Video:mute mute Videodisplay Videodisplay muteToggle Video:muteToggle toggle mute and unmute Core Videodisplay nextChapter Video:nextChapter Core numberOfVideodisplaysSet Video:numberOfVideodisplaysSet no: Number of videodisplays the number of videodisplays has been set Videodisplay pause Video:pause triggeredByMaster: Whether or not the event has been triggered by master pauses the video Core, Controls, Videodisplay Controls, Videodisplay play Video:play triggeredByMaster: Whether or not the event has been triggered by master plays the video Core, Controls, Videodisplay Controls,Videodisplay playPause Video:playPause Core Videodisplay previousChapter Video:previousChapter Core playbackRateChanged Video:playbackRateChanged rate: The video playback rate (0.0-x, default: 1.0) The video playback rate changed Controls Controls, Videodisplay playbackRateIncrease Video:playbackRateIncrease Core Videodisplay playbackRateDecrease Video:playbackRateDecrease Core Videodisplay playerLoaded Video:playerLoaded player loaded successfully Videodisplay ready Video:ready all videos loaded successfully Videodisplay Controls, Notifications seek Video:seek time: Current time in seconds seek video to a given position in seconds Core, Controls, Slide text (Tab) Videodisplay seekLeft Video:seekLeft Core Videodisplay seekRight Video:seekRight Core Videodisplay synchronizing Video:synchronizing synchronizing videos with the master video Videodisplay timeupdate Video:timeupdate time: Current time in seconds, triggeredByMaster: Whether or not the event has been triggered by master a timeupdate happened Videodisplay Controls, Usertracking qualitySet Video:qualitySet quality: the quality that has been set a video quality has been set Controls Videodisplay unmute Video:unmute unmute Controls Controls usingFlash Video:usingFlash flash: true if flash is being used, false else flash is being used Videodisplay Controls videoFormatsFound Video:videoFormatsFound format: array of video formats if different video formats (qualities) have been found Videodisplay Controls volumechange Video:volumechange vol: Current volume (0 is off (muted), 1.0 is all the way up, 0.5 is half way) a volume change happened Videodisplay volumeDown Video:volumeDown Core Controls volumeGet Video:volumeGet callback: A callback function with the current volume as a parameter get the volume Videodisplay volumeSet Video:volumeSet percentAsDecimal: Volume to set (0 is off (muted), 1.0 is all the way up, 0.5 is half way) set the volume Controls Controls, Videodisplay volumeUp Video:volumeUp Core Controls","title":"Events"},{"location":"modules/player/events/#theodul-pass-player-events","text":"A Theodul Pass Player plugin can trigger and/or subscribe to events. An event is defined in the events section of the plugin and looks like this: NAME: new Engage.Event(\"MODULE:NAME\", \"DESCRIPTION\", \"OPTION\") The event has the event name \"MODULE:NAME\", the description DESCRIPTION and one of the options \"trigger\", \"handler\" or \"both\" as OPTION. When the plugin just triggers the event, the option is \"trigger\", when it just handles the events the option is \"handler\" and when it does both - trigger and handle it - the option is \"both\". An event can be triggered via Engage.trigger(plugin.events.NAME.getName(), [parameter(s)]); and can be subscribed to via Engage.on(plugin.events.NAME.getName(), function () {}); The following list contains all events of the Core + of all official plugins, sorted alphabetically after \"Event name\" for version 1.0 of Feb 12, 2015. Currently official plugins are Controls MHConnection Notifications Usertracking Description Description (Tab) Slide text (Tab) Shortcuts (Tab) Timeline statistics Videodisplay Name Event name Additional parameters Description Triggered in Handled in coreInit Core:init Core plugin_load_done Core:plugin_load_done Core Core, Controls, MHConnection, Notifications, Usertracking, Description, Description (Tab), Slide text (Tab), Shortcuts (Tab), Timeline statistics, Videodisplay timelineplugin_closed Engage:timelineplugin_closed Note: No \"Engage Event\", just use as string, example: Engage.on(\"Engage:timelineplugin_closed\", function() {}); when the timeline plugin container closed Core timelineplugin_opened Engage:timelineplugin_opened Note: No \"Engage Event\", just use as string, example: Engage.on(\"Engage:timelineplugin_opened\", function() {}); when the timeline plugin container opened Core Timeline statistics getMediaInfo MhConnection:getMediaInfo MHConnection getMediaPackage MhConnection:getMediaPackage MHConnection mediaPackageModelError MhConnection:mediaPackageModelError MHConnection Core, Controls, Notifications, Usertracking, Description, Description (Tab), Slide text (Tab), Shortcuts (Tab), Timeline statistics, Videodisplay customError Notification:customError msg: The message to display an error occurred Core, Controls, Videodisplay Notifications customNotification Notification:customNotification msg: The message to display a custom message Videodisplay Notifications customOKMessage Notification:customOKMessage msg: The message to display a custom message with an OK button Controls Notifications customSuccess Notification:customSuccess msg: The message to display a custom success message Core, Controls Notifications segmentMouseout Segment:mouseOut no: Segment number the mouse is off a segment Controls, Slide text (Tab) Controls, Slide text (Tab) segmentMouseover Segment:mouseOver no: Segment number the mouse is over a segment Controls, Slide text (Tab) Controls, Slide text (Tab) sliderMousein Slider:mouseIn the mouse entered the slider Controls sliderMouseout Slider:mouseOut the mouse is off the slider Controls sliderMousemove Slider:mouseMoved timeInMs: The time on the hovered position in ms the mouse is moving over the slider Controls sliderStart Slider:start slider started Controls sliderStop Slider:stop time: The time the slider stopped at slider stopped Controls Videodisplay aspectRatioSet Video:aspectRatioSet as: (array) as[0] = width, as[1] = height, as[2] = aspect ratio in % the aspect ratio has been calculated Videodisplay Controls audioCodecNotSupported Video:audioCodecNotSupported when the audio codec seems not to be supported by the browser Videodisplay Notifications autoplay Video:autoplay autoplay the video Core Videodisplay bufferedAndAutoplaying Video:bufferedAndAutoplaying buffering successful, was playing, autoplaying now Videodisplay Notifications bufferedButNotAutoplaying Video:bufferedButNotAutoplaying buffering successful, was not playing, not autoplaying now Videodisplay Notifications buffering Video:buffering video is buffering Videodisplay Notifications ended Video:ended triggeredByMaster: Whether or not the event has been triggered by master video ended Videodisplay Controls fullscreenCancel Video:fullscreenCancel cancel fullscreen Controls, Videodisplay Videodisplay fullscreenChange Video:fullscreenChange a fullscreen change happened Videodisplay Controls fullscreenEnable Video:fullscreenEnable enable fullscreen Controls, Core Controls, Videodisplay isAudioOnly Video:isAudioOnly audio: true if audio only, false else whether it's audio only or not Videodisplay Controls, Notifications initialSeek Video:initialSeek time: The time to seek to Seeks initially after all plugins have been loaded after a short delay Core Videodisplay mute Video:mute mute Videodisplay Videodisplay muteToggle Video:muteToggle toggle mute and unmute Core Videodisplay nextChapter Video:nextChapter Core numberOfVideodisplaysSet Video:numberOfVideodisplaysSet no: Number of videodisplays the number of videodisplays has been set Videodisplay pause Video:pause triggeredByMaster: Whether or not the event has been triggered by master pauses the video Core, Controls, Videodisplay Controls, Videodisplay play Video:play triggeredByMaster: Whether or not the event has been triggered by master plays the video Core, Controls, Videodisplay Controls,Videodisplay playPause Video:playPause Core Videodisplay previousChapter Video:previousChapter Core playbackRateChanged Video:playbackRateChanged rate: The video playback rate (0.0-x, default: 1.0) The video playback rate changed Controls Controls, Videodisplay playbackRateIncrease Video:playbackRateIncrease Core Videodisplay playbackRateDecrease Video:playbackRateDecrease Core Videodisplay playerLoaded Video:playerLoaded player loaded successfully Videodisplay ready Video:ready all videos loaded successfully Videodisplay Controls, Notifications seek Video:seek time: Current time in seconds seek video to a given position in seconds Core, Controls, Slide text (Tab) Videodisplay seekLeft Video:seekLeft Core Videodisplay seekRight Video:seekRight Core Videodisplay synchronizing Video:synchronizing synchronizing videos with the master video Videodisplay timeupdate Video:timeupdate time: Current time in seconds, triggeredByMaster: Whether or not the event has been triggered by master a timeupdate happened Videodisplay Controls, Usertracking qualitySet Video:qualitySet quality: the quality that has been set a video quality has been set Controls Videodisplay unmute Video:unmute unmute Controls Controls usingFlash Video:usingFlash flash: true if flash is being used, false else flash is being used Videodisplay Controls videoFormatsFound Video:videoFormatsFound format: array of video formats if different video formats (qualities) have been found Videodisplay Controls volumechange Video:volumechange vol: Current volume (0 is off (muted), 1.0 is all the way up, 0.5 is half way) a volume change happened Videodisplay volumeDown Video:volumeDown Core Controls volumeGet Video:volumeGet callback: A callback function with the current volume as a parameter get the volume Videodisplay volumeSet Video:volumeSet percentAsDecimal: Volume to set (0 is off (muted), 1.0 is all the way up, 0.5 is half way) set the volume Controls Controls, Videodisplay volumeUp Video:volumeUp Core Controls","title":"Theodul Pass Player - Events"},{"location":"modules/player/plugin.development/","text":"How To Create A New Plugin Plugin Archetype The Maven Archetype Plugin provides a convenient mechanism for automatically generating projects. Project templates are called Archetypes and they are basically maven artifacts of a special kind of packaging, \u2018maven-archetype\u2019. With the Theodul Plugin Archetype you can create a new plugin project in no time and start writing the plugin\u2019s business logic right away, without caring about the POM or SCR component declarations. Installation The Theodul Plugin Archetype is included in the Opencast source code (Theodul Player branch) in the modules directory. To make the artifact available on your system you need to install it like any other atrifacts. In the Opencast source directory type: > cd modules/engage-theodul-plugin-archetype > mvn install After successful build and installation the archetype is available in your system. Generating a new plugin To generate a new plugin project simply go to the modules directory inside the Opencast source directory and type: > mvn archetype:generate -DarchetypeGroupId=org.opencastproject -DarchetypeArtifactId=theodul-plugin Provided the archetype is installed maven will now ask you for the properties configuration for the new project: [INFO] Generating project in Interactive mode [INFO] Archetype [org.opencastproject:theodul-plugin:1.5-SNAPSHOT] found in catalog local Define value for property 'groupId': : org.opencastproject Define value for property 'artifactId': : engage-theodul-plugin-test Define value for property 'version': 1.0-SNAPSHOT: : 1.5-SNAPSHOT Define value for property 'package': org.opencastproject: : org.opencastproject.engage.theodul.plugin.custom.test Define value for property 'plugin_description': : A test plugin Define value for property 'plugin_name': : testName Define value for property 'plugin_type': : custom Define value for property 'plugin_version': : 0.1 Define value for property 'plugin_rest': : false Confirm properties configuration: groupId: org.opencastproject artifactId: engage-theodul-plugin-test version: 1.5-SNAPSHOT package: org.opencastproject.engage.theodul.plugin.test plugin_description: A test plugin plugin_name: test plugin_rest: true Y: : y [INFO] ---------------------------------------------------------------------------- [INFO] Using following parameters for creating project from Archetype: theodul-plugin:1.5-SNAPSHOT [INFO] ---------------------------------------------------------------------------- [INFO] Parameter: groupId, Value: org.opencastproject [INFO] Parameter: artifactId, Value: engage-theodul-plugin-test [INFO] Parameter: version, Value: 1.5-SNAPSHOT [INFO] Parameter: package, Value: org.opencastproject.engage.theodul.plugin.test [INFO] Parameter: packageInPathFormat, Value: org/opencastproject/engage/theodul/plugin/test [INFO] Parameter: package, Value: org.opencastproject.engage.theodul.plugin.test [INFO] Parameter: version, Value: 1.5-SNAPSHOT [INFO] Parameter: plugin_description, Value: A test plugin [INFO] Parameter: plugin_name, Value: test [INFO] Parameter: groupId, Value: org.opencastproject [INFO] Parameter: plugin_rest, Value: true [INFO] Parameter: artifactId, Value: engage-theodul-plugin-test [INFO] project created from Archetype in dir: /home/wulff/code/UOS/plugin-archetype/test/engage-theodul-plugin-test [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 3:39.195s [INFO] Finished at: Thu Jan 23 15:48:37 CET 2014 [INFO] Final Memory: 15M/308M [INFO] ------------------------------------------------------------------------ There you go, the newly created plugin project is waiting to be filled with life in the directory that is named after the atrifactId you entered before. Project Properties In addition to the above explanation, here is a description of the properties you have to specify when generating a new project with the Theodul Plugin Archetype: groupId Maven group ID. For the Opencast developers this is org.opencastproject artifactId Maven artifact ID. Name by which your project is identified as an artifact by maven. Think of it as the project name. It will also be used as the name for your projects root directory. During the course of the Theodul project the following naming scheme came up: engage-theodul-plugin-<plugin type>-<plugin name> version The project version. For Opencast developers: simply put in the version of the Opencast source tree your are working on. package The Java package in which the source for the back end part of your plugin will live. The following scheme is used by the Theodul developers: org.opencastproject.engage.theodul.plugin.<plugin type>.<plugin name> plugin_version The version of the plugin itself. This is not to be confused with the maven project version which will, for instance, be updated when the Opencast version changes. plugin_type The type of the plugin to be created. See https://opencast.jira.com/wiki/display/MH/Architecture Possible types are: custom, controls, timeline, video, description, tab plugin_name The name by which your plugin will be registered by the plugin manager when running. plugin_description (optional) A short description of the plugin. The description will be provided by the plugin list endpoint together with the other plugin data. plugin_rest (boolean) Whether or not the plugin should provide a Opencast Rest endpoint. If set to true, the Java class that makes up the back end part of your plugin will be augmented with the annotations necessary to work as a Rest endpoint provider in Opencast. Also an example endpoint (GET:sayHello) will be generated. Example Plugin Have a look at the snow showcase example plugin (custom) . Debugging To display debug information in the developer console, add the following parameters to the URL: Display debug information &debug=true Display event debug information &debugEvents=true","title":"Plugin Development"},{"location":"modules/player/plugin.development/#how-to-create-a-new-plugin","text":"","title":"How To Create A New Plugin"},{"location":"modules/player/plugin.development/#plugin-archetype","text":"The Maven Archetype Plugin provides a convenient mechanism for automatically generating projects. Project templates are called Archetypes and they are basically maven artifacts of a special kind of packaging, \u2018maven-archetype\u2019. With the Theodul Plugin Archetype you can create a new plugin project in no time and start writing the plugin\u2019s business logic right away, without caring about the POM or SCR component declarations.","title":"Plugin Archetype"},{"location":"modules/player/plugin.development/#installation","text":"The Theodul Plugin Archetype is included in the Opencast source code (Theodul Player branch) in the modules directory. To make the artifact available on your system you need to install it like any other atrifacts. In the Opencast source directory type: > cd modules/engage-theodul-plugin-archetype > mvn install After successful build and installation the archetype is available in your system.","title":"Installation"},{"location":"modules/player/plugin.development/#generating-a-new-plugin","text":"To generate a new plugin project simply go to the modules directory inside the Opencast source directory and type: > mvn archetype:generate -DarchetypeGroupId=org.opencastproject -DarchetypeArtifactId=theodul-plugin Provided the archetype is installed maven will now ask you for the properties configuration for the new project: [INFO] Generating project in Interactive mode [INFO] Archetype [org.opencastproject:theodul-plugin:1.5-SNAPSHOT] found in catalog local Define value for property 'groupId': : org.opencastproject Define value for property 'artifactId': : engage-theodul-plugin-test Define value for property 'version': 1.0-SNAPSHOT: : 1.5-SNAPSHOT Define value for property 'package': org.opencastproject: : org.opencastproject.engage.theodul.plugin.custom.test Define value for property 'plugin_description': : A test plugin Define value for property 'plugin_name': : testName Define value for property 'plugin_type': : custom Define value for property 'plugin_version': : 0.1 Define value for property 'plugin_rest': : false Confirm properties configuration: groupId: org.opencastproject artifactId: engage-theodul-plugin-test version: 1.5-SNAPSHOT package: org.opencastproject.engage.theodul.plugin.test plugin_description: A test plugin plugin_name: test plugin_rest: true Y: : y [INFO] ---------------------------------------------------------------------------- [INFO] Using following parameters for creating project from Archetype: theodul-plugin:1.5-SNAPSHOT [INFO] ---------------------------------------------------------------------------- [INFO] Parameter: groupId, Value: org.opencastproject [INFO] Parameter: artifactId, Value: engage-theodul-plugin-test [INFO] Parameter: version, Value: 1.5-SNAPSHOT [INFO] Parameter: package, Value: org.opencastproject.engage.theodul.plugin.test [INFO] Parameter: packageInPathFormat, Value: org/opencastproject/engage/theodul/plugin/test [INFO] Parameter: package, Value: org.opencastproject.engage.theodul.plugin.test [INFO] Parameter: version, Value: 1.5-SNAPSHOT [INFO] Parameter: plugin_description, Value: A test plugin [INFO] Parameter: plugin_name, Value: test [INFO] Parameter: groupId, Value: org.opencastproject [INFO] Parameter: plugin_rest, Value: true [INFO] Parameter: artifactId, Value: engage-theodul-plugin-test [INFO] project created from Archetype in dir: /home/wulff/code/UOS/plugin-archetype/test/engage-theodul-plugin-test [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 3:39.195s [INFO] Finished at: Thu Jan 23 15:48:37 CET 2014 [INFO] Final Memory: 15M/308M [INFO] ------------------------------------------------------------------------ There you go, the newly created plugin project is waiting to be filled with life in the directory that is named after the atrifactId you entered before.","title":"Generating a new plugin"},{"location":"modules/player/plugin.development/#project-properties","text":"In addition to the above explanation, here is a description of the properties you have to specify when generating a new project with the Theodul Plugin Archetype:","title":"Project Properties"},{"location":"modules/player/plugin.development/#groupid","text":"Maven group ID. For the Opencast developers this is org.opencastproject","title":"groupId"},{"location":"modules/player/plugin.development/#artifactid","text":"Maven artifact ID. Name by which your project is identified as an artifact by maven. Think of it as the project name. It will also be used as the name for your projects root directory. During the course of the Theodul project the following naming scheme came up: engage-theodul-plugin-<plugin type>-<plugin name>","title":"artifactId"},{"location":"modules/player/plugin.development/#version","text":"The project version. For Opencast developers: simply put in the version of the Opencast source tree your are working on.","title":"version"},{"location":"modules/player/plugin.development/#package","text":"The Java package in which the source for the back end part of your plugin will live. The following scheme is used by the Theodul developers: org.opencastproject.engage.theodul.plugin.<plugin type>.<plugin name>","title":"package"},{"location":"modules/player/plugin.development/#plugin_version","text":"The version of the plugin itself. This is not to be confused with the maven project version which will, for instance, be updated when the Opencast version changes.","title":"plugin_version"},{"location":"modules/player/plugin.development/#plugin_type","text":"The type of the plugin to be created. See https://opencast.jira.com/wiki/display/MH/Architecture Possible types are: custom, controls, timeline, video, description, tab","title":"plugin_type"},{"location":"modules/player/plugin.development/#plugin_name","text":"The name by which your plugin will be registered by the plugin manager when running.","title":"plugin_name"},{"location":"modules/player/plugin.development/#plugin_description","text":"(optional) A short description of the plugin. The description will be provided by the plugin list endpoint together with the other plugin data.","title":"plugin_description"},{"location":"modules/player/plugin.development/#plugin_rest","text":"(boolean) Whether or not the plugin should provide a Opencast Rest endpoint. If set to true, the Java class that makes up the back end part of your plugin will be augmented with the annotations necessary to work as a Rest endpoint provider in Opencast. Also an example endpoint (GET:sayHello) will be generated.","title":"plugin_rest"},{"location":"modules/player/plugin.development/#example-plugin","text":"Have a look at the snow showcase example plugin (custom) .","title":"Example Plugin"},{"location":"modules/player/plugin.development/#debugging","text":"To display debug information in the developer console, add the following parameters to the URL: Display debug information &debug=true Display event debug information &debugEvents=true","title":"Debugging"},{"location":"modules/player/storage/","text":"How to store data in the browser persistently The Theodul Pass Player uses basil.js for storing persistent data such as the volume and the playback rate. Basil.js unifies localstorage, cookies and session storage and provides an easy-to-use JavaScript API. Example Usage In your plugin you just have to require the basil lib which is being distributed globally: define([..., \"basil\", ...], function(..., Basil, ...) { ... } After that basil needs to be set up: var basilOptions = { namespace: 'mhStorage' }; Basil = new window.Basil(basilOptions); The default plugins have \"mhStorage\" as their namespace, feel free to set your own. The default storage is the localstorage; if the localstorage is not available, a cookie is being used and so on. After setting up basil, the usage is straightforward: Basil.set(\"someKey\", \"someValue); // set a value Basil.get(\"someKey\"); // get a value","title":"Storage"},{"location":"modules/player/storage/#how-to-store-data-in-the-browser-persistently","text":"The Theodul Pass Player uses basil.js for storing persistent data such as the volume and the playback rate. Basil.js unifies localstorage, cookies and session storage and provides an easy-to-use JavaScript API.","title":"How to store data in the browser persistently"},{"location":"modules/player/storage/#example-usage","text":"In your plugin you just have to require the basil lib which is being distributed globally: define([..., \"basil\", ...], function(..., Basil, ...) { ... } After that basil needs to be set up: var basilOptions = { namespace: 'mhStorage' }; Basil = new window.Basil(basilOptions); The default plugins have \"mhStorage\" as their namespace, feel free to set your own. The default storage is the localstorage; if the localstorage is not available, a cookie is being used and so on. After setting up basil, the usage is straightforward: Basil.set(\"someKey\", \"someValue); // set a value Basil.get(\"someKey\"); // get a value","title":"Example Usage"},{"location":"modules/player/testing/","text":"How To Test With Phantom.js and Jasmine Integration Of Jasmine Into The Build Process (Maven) Jasmine is integrated with the jasmine-maven-plugin into the maven build process. Therefore only the pom.xml file will be enhanced by the following code, which specifies the jasmine-maven-plugin as plugin for the build process. The configuration of the jasmine-maven-plugin is also done in this file. The meaning of every configuration parameter can be looked up on the jasmine-maven-plugin project page under this link . The following configuration uses a the specRunnerTemplate REQUIRE_JS in order to function properly with RequireJS . Further information about spec runner templates can be found here . On the next build the needed dependencies will be automatically resolved just like it is in the nature of maven. pom.xml <build> <plugins> ... <plugin> <groupId>com.github.searls</groupId> <artifactId>jasmine-maven-plugin</artifactId> <version>1.3.1.2</version> <executions> <execution> <goals> <goal>test</goal> </goals> </execution> </executions> <configuration> <preloadSources> <source>${project.basedir}/src/test/resources/js/lib/require.js</source> </preloadSources> <jsSrcDir>${project.basedir}/src/main/resources/static</jsSrcDir> <sourceIncludes> <include>**/*.js</include> <include>**/*.coffee</include> </sourceIncludes> <jsTestSrcDir>${project.basedir}/src/test/resources/js/spec</jsTestSrcDir> <specIncludes> <include>**/spec_helper.js</include> <include>**/*.js</include> <include>**/*.coffee</include> </specIncludes> <specRunnerTemplate>REQUIRE_JS</specRunnerTemplate> <format>progress</format> </configuration> </plugin> </plugins> </build> Testing The Engage Core This chapter gives an overview over the directory structure used for testing the theodul engage core module, the configuration for the specs in the spec_helper.js and how to write specs for the core. Directory Structure The test relevant files are located in the src/test/resources/ui/js/spec tree. Files that filename ends on _spec.js are considered as files with executable tests. The spec_helper.js in configured in the pom.xml for the initial setup. Directory Structure Testing Engage Core |-src |---main |-----java #Java impl of the plugin manager |-----resources |-------ui #UI of the core, core.html and engage_init.js |---------css #Global CSS Styles |---------js #JavaScript logic |-----------engage #Core logic, engage_core.js and engage_model.js |-----------lib #External libraries, backbone.js, jquery.js, require.js and underscore.js |---test #Unit Tests |-----resources |-------ui #JavaScript Unit Tests |---------js |-----------spec #Tests the *_spec.js and the helper file spec_helper.js Spec Helper The file spec_helper.js takes over the configuration of RequireJS which is usually done by the engage_init.js . The paths differ slighty from the player has at runtime. spec_helper for engage_core module /*global requirejs*/ requirejs.config({ baseUrl: 'src/js/lib', paths: { require: 'require', jquery: 'jquery', underscore: 'underscore', backbone: 'backbone', engage: '../engage', plugins: '../engage/plugin/*/static' }, shim: { 'backbone': { //script dependencies deps: ['underscore', 'jquery'], //global variable exports: 'Backbone' }, 'underscore': { //global variable exports: '_' } } }); var PLUGIN_MANAGER_PATH = '/engage/theodul/manager/list.json'; var PLUGIN_PATH = '/engage/theodul/plugin/'; Testing Engage Plugins This chapter gives an overview over the directory structure used for testing a theodul engage plugin module, the configuration for the specs in the spec_helper.js and how to write specs for a plugin. Directory Structure The test relevant files are located in the src/test/resources/ui/js/spec tree. Files that filename ends on _spec.js are considered as files with executable tests. The spec_helper.js in configured in the pom.xml for the initial setup. In the directory test/resources/ui/js/engage is a mockup of the theodul engage core module in order to be able to test the plugin module independent. The directory test/resources/ui/js/lib provides the libraries which are provides by the engage core module at runtime of the player, as well to be able to test the plugin module independently. Directory Structure Testing Plugins |-src |---main |-----java #Java impl of the plugin manager |-----resources |-------ui #UI of the core, core.html and engage_init.js |---------css #Global CSS Styles |---------js #JavaScript logic |-----------engage #Core logic, engage_core.js and engage_model.js |-----------lib #External libraries, backbone.js, jquery.js, require.js and underscore.js |---test #Unit Tests |-----resources |-------ui #JavaScript Unit Tests |---------js |-----------engage #Mockup of the engage_core.js and engage_model.js |-----------lib #Libraries used and provided by the core (A copy of the lib directory in the engage core module) |-----------spec #Tests the *_spec.js and the helper file spec_helper.js Spec Helper The file spec_helper.js takes over the configuration of RequireJS which is usually done by the engage_init.js . The paths differ slighty from the player uses at runtime. /*global requirejs*/ requirejs.config({ baseUrl: 'src/', paths: { require: 'test/resources/js/lib/require', jquery: 'test/resources/js/lib/jquery', underscore: 'test/resources/js/lib/underscore', backbone: 'test/resources/js/lib/backbone', engage: 'test/resources/js/engage' }, shim: { 'backbone': { //script dependencies deps: ['underscore', 'jquery'], //global variable exports: 'Backbone' }, 'underscore': { //global variable exports: '_' } } }); Writing Specs TODO Running The Tests Now you can start the build process and the jasmine specs will be executed. Each . stands for a successful test. F stands for a failure and will stop the build process like it is specified in the configuration. The example output shows a manipulated version of the tests for the theodul engage core in order to illustrate a failing test. Normally all three tests should succeed at this point. Testing on build mvn install -DdeployTo=${FELIX_HOME} // some output before [INFO] ------------------------------------------------------- J A S M I N E S P E C S ------------------------------------------------------- [INFO] F.. 1 failure: 1.) EngageCore it should have a model <<< FAILURE! * Expected { cid : 'c3', ... _pending : false } not to be defined. Results: 3 specs, 1 failures // some output before The jasmine-maven-plugin can also be executed manually and show the result in a browser. This can be achieved by the following command: Manual testing mvn jasmine:bdd [INFO] Scanning for projects... [INFO] [INFO] ------------------------------------------------------------------------ [INFO] Building engage-theodul-core 1.5-SNAPSHOT [INFO] ------------------------------------------------------------------------ [INFO] [INFO] --- jasmine-maven-plugin:1.3.1.2:bdd (default-cli) @ engage-theodul-core --- 2014-01-28 14:33:30.722:INFO:oejs.Server:jetty-8.1.10.v20130312 2014-01-28 14:33:30.746:INFO:oejs.AbstractConnector:Started SelectChannelConnector@0.0.0.0:8234 [INFO] Server started--it's time to spec some JavaScript! You can run your specs as you develop by visiting this URL in a web browser: http://localhost:8234 The server will monitor these two directories for scripts that you add, remove, and change: source directory: src/main/resources/ui spec directory: src/test/resources/ui/js/spec Just leave this process running as you test-drive your code, refreshing your browser window to re-run your specs. You can kill the server with Ctrl-C when you're done. In a browser you should see an output like it is shown on the next screenshot.","title":"Testing"},{"location":"modules/player/testing/#how-to-test-with-phantomjs-and-jasmine","text":"","title":"How To Test With Phantom.js and Jasmine"},{"location":"modules/player/testing/#integration-of-jasmine-into-the-build-process-maven","text":"Jasmine is integrated with the jasmine-maven-plugin into the maven build process. Therefore only the pom.xml file will be enhanced by the following code, which specifies the jasmine-maven-plugin as plugin for the build process. The configuration of the jasmine-maven-plugin is also done in this file. The meaning of every configuration parameter can be looked up on the jasmine-maven-plugin project page under this link . The following configuration uses a the specRunnerTemplate REQUIRE_JS in order to function properly with RequireJS . Further information about spec runner templates can be found here . On the next build the needed dependencies will be automatically resolved just like it is in the nature of maven. pom.xml <build> <plugins> ... <plugin> <groupId>com.github.searls</groupId> <artifactId>jasmine-maven-plugin</artifactId> <version>1.3.1.2</version> <executions> <execution> <goals> <goal>test</goal> </goals> </execution> </executions> <configuration> <preloadSources> <source>${project.basedir}/src/test/resources/js/lib/require.js</source> </preloadSources> <jsSrcDir>${project.basedir}/src/main/resources/static</jsSrcDir> <sourceIncludes> <include>**/*.js</include> <include>**/*.coffee</include> </sourceIncludes> <jsTestSrcDir>${project.basedir}/src/test/resources/js/spec</jsTestSrcDir> <specIncludes> <include>**/spec_helper.js</include> <include>**/*.js</include> <include>**/*.coffee</include> </specIncludes> <specRunnerTemplate>REQUIRE_JS</specRunnerTemplate> <format>progress</format> </configuration> </plugin> </plugins> </build>","title":"Integration Of Jasmine Into The Build Process (Maven)"},{"location":"modules/player/testing/#testing-the-engage-core","text":"This chapter gives an overview over the directory structure used for testing the theodul engage core module, the configuration for the specs in the spec_helper.js and how to write specs for the core.","title":"Testing The Engage Core"},{"location":"modules/player/testing/#directory-structure","text":"The test relevant files are located in the src/test/resources/ui/js/spec tree. Files that filename ends on _spec.js are considered as files with executable tests. The spec_helper.js in configured in the pom.xml for the initial setup. Directory Structure Testing Engage Core |-src |---main |-----java #Java impl of the plugin manager |-----resources |-------ui #UI of the core, core.html and engage_init.js |---------css #Global CSS Styles |---------js #JavaScript logic |-----------engage #Core logic, engage_core.js and engage_model.js |-----------lib #External libraries, backbone.js, jquery.js, require.js and underscore.js |---test #Unit Tests |-----resources |-------ui #JavaScript Unit Tests |---------js |-----------spec #Tests the *_spec.js and the helper file spec_helper.js","title":"Directory Structure"},{"location":"modules/player/testing/#spec-helper","text":"The file spec_helper.js takes over the configuration of RequireJS which is usually done by the engage_init.js . The paths differ slighty from the player has at runtime. spec_helper for engage_core module /*global requirejs*/ requirejs.config({ baseUrl: 'src/js/lib', paths: { require: 'require', jquery: 'jquery', underscore: 'underscore', backbone: 'backbone', engage: '../engage', plugins: '../engage/plugin/*/static' }, shim: { 'backbone': { //script dependencies deps: ['underscore', 'jquery'], //global variable exports: 'Backbone' }, 'underscore': { //global variable exports: '_' } } }); var PLUGIN_MANAGER_PATH = '/engage/theodul/manager/list.json'; var PLUGIN_PATH = '/engage/theodul/plugin/';","title":"Spec Helper"},{"location":"modules/player/testing/#testing-engage-plugins","text":"This chapter gives an overview over the directory structure used for testing a theodul engage plugin module, the configuration for the specs in the spec_helper.js and how to write specs for a plugin.","title":"Testing Engage Plugins"},{"location":"modules/player/testing/#directory-structure_1","text":"The test relevant files are located in the src/test/resources/ui/js/spec tree. Files that filename ends on _spec.js are considered as files with executable tests. The spec_helper.js in configured in the pom.xml for the initial setup. In the directory test/resources/ui/js/engage is a mockup of the theodul engage core module in order to be able to test the plugin module independent. The directory test/resources/ui/js/lib provides the libraries which are provides by the engage core module at runtime of the player, as well to be able to test the plugin module independently. Directory Structure Testing Plugins |-src |---main |-----java #Java impl of the plugin manager |-----resources |-------ui #UI of the core, core.html and engage_init.js |---------css #Global CSS Styles |---------js #JavaScript logic |-----------engage #Core logic, engage_core.js and engage_model.js |-----------lib #External libraries, backbone.js, jquery.js, require.js and underscore.js |---test #Unit Tests |-----resources |-------ui #JavaScript Unit Tests |---------js |-----------engage #Mockup of the engage_core.js and engage_model.js |-----------lib #Libraries used and provided by the core (A copy of the lib directory in the engage core module) |-----------spec #Tests the *_spec.js and the helper file spec_helper.js","title":"Directory Structure"},{"location":"modules/player/testing/#spec-helper_1","text":"The file spec_helper.js takes over the configuration of RequireJS which is usually done by the engage_init.js . The paths differ slighty from the player uses at runtime. /*global requirejs*/ requirejs.config({ baseUrl: 'src/', paths: { require: 'test/resources/js/lib/require', jquery: 'test/resources/js/lib/jquery', underscore: 'test/resources/js/lib/underscore', backbone: 'test/resources/js/lib/backbone', engage: 'test/resources/js/engage' }, shim: { 'backbone': { //script dependencies deps: ['underscore', 'jquery'], //global variable exports: 'Backbone' }, 'underscore': { //global variable exports: '_' } } });","title":"Spec Helper"},{"location":"modules/player/testing/#writing-specs","text":"TODO","title":"Writing Specs"},{"location":"modules/player/testing/#running-the-tests","text":"Now you can start the build process and the jasmine specs will be executed. Each . stands for a successful test. F stands for a failure and will stop the build process like it is specified in the configuration. The example output shows a manipulated version of the tests for the theodul engage core in order to illustrate a failing test. Normally all three tests should succeed at this point. Testing on build mvn install -DdeployTo=${FELIX_HOME} // some output before [INFO] ------------------------------------------------------- J A S M I N E S P E C S ------------------------------------------------------- [INFO] F.. 1 failure: 1.) EngageCore it should have a model <<< FAILURE! * Expected { cid : 'c3', ... _pending : false } not to be defined. Results: 3 specs, 1 failures // some output before The jasmine-maven-plugin can also be executed manually and show the result in a browser. This can be achieved by the following command: Manual testing mvn jasmine:bdd [INFO] Scanning for projects... [INFO] [INFO] ------------------------------------------------------------------------ [INFO] Building engage-theodul-core 1.5-SNAPSHOT [INFO] ------------------------------------------------------------------------ [INFO] [INFO] --- jasmine-maven-plugin:1.3.1.2:bdd (default-cli) @ engage-theodul-core --- 2014-01-28 14:33:30.722:INFO:oejs.Server:jetty-8.1.10.v20130312 2014-01-28 14:33:30.746:INFO:oejs.AbstractConnector:Started SelectChannelConnector@0.0.0.0:8234 [INFO] Server started--it's time to spec some JavaScript! You can run your specs as you develop by visiting this URL in a web browser: http://localhost:8234 The server will monitor these two directories for scripts that you add, remove, and change: source directory: src/main/resources/ui spec directory: src/test/resources/ui/js/spec Just leave this process running as you test-drive your code, refreshing your browser window to re-run your specs. You can kill the server with Ctrl-C when you're done. In a browser you should see an output like it is shown on the next screenshot.","title":"Running The Tests"}]}